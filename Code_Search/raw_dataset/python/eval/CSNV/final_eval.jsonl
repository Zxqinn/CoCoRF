{"id": "https://github.com/JoseAntFer/pyny3d/blob/fb81684935a24f7e50c975cb4383c81a63ab56df/pyny3d/utils.py#L8-L33", "method_name": "sort_numpy", "code": "def sort_numpy(array, col=0, order_back=False): \n     x = array[:,col] \n     sorted_index = np.argsort(x, kind = \"quicksort\") \n     sorted_array = array[sorted_index] \n     if not order_back: \n         return sorted_array \n     else: \n         n_points = sorted_index.shape[0] \n         order_back = np.empty(n_points, dtype=int) \n         order_back[sorted_index] = np.arange(n_points) \n         return [sorted_array, order_back]", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/queues/priority_queue.py#L38-L49", "method_name": "push", "code": "def push(self, item, priority=None):\n         priority = item if priority is None else priority\n         node = PriorityQueueNode(item, priority)\n         for index, current in enumerate(self.priority_queue_list):\n             if current.priority < node.priority:\n                 self.priority_queue_list.insert(index, node)\n                 return\n         self.priority_queue_list.append(node)", "query": "priority queue"}
{"id": "https://github.com/mapbox/mapbox-sdk-py/blob/72d19dbcf2d254a6ea08129a726471fd21f13023/mapbox/services/base.py#L122-L144", "method_name": "handle_http_error", "code": "def handle_http_error(self, response, custom_messages=None,\n                           raise_for_status=False):\n         if not custom_messages:\n             custom_messages = {}\n         if response.status_code in custom_messages.keys():\n             raise errors.HTTPError(custom_messages[response.status_code])\n         if raise_for_status:\n             response.raise_for_status()", "query": "custom http error response"}
{"id": "https://github.com/dragnet-org/dragnet/blob/532c9d9f28e5b1b57f3cabc708218d3863a16322/dragnet/__init__.py#L9-L13", "method_name": "extract_content", "code": "def extract_content(html, encoding=None, as_blocks=False):\n     if \"content\" not in _LOADED_MODELS:\n         _LOADED_MODELS[\"content\"] = load_pickled_model(\n             \"kohlschuetter_readability_weninger_content_model.pkl.gz\")\n     return _LOADED_MODELS[\"content\"].extract(html, encoding=encoding, as_blocks=as_blocks)", "query": "extract data from html content"}
{"id": "https://github.com/sarugaku/virtenv/blob/fc42a9d8dc9f1821d3893899df78e08a081f6ca3/virtenv_cli.py#L20-L28", "method_name": "which", "code": "def which(name):\n     for p in os.environ[\"PATH\"].split(os.pathsep):\n         exe = os.path.join(p, name)\n         if is_executable(exe):\n             return os.path.abspath(exe)\n         for ext in [\"\"] + os.environ.get(\"PATHEXT\", \"\").split(os.pathsep):\n             exe = \"{}{}\".format(exe, ext.lower())\n             if is_executable(exe):\n                 return os.path.abspath(exe)", "query": "get executable path"}
{"id": "https://github.com/rkargon/pixelsorter/blob/0775d1e487fbcb023e411e1818ba3290b0e8665e/pixelsorter/util.py#L72-L84", "method_name": "weighted_random_choice", "code": "def weighted_random_choice(items):\n     l = list(items)\n     r = random.random() * sum([i[1] for i in l])\n     for x, p in l:\n         if p > r:\n             return x\n         r -= p\n     return None", "query": "randomly extract x items from a list"}
{"id": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/strip_url_params.py#L85-L95", "method_name": "strip_url_params3", "code": "def strip_url_params3(url, strip=None):\n     if not strip: strip = []\n     parse = urllib.parse.urlparse(url)\n     query = urllib.parse.parse_qs(parse.query)\n     query = {k: v[0] for k, v in query.items() if k not in strip}\n     query = urllib.parse.urlencode(query)\n     new = parse._replace(query=query)\n     return new.geturl()", "query": "parse query string in url"}
{"id": "https://github.com/eddiejessup/spatious/blob/b7ae91bec029e85a45a7f303ee184076433723cd/spatious/vector.py#L185-L216", "method_name": "sphere_pick_polar", "code": "def sphere_pick_polar(d, n=1, rng=None):\n     if rng is None:\n         rng = np.random\n     a = np.empty([n, d])\n     if d == 1:\n         a[:, 0] = rng.randint(2, size=n) * 2 - 1\n     elif d == 2:\n         a[:, 0] = 1.0\n         a[:, 1] = rng.uniform(-np.pi, +np.pi, n)\n     elif d == 3:\n         u, v = rng.uniform(0.0, 1.0, (2, n))\n         a[:, 0] = 1.0\n         a[:, 1] = np.arccos(2.0 * v - 1.0)\n         a[:, 2] = 2.0 * np.pi * u\n     else:\n         raise Exception(\"Invalid vector for polar representation\")\n     return a", "query": "randomly pick a number"}
{"id": "https://github.com/mapbox/mapbox-sdk-py/blob/72d19dbcf2d254a6ea08129a726471fd21f13023/mapbox/services/base.py#L122-L144", "method_name": "handle_http_error", "code": "def handle_http_error(self, response, custom_messages=None,\n                           raise_for_status=False):\n         if not custom_messages:\n             custom_messages = {}\n         if response.status_code in custom_messages.keys():\n             raise errors.HTTPError(custom_messages[response.status_code])\n         if raise_for_status:\n             response.raise_for_status()", "query": "custom http error response"}
{"id": "https://github.com/rigetti/grove/blob/dc6bf6ec63e8c435fe52b1e00f707d5ce4cdb9b3/grove/tomography/operator_utils.py#L70-L94", "method_name": "make_diagonal_povm", "code": "def make_diagonal_povm(pi_basis, confusion_rate_matrix):\n     confusion_rate_matrix = np.asarray(confusion_rate_matrix)\n     if not np.allclose(confusion_rate_matrix.sum(axis=0), np.ones(confusion_rate_matrix.shape[1])):\n         raise CRMUnnormalizedError(\"Unnormalized confusion matrix:\n{}\".format(\n             confusion_rate_matrix))\n     if not (confusion_rate_matrix >= 0).all() or not (confusion_rate_matrix <= 1).all():\n         raise CRMValueError(\"Confusion matrix must have values in [0, 1]:\"\n                             \"\n{}\".format(confusion_rate_matrix))\n     ops = [sum((pi_j * pjk for (pi_j, pjk) in izip(pi_basis.ops, pjs)), 0)\n            for pjs in confusion_rate_matrix]\n     return DiagonalPOVM(pi_basis=pi_basis, confusion_rate_matrix=confusion_rate_matrix, ops=ops)", "query": "confusion matrix"}
{"id": "https://github.com/fredericklussier/ObservablePy/blob/fd7926a0568621f80b1d567d18f199976f1fa4e8/observablePy/ObservableStore.py#L72-L84", "method_name": "add", "code": "def add(self, observableElement):\n         if observableElement not in self._observables:\n             self._observables.append(observableElement)\n         else:\n             raise RuntimeError(\n                 \"{0} is already an observable element\"\n                 .format(observableElement))", "query": "get current observable value"}
{"id": "https://github.com/Unidata/MetPy/blob/16f68a94919b9a82dcf9cada2169cf039129e67b/metpy/calc/tools.py#L819-L870", "method_name": "lat_lon_grid_deltas", "code": "def lat_lon_grid_deltas(longitude, latitude, **kwargs):\n     r\n     from pyproj import Geod\n     if latitude.ndim != longitude.ndim:\n         raise ValueError(\"Latitude and longitude must have the same number of dimensions.\")\n     if latitude.ndim < 2:\n         longitude, latitude = np.meshgrid(longitude, latitude)\n     geod_args = {\"ellps\": \"sphere\"}\n     if kwargs:\n         geod_args = kwargs\n     g = Geod(**geod_args)\n     forward_az, _, dy = g.inv(longitude[..., :-1, :], latitude[..., :-1, :],\n                               longitude[..., 1:, :], latitude[..., 1:, :])\n     dy[(forward_az < -90.)   (forward_az > 90.)] *= -1\n     forward_az, _, dx = g.inv(longitude[..., :, :-1], latitude[..., :, :-1],\n                               longitude[..., :, 1:], latitude[..., :, 1:])\n     dx[(forward_az < 0.)   (forward_az > 180.)] *= -1\n     return dx * units.meter, dy * units.meter", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L163-L170", "method_name": "dec2str", "code": "def dec2str(n):\n     s = hex(int(n))[2:].rstrip(\"L\")\n     if len(s) % 2 != 0:\n         s = \"0\" + s\n     return hex2str(s)", "query": "convert decimal to hex"}
{"id": "https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L471-L484", "method_name": "file_unzipper", "code": "def file_unzipper(directory):\n    debug.log(\"Unzipping directory (%s)...\"%directory)\n    for root, dirs, files in os.walk(directory, topdown=False):\n       if root != \"\":\n          orig_dir = os.getcwd()\n          os.chdir(directory)\n          Popen(\"gunzip -q -f *.gz > /dev/null 2>&1\", shell=True).wait()\n          Popen(\"unzip -qq -o \"*.zip\" > /dev/null 2>&1\", shell=True).wait()\n          Popen(\"rm -f *.zip > /dev/null 2>&1\", shell=True).wait()\n          os.chdir(orig_dir)", "query": "unzipping large files"}
{"id": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/clustering.py#L35-L56", "method_name": "cluster_kmeans", "code": "def cluster_kmeans(data, n_clusters, **kwargs):\n     km = cl.KMeans(n_clusters, **kwargs)\n     kmf = km.fit(data)\n     labels = kmf.labels_\n     return labels, [np.nan]", "query": "k means clustering"}
{"id": "https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/editor.py#L53-L59", "method_name": "unzip", "code": "def unzip(self, in_file, out_file):\n         with ZipFile(in_file) as zf:\n             zf.extract(\"collection.anki2\", path=self.tempdir)\n         shutil.move(os.path.join(self.tempdir, \"collection.anki2\"),\n                     out_file)\n         return out_file", "query": "unzipping large files"}
{"id": "https://github.com/gpagliuca/pyfas/blob/5daa1199bd124d315d02bef0ad3888a8f58355b2/build/lib/pyfas/ppl.py#L127-L151", "method_name": "to_excel", "code": "def to_excel(self, *args): \n         path = os.getcwd() \n         fname = self.fname.replace(\".ppl\", \"_ppl\") + \".xlsx\" \n         if len(args) > 0 and args[0] != \"\": \n             path = args[0] \n             if os.path.exists(path) == False: \n                 os.mkdir(path) \n         xl_file = pd.ExcelWriter(path + os.sep + fname) \n         for idx in self.filter_data(\"\"): \n             self.extract(idx) \n         labels = list(self.filter_data(\"\").values()) \n         for prof in self.data: \n             data_df = pd.DataFrame() \n             data_df[\"X\"] = self.data[prof][0] \n             for timestep, data in zip(self.time, self.data[prof][1]): \n                 data_df[timestep] = data \n             myvar = labels[prof-1].split(\" \")[0] \n             br_label = labels[prof-1].split(\"\\\"\")[5] \n             unit = labels[prof-1].split(\"\\\"\")[7].replace(\"/\", \"-\") \n             mylabel = \"{} - {} - {}\".format(myvar, br_label, unit) \n             data_df.to_excel(xl_file, sheet_name=mylabel) \n         xl_file.save()", "query": "export to excel"}
{"id": "https://github.com/kalefranz/auxlib/blob/6ff2d6b57d128d0b9ed8f01ad83572e938da064f/auxlib/crypt.py#L77-L97", "method_name": "aes_encrypt", "code": "def aes_encrypt(base64_encryption_key, data):\n     if isinstance(data, text_type):\n         data = data.encode(\"UTF-8\")\n     aes_key_bytes, hmac_key_bytes = _extract_keys(base64_encryption_key)\n     data = _pad(data)\n     iv_bytes = os.urandom(AES_BLOCK_SIZE)\n     cipher = AES.new(aes_key_bytes, mode=AES.MODE_CBC, IV=iv_bytes)\n     data = iv_bytes + cipher.encrypt(data)  \n     hmac_signature = hmac.new(hmac_key_bytes, data, hashlib.sha256).digest()\n     return as_base64(data + hmac_signature)", "query": "aes encryption"}
{"id": "https://github.com/Open-ET/openet-core-beta/blob/f2b81ccf87bf7e7fe1b9f3dd1d4081d0ec7852db/openet/core/utils.py#L82-L95", "method_name": "date_0utc", "code": "def date_0utc(date):\n     return ee.Date.fromYMD(date.get(\"year\"), date.get(\"month\"),\n                            date.get(\"day\"))", "query": "get current date"}
{"id": "https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/excel.py#L155-L157", "method_name": "save", "code": "def save(self):\n        pyexcel_export.save_data(self.excel_filename, data=self.excel_raw, retain_meta=True,\n                                 created=self.created, modified=datetime.now().isoformat())", "query": "export to excel"}
{"id": "https://github.com/HacKanCuBa/passphrase-py/blob/219d6374338ed9a1475b4f09b0d85212376f11e0/passphrase/passphrase.py#L71-L76", "method_name": "randnum_min", "code": "def randnum_min(self, randnum: int) -> None:\n         if not isinstance(randnum, int):\n             raise TypeError(\"randnum_min can only be int\")\n         if randnum < 0:\n             raise ValueError(\"randnum_min should be greater than 0\")\n         self._randnum_min = randnum", "query": "randomly pick a number"}
{"id": "https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/tag.py#L148-L152", "method_name": "html", "code": "def html(self) -> str:\n         if self._inner_element:\n             return self.start_tag + self._inner_element.html + self.end_tag\n         return super().html", "query": "get inner html"}
{"id": "https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/plotlywrapper.py#L832-L850", "method_name": "heatmap", "code": "def heatmap(z, x=None, y=None, colorscale=\"Viridis\"):\n     z = np.atleast_1d(z)\n     data = [go.Heatmap(z=z, x=x, y=y, colorscale=colorscale)]\n     return Chart(data=data)", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/DenisCarriere/geocoder/blob/39b9999ec70e61da9fa52fe9fe82a261ad70fa8b/geocoder/opencage.py#L409-L415", "method_name": "_catch_errors", "code": "def _catch_errors(self, json_response):\n         status = json_response.get(\"status\")\n         if status and status.get(\"code\") != 200:\n             self.status_code = status.get(\"code\")\n             self.error = status.get(\"message\")\n         return self.error", "query": "get the description of a http status code"}
{"id": "https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/traces/filter.py#L28-L34", "method_name": "apply_filter", "code": "def apply_filter(x, filter=None, axis=0):\n     x = _as_array(x)\n     if x.shape[axis] == 0:\n         return x\n     b, a = filter\n     return signal.filtfilt(b, a, x, axis=axis)", "query": "filter array"}
{"id": "https://github.com/flatironinstitute/kbucket/blob/867915ebb0ea153a399c3e392698f89bf43c7903/kbucket/kbucketclient.py#L639-L645", "method_name": "_read_json_file", "code": "def _read_json_file(path):\n   try:\n     with open(path) as f:\n       return json.load(f)\n   except:\n     print (\"Warning: Unable to read or parse json file: \"+path)\n     return None", "query": "parse json file"}
{"id": "https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/controller/widgets/SignalFrame.py#L31-L33", "method_name": "perform_filter", "code": "def perform_filter(result_array: Array, data, f_low, f_high, filter_bw):\n    result_array = np.frombuffer(result_array.get_obj(), dtype=np.complex64)\n    result_array[:] = Filter.apply_bandpass_filter(data, f_low, f_high, filter_bw=filter_bw)", "query": "filter array"}
{"id": "https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L662-L667", "method_name": "checkbox_uncheck", "code": "def checkbox_uncheck(self, force_check=False):\n         if self.get_attribute(\"checked\"):\n             self.click(force_click=force_check)", "query": "make the checkbox checked"}
{"id": "https://github.com/nutechsoftware/alarmdecoder/blob/b0c014089e24455228cb4402cf30ba98157578cd/alarmdecoder/devices/serial_device.py#L149-L172", "method_name": "write", "code": "def write(self, data):\n         try:\n             if isinstance(data, str) or (sys.version_info < (3,) and isinstance(data, unicode)):\n                 data = data.encode(\"utf-8\")\n             self._device.write(data)\n         except serial.SerialTimeoutException:\n             pass\n         except serial.SerialException as err:\n             raise CommError(\"Error writing to device.\", err)\n         else:\n             self.on_write(data=data)", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/GreatFruitOmsk/tailhead/blob/a3b1324a39935f8ffcfda59328a9a458672889d9/tailhead/__init__.py#L72-L79", "method_name": "read", "code": "def read(self, read_size=-1):\n         read_str = self.file.read(read_size)\n         return len(read_str), read_str", "query": "read properties file"}
{"id": "https://github.com/EconForge/dolo/blob/d91ddf148b009bf79852d9aec70f3a1877e0f79a/dolo/compiler/symbolic.py#L10-L17", "method_name": "std_tsymbol", "code": "def std_tsymbol(tsymbol):\n     s, date = tsymbol\n     if date == 0:\n         return \"_{}_\".format(s)\n     elif date <= 0:\n         return \"_{}_m{}_\".format(s, str(-date))\n     elif date >= 0:\n         return \"_{}__{}_\".format(s, str(date))", "query": "format date"}
{"id": "https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/url.py#L250-L272", "method_name": "url_parse_query", "code": "def url_parse_query(query, encoding=None):\n     if isinstance(query, unicode):\n         if encoding is None:\n             encoding = url_encoding\n         query = query.encode(encoding, \"ignore\")\n     append = \"\"\n     while \"?\" in query:\n         query, rest = query.rsplit(\"?\", 1)\n         append = \"?\"+url_parse_query(rest)+append\n     l = []\n     for k, v, sep in parse_qsl(query, keep_blank_values=True):\n         k = url_quote_part(k, \"/-:,;\")\n         if v:\n             v = url_quote_part(v, \"/-:,;\")\n             l.append(\"%s=%s%s\" % (k, v, sep))\n         elif v is None:\n             l.append(\"%s%s\" % (k, sep))\n         else:\n             l.append(\"%s=%s\" % (k, sep))\n     return \"\".join(l) + append", "query": "parse query string in url"}
{"id": "https://github.com/flowersteam/explauto/blob/cf0f81ecb9f6412f7276a95bd27359000e1e26b6/explauto/experiment/log.py#L67-L93", "method_name": "scatter_plot", "code": "def scatter_plot(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot):\n         plot_specs = {\"marker\": \"o\", \"linestyle\": \"None\"}\n         plot_specs.update(kwargs_plot)\n         data = self.data_t(topic_dims, t)\n         ax.plot(*(data.T), **plot_specs)\n         if ms_limits:\n             ax.axis(self.axes_limits(topic_dims))", "query": "scatter plot"}
{"id": "https://github.com/deshima-dev/decode/blob/e789e174cd316e7ec8bc55be7009ad35baced3c0/decode/core/array/functions.py#L109-L121", "method_name": "empty", "code": "def empty(shape, dtype=None, **kwargs):\n     data = np.empty(shape, dtype)\n     return dc.array(data, **kwargs)", "query": "empty array"}
{"id": "https://github.com/modin-project/modin/blob/5b77d242596560c646b8405340c9ce64acb183cb/modin/backends/pandas/query_compiler.py#L1251-L1263", "method_name": "median", "code": "def median(self, **kwargs):\n         if self._is_transposed:\n             kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n             return self.transpose().median(**kwargs)\n         axis = kwargs.get(\"axis\", 0)\n         func = self._build_mapreduce_func(pandas.DataFrame.median, **kwargs)\n         return self._full_axis_reduce(axis, func)", "query": "deducting the median from each column"}
{"id": "https://github.com/hardbyte/python-can/blob/cdc5254d96072df7739263623f3e920628a7d214/can/interfaces/socketcan/utils.py#L70-L90", "method_name": "error_code_to_str", "code": "def error_code_to_str(code):\n     try:\n         name = errno.errorcode[code]\n     except KeyError:\n         name = \"UNKNOWN\"\n     try:\n         description = os.strerror(code)\n     except ValueError:\n         description = \"no description available\"\n     return \"{} (errno {}): {}\".format(name, code, description)", "query": "get the description of a http status code"}
{"id": "https://github.com/sentinel-hub/sentinelhub-py/blob/08a83b7f1e289187159a643336995d8369860fea/sentinelhub/io_utils.py#L258-L271", "method_name": "write_csv", "code": "def write_csv(filename, data, delimiter=CSV_DELIMITER):\n     with open(filename, \"w\") as file:\n         csv_writer = csv.writer(file, delimiter=delimiter)\n         for line in data:\n             csv_writer.writerow(line)", "query": "write csv"}
{"id": "https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/pipeline/factors/factor.py#L786-L843", "method_name": "linear_regression", "code": "def linear_regression(self, target, regression_length, mask=NotSpecified):\n         from .statistical import RollingLinearRegression\n         return RollingLinearRegression(\n             dependent=self,\n             independent=target,\n             regression_length=regression_length,\n             mask=mask,\n         )", "query": "linear regression"}
{"id": "https://github.com/keybase/python-triplesec/blob/0a73e18cfe542d0cd5ee57bd823a67412b4b717e/triplesec/crypto.py#L86-L92", "method_name": "encrypt", "code": "def encrypt(cls, data, key, iv_data):\n         validate_key_size(key, cls.key_size, \"AES\")\n         iv, ctr = iv_data\n         ciphertext = Crypto_AES.new(key, Crypto_AES.MODE_CTR,\n                                     counter=ctr).encrypt(data)\n         return iv + ciphertext", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/jepegit/cellpy/blob/9f4a84cdd11f72cfa02cda8c2d7b5174abbb7370/cellpy/readers/core.py#L261-L266", "method_name": "no_data", "code": "def no_data(self):\n         try:\n             empty = self.dfdata.empty\n         except AttributeError:\n             empty = True\n         return empty", "query": "empty array"}
{"id": "https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/seeyou/reader.py#L130-L143", "method_name": "decode_longitude", "code": "def decode_longitude(self, longitude):\n         match = RE_LONGITUDE.match(longitude)\n         if not match:\n             raise ParserError(\"Reading longitude failed\")\n         longitude = int(match.group(1)) + float(match.group(2)) / 60.\n         if not (0 <= longitude <= 180):\n             raise ParserError(\"Longitude out of bounds\")\n         if match.group(3).upper() == \"W\":\n             longitude = -longitude\n         return longitude", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/xlsx.py#L152-L193", "method_name": "export_to_xlsx", "code": "def export_to_xlsx(table, filename_or_fobj=None, sheet_name=\"Sheet1\", *args, **kwargs):\n     workbook = Workbook()\n     sheet = workbook.active\n     sheet.title = sheet_name\n     prepared_table = prepare_to_export(table, *args, **kwargs)\n     field_names = next(prepared_table)\n     for col_index, field_name in enumerate(field_names):\n         cell = sheet.cell(row=1, column=col_index + 1)\n         cell.value = field_name\n     _convert_row = _python_to_cell(list(map(table.fields.get, field_names)))\n     for row_index, row in enumerate(prepared_table, start=1):\n         for col_index, (value, number_format) in enumerate(_convert_row(row)):\n             cell = sheet.cell(row=row_index + 1, column=col_index + 1)\n             cell.value = value\n             if number_format is not None:\n                 cell.number_format = number_format\n     return_result = False\n     if filename_or_fobj is None:\n         filename_or_fobj = BytesIO()\n         return_result = True\n     source = Source.from_file(filename_or_fobj, mode=\"wb\", plugin_name=\"xlsx\")\n     workbook.save(source.fobj)\n     source.fobj.flush()\n     if return_result:\n         source.fobj.seek(0)\n         result = source.fobj.read()\n     else:\n         result = source.fobj\n     if source.should_close:\n         source.fobj.close()\n     return result", "query": "export to excel"}
{"id": "https://github.com/ransford/sllurp/blob/d744b7e17d7ba64a24d9a31bde6cba65d91ad9b1/sllurp/epc/sgtin_96.py#L27-L71", "method_name": "parse_sgtin_96", "code": "def parse_sgtin_96(sgtin_96):\n     if not sgtin_96:\n         raise Exception(\"Pass in a value.\")\n     if not sgtin_96.startswith(\"30\"):\n         raise Exception(\"Not SGTIN-96.\")\n     binary = \"{0:020b}\".format(int(sgtin_96, 16)).zfill(96)\n     header = int(binary[:8], 2)\n     tag_filter = int(binary[8:11], 2)\n     partition = binary[11:14]\n     partition_value = int(partition, 2)\n     m, l, n, k = SGTIN_96_PARTITION_MAP[partition_value]\n     company_start = 8 + 3 + 3\n     company_end = company_start + m\n     company_data = int(binary[company_start:company_end], 2)\n     if company_data > pow(10, l):\n         raise Exception(\"Company value is too large\")\n     company_prefix = str(company_data).zfill(l)\n     item_start = company_end\n     item_end = item_start + n\n     item_data = binary[item_start:item_end]\n     item_number = int(item_data, 2)\n     item_reference = str(item_number).zfill(k)\n     serial = int(binary[-38:], 2)\n     return {\n         \"header\": header,\n         \"filter\": tag_filter,\n         \"partition\": partition,\n         \"company_prefix\": company_prefix,\n         \"item_reference\": item_reference,\n         \"serial\": serial\n     }", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/util/summary.py#L11-L86", "method_name": "summary", "code": "def summary(model, input_size):\n     def register_hook(module):\n         def hook(module, input, output):\n             class_name = str(module.__class__).split(\".\")[-1].split(\"\"\")[0]\n             module_idx = len(summary)\n             m_key = \"%s-%i\" % (class_name, module_idx + 1)\n             summary[m_key] = OrderedDict()\n             summary[m_key][\"input_shape\"] = list(input[0].size())\n             summary[m_key][\"input_shape\"][0] = -1\n             if isinstance(output, (list, tuple)):\n                 summary[m_key][\"output_shape\"] = [[-1] + list(o.size())[1:] for o in output]\n             else:\n                 summary[m_key][\"output_shape\"] = list(output.size())\n                 summary[m_key][\"output_shape\"][0] = -1\n             params = 0\n             if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n                 params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                 summary[m_key][\"trainable\"] = module.weight.requires_grad\n             if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n                 params += torch.prod(torch.LongTensor(list(module.bias.size())))\n             summary[m_key][\"nb_params\"] = params\n         if (not isinstance(module, nn.Sequential) and\n                 not isinstance(module, nn.ModuleList) and\n                 not (module == model)):\n             hooks.append(module.register_forward_hook(hook))\n     if torch.cuda.is_available():\n         dtype = torch.cuda.FloatTensor\n         model = model.cuda()\n     else:\n         dtype = torch.FloatTensor\n         model = model.cpu()\n     if isinstance(input_size[0], (list, tuple)):\n         x = [Variable(torch.rand(2, *in_size)).type(dtype) for in_size in input_size]\n     else:\n         x = Variable(torch.rand(2, *input_size)).type(dtype)\n     summary = OrderedDict()\n     hooks = []\n     model.apply(register_hook)\n     model(x)\n     for h in hooks:\n         h.remove()\n     print(\"----------------------------------------------------------------\")\n     line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param \n     print(line_new)\n     print(\"================================================================\")\n     total_params = 0\n     trainable_params = 0\n     for layer in summary:\n         line_new = \"{:>20}  {:>25} {:>15}\".format(layer, str(summary[layer][\"output_shape\"]),\n                                                   \"{0:,}\".format(summary[layer][\"nb_params\"]))\n         total_params += summary[layer][\"nb_params\"]\n         if \"trainable\" in summary[layer]:\n             if summary[layer][\"trainable\"] == True:\n                 trainable_params += summary[layer][\"nb_params\"]\n         print(line_new)\n     print(\"================================================================\")\n     print(\"Total params: {0:,}\".format(total_params))\n     print(\"Trainable params: {0:,}\".format(trainable_params))\n     print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n     print(\"----------------------------------------------------------------\")", "query": "print model summary"}
{"id": "https://github.com/ctuning/ck/blob/7e009814e975f8742790d3106340088a46223714/ck/kernel.py#L1770-L1830", "method_name": "copy_to_clipboard", "code": "def copy_to_clipboard(i): \n     s=i[\"string\"]\n     failed=False\n     ee=\"\"\n     try:\n        import pyperclip\n     except Exception as e:\n        ee=format(e)\n        failed=True\n        pass\n     if not failed:\n        pyperclip.copy(s)\n     else:\n        failed=False\n        try:\n           from Tkinter import Tk\n        except ImportError as e:\n           ee=format(e)\n           failed=True\n           pass\n        if failed:\n           failed=False\n           try:\n              from tkinter import Tk\n           except ImportError as e:\n              ee=format(e)\n              failed=True\n              pass\n        if failed:\n           return {\"return\":1, \"error\":\"none of pyperclip/Tkinter/tkinter packages is installed\"}\n        try:\n           r = Tk()\n           r.withdraw()\n           r.clipboard_clear()\n           r.clipboard_append(s)\n           r.destroy()\n        except Exception as e:\n           return {\"return\":1, \"error\":\"problem copying string to clipboard (\"+format(e)+\")\"}\n     return {\"return\":0}", "query": "copy to clipboard"}
{"id": "https://github.com/ethereum/pyethereum/blob/b704a5c6577863edc539a1ec3d2620a443b950fb/ethereum/tools/_solidity.py#L24-L43", "method_name": "get_compiler_path", "code": "def get_compiler_path():\n     given_binary = os.environ.get(\"SOLC_BINARY\")\n     if given_binary:\n         return given_binary\n     for path in os.getenv(\"PATH\", \"\").split(os.pathsep):\n         path = path.strip(\"\"\")\n         executable_path = os.path.join(path, BINARY)\n         if os.path.isfile(executable_path) and os.access(\n                 executable_path, os.X_OK):\n             return executable_path\n     return None", "query": "get executable path"}
{"id": "https://github.com/dade-ai/snipy/blob/408520867179f99b3158b57520e2619f3fecd69b/snipy/img/imageutil.py#L479-L482", "method_name": "_convert_uint8", "code": "def _convert_uint8(im):\n     if im.dtype != np.uint8:\n         im = np.uint8(im * 255)\n     return im", "query": "converting uint8 array to image"}
{"id": "https://github.com/LiftoffSoftware/htmltag/blob/f6989f9a3301e7c96ee613e5dbbe43b2bde615c7/htmltag.py#L442-L448", "method_name": "escape", "code": "def escape(self, string):\n         html_entities = {\"&\": \"&amp;\", \"<\": \"&lt;\", \">\": \"&gt;\"}\n         return HTML(\"\".join(html_entities.get(c, c) for c in string))", "query": "html entities replace"}
{"id": "https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660", "method_name": "checkbox_check", "code": "def checkbox_check(self, force_check=False):\n         if not self.get_attribute(\"checked\"):\n             self.click(force_click=force_check)", "query": "check if a checkbox is checked"}
{"id": "https://github.com/KelSolaar/Umbra/blob/66f45f08d9d723787f1191989f8b0dda84b412ce/umbra/components/factory/script_editor/editor.py#L448-L467", "method_name": "set_file", "code": "def set_file(self, file=None, is_modified=False, is_untitled=False):\n         LOGGER.debug(\"> Setting \"{0}\" editor file.\".format(file))\n         self.__file = file\n         self.__is_untitled = is_untitled\n         self.set_modified(is_modified)\n         self.set_title()\n         return True", "query": "set file attrib hidden"}
{"id": "https://github.com/jcassee/django-geckoboard/blob/6ebdaa86015fe645360abf1ba1290132de4cf6d6/django_geckoboard/decorators.py#L446-L460", "method_name": "_encrypt", "code": "def _encrypt(data):\n     BS = AES.block_size\n     def pad(s):\n         n = BS - len(s) % BS\n         char = chr(n).encode(\"utf8\")\n         return s + n * char\n     password = settings.GECKOBOARD_PASSWORD\n     salt = Random.new().read(BS - len(\"Salted__\"))\n     key, iv = _derive_key_and_iv(password, salt, 32, BS)\n     cipher = AES.new(key, AES.MODE_CBC, iv)\n     encrypted = b\"Salted__\" + salt + cipher.encrypt(pad(data))\n     return base64.b64encode(encrypted)", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/monarch-initiative/dipper/blob/24cc80db355bbe15776edc5c7b41e0886959ba41/scripts/scrape-impc.py#L14-L40", "method_name": "parse_item", "code": "def parse_item(self, response):\n         key = None\n         if re.search(r\"parameterontologies\", response.url) and \\\n             response.css(\"html body div\n                          \".dark::text\").extract() is not None and \\\n             len(response.css(\"html body div\n                              \".dark::text\").extract()) > 0:\n             key = response.css(\"html body div\n                                \".dark::text\").extract()[0]\n         elif (re.search(r\"parameters\", response.url) or re.search(r\"protocol\", response.url)) and \\\n             response.css(\"html body div\n                          \".dark::text\").extract() is not None and \\\n             len(response.css(\"html body div\n                              \".dark::text\").extract()) > 0:\n             key = response.css(\"html body div\n                                \".dark::text\").extract()[0]\n         elif re.search(r\"procedures\", response.url) and \\\n             response.css(\"html body div\n                          \"::text\").extract() is not None and \\\n             len(response.css(\"html body div\n                              \"::text\").extract()) > 0:\n             key = response.css(\"html body div\n                                \"::text\").extract()[0]\n         if key is not None:\n             yield {\n                 key: response.url\n             }", "query": "extract data from html content"}
{"id": "https://github.com/ctuning/ck/blob/7e009814e975f8742790d3106340088a46223714/ck/kernel.py#L1770-L1830", "method_name": "copy_to_clipboard", "code": "def copy_to_clipboard(i): \n     s=i[\"string\"]\n     failed=False\n     ee=\"\"\n     try:\n        import pyperclip\n     except Exception as e:\n        ee=format(e)\n        failed=True\n        pass\n     if not failed:\n        pyperclip.copy(s)\n     else:\n        failed=False\n        try:\n           from Tkinter import Tk\n        except ImportError as e:\n           ee=format(e)\n           failed=True\n           pass\n        if failed:\n           failed=False\n           try:\n              from tkinter import Tk\n           except ImportError as e:\n              ee=format(e)\n              failed=True\n              pass\n        if failed:\n           return {\"return\":1, \"error\":\"none of pyperclip/Tkinter/tkinter packages is installed\"}\n        try:\n           r = Tk()\n           r.withdraw()\n           r.clipboard_clear()\n           r.clipboard_append(s)\n           r.destroy()\n        except Exception as e:\n           return {\"return\":1, \"error\":\"problem copying string to clipboard (\"+format(e)+\")\"}\n     return {\"return\":0}", "query": "copy to clipboard"}
{"id": "https://github.com/aboSamoor/polyglot/blob/d0d2aa8d06cec4e03bd96618ae960030f7069a17/polyglot/decorators.py#L23-L32", "method_name": "memoize", "code": "def memoize(obj):\n   cache = obj.cache = {}\n   @functools.wraps(obj)\n   def memoizer(*args, **kwargs):\n     key = tuple(list(args) + sorted(kwargs.items()))\n     if key not in cache:\n       cache[key] = obj(*args, **kwargs)\n     return cache[key]\n   return memoizer", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568", "method_name": "assert_checked_checkbox", "code": "def assert_checked_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     assert check_box.is_selected(), \"Check box should be selected.\"", "query": "make the checkbox checked"}
{"id": "https://github.com/djtaylor/python-lsbinit/blob/a41fc551226f61ac2bf1b8b0f3f5395db85e75a2/lsbinit/pid.py#L43-L78", "method_name": "ps", "code": "def ps(self):\n         pid = self.get()\n         parent   = None\n         children = []\n         if pid:\n             proc   = Popen([\"ps\", \"-ef\"], stdout=PIPE)\n             for _line in proc.stdout.readlines():\n                 line = self.unicode(_line.rstrip())\n                 this_pid, this_parent = self._ps_extract_pid(line)\n                 try:\n                     if int(pid) == int(this_parent):\n                         children.append(\"{}; [{}]\".format(this_pid.rstrip(), re.sub(\" +\", \" \", line)))\n                     if int(pid) == int(this_pid):\n                         parent = re.sub(\" +\", \" \", line)\n                 except ValueError:\n                     continue\n         return (parent, children)", "query": "get current process id"}
{"id": "https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L471-L484", "method_name": "file_unzipper", "code": "def file_unzipper(directory):\n    debug.log(\"Unzipping directory (%s)...\"%directory)\n    for root, dirs, files in os.walk(directory, topdown=False):\n       if root != \"\":\n          orig_dir = os.getcwd()\n          os.chdir(directory)\n          Popen(\"gunzip -q -f *.gz > /dev/null 2>&1\", shell=True).wait()\n          Popen(\"unzip -qq -o \"*.zip\" > /dev/null 2>&1\", shell=True).wait()\n          Popen(\"rm -f *.zip > /dev/null 2>&1\", shell=True).wait()\n          os.chdir(orig_dir)", "query": "unzipping large files"}
{"id": "https://github.com/SpheMakh/Stimela/blob/292e80461a0c3498da8e7e987e2891d3ae5981ad/stimela/utils/__init__.py#L321-L329", "method_name": "substitute_globals", "code": "def substitute_globals(string, globs=None):\n     sub = set(re.findall(\"\\{(.*?)\\}\", string))\n     globs = globs or inspect.currentframe().f_back.f_globals\n     if sub:\n         for item in map(str, sub):\n             string = string.replace(\"${%s}\"%item, globs[item])\n         return string\n     else:\n         return False", "query": "positions of substrings in string"}
{"id": "https://github.com/Ezhil-Language-Foundation/open-tamil/blob/b7556e88878d29bbc6c944ee17cdd3f75b8ea9f0/solthiruthi/datastore.py#L340-L358", "method_name": "isWord", "code": "def isWord(self,word,ret_ref_trie=False):\n         letters = utf8.get_letters(word)\n         wLen = len(letters)\n         ref_trie = self.trie\n         ref_word_limits = self.word_limits\n         for itr,letter in enumerate(letters):\n             idx = self.getidx( letter )\n             if itr == (wLen-1):\n                 break\n             if not ref_trie[idx][1]:\n                 return False \n             ref_trie = ref_trie[idx][1]\n             ref_word_limits = ref_word_limits[idx][1]\n         if ret_ref_trie:\n             return ref_word_limits[idx][0],ref_trie,ref_word_limits\n         return ref_word_limits[idx][0]", "query": "determine a string is a valid word"}
{"id": "https://github.com/shidenggui/easytrader/blob/e5ae4daeda4ea125763a95b280dd694c7f68257d/easytrader/helpers.py#L137-L139", "method_name": "str2num", "code": "def str2num(num_str, convert_type=\"float\"):\n    num = float(grep_comma(num_str))\n    return num if convert_type == \"float\" else int(num)", "query": "convert string to number"}
{"id": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Main.py#L13-L18", "method_name": "func", "code": "def func(X, y):\n     from sklearn.linear_model import LinearRegression\n     from sklearn.model_selection import cross_val_score\n     model = LinearRegression()\n     model.fit(X, y)\n     return model.predict(X)", "query": "linear regression"}
{"id": "https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/plotter.py#L31-L82", "method_name": "heatmap", "code": "def heatmap(dm, partition=None, cmap=CM.Blues, fontsize=10):\n     assert isinstance(dm, DistanceMatrix)\n     datamax = float(np.abs(dm.values).max())\n     length = dm.shape[0]\n     if partition:\n         sorting = np.array(flatten_list(partition.get_membership()))\n         new_dm = dm.reorder(dm.df.columns[sorting])\n     else:\n         new_dm = dm\n     fig = plt.figure()\n     ax = fig.add_subplot(111)\n     ax.xaxis.tick_top()\n     ax.grid(False)\n     tick_positions = np.array(list(range(length))) + 0.5\n     if fontsize is not None:\n         ax.set_yticks(tick_positions)\n         ax.set_xticks(tick_positions)\n         ax.set_xticklabels(new_dm.df.columns, rotation=90, fontsize=fontsize, ha=\"center\")\n         ax.set_yticklabels(new_dm.df.index, fontsize=fontsize, va=\"center\")\n     cbar_ticks_at = [0, 0.5 * datamax, datamax]\n     cax = ax.imshow(\n         new_dm.values,\n         interpolation=\"nearest\",\n         extent=[0., length, length, 0.],\n         vmin=0,\n         vmax=datamax,\n         cmap=cmap,\n     )\n     cbar = fig.colorbar(cax, ticks=cbar_ticks_at, format=\"%1.2g\")\n     cbar.set_label(\"Distance\")\n     return fig", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/callowayproject/Calloway/blob/d22e98d41fbd298ab6393ba7bd84a75528be9f81/calloway/apps/django_ext/markov.py#L51-L64", "method_name": "random_output", "code": "def random_output(self, max=100):\n         output = []\n         item1 = item2 = MarkovChain.START\n         for i in range(max-3):\n             item3 = self[(item1, item2)].roll()\n             if item3 is MarkovChain.END:\n                 break\n             output.append(item3)\n             item1 = item2\n             item2 = item3\n         return output", "query": "randomly extract x items from a list"}
{"id": "https://github.com/securestate/termineter/blob/d657d25d97c7739e650b951c396404e857e56625/lib/c1218/connection.py#L210-L222", "method_name": "read", "code": "def read(self, size):\n \t\tdata = self.serial_h.read(size)\n \t\tself.logger.debug(\"read data, length: \" + str(len(data)) + \" data: \" + binascii.b2a_hex(data).decode(\"utf-8\"))\n \t\tself.serial_h.write(ACK)\n \t\tif sys.version_info[0] == 2:\n \t\t\tdata = bytearray(data)\n \t\treturn data", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/raymontag/kppy/blob/a43f1fff7d49da1da4b3d8628a1b3ebbaf47f43a/kppy/database.py#L861-L871", "method_name": "_cbc_encrypt", "code": "def _cbc_encrypt(self, content, final_key):\n         aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv)\n         padding = (16 - len(content) % AES.block_size)\n         for _ in range(padding):\n             content += chr(padding).encode()\n         temp = bytes(content)\n         return aes.encrypt(temp)", "query": "aes encryption"}
{"id": "https://github.com/lpantano/seqcluster/blob/774e23add8cd4fdc83d626cea3bd1f458e7d060d/seqcluster/libs/thinkbayes.py#L1530-L1535", "method_name": "EvalBinomialPmf", "code": "def EvalBinomialPmf(k, n, p):\n     return scipy.stats.binom.pmf(k, n, p)", "query": "binomial distribution"}
{"id": "https://github.com/google/prettytensor/blob/75daa0b11252590f548da5647addc0ea610c4c45/prettytensor/tutorial/data_utils.py#L82-L89", "method_name": "permute_data", "code": "def permute_data(arrays, random_state=None):\n   if any(len(a) != len(arrays[0]) for a in arrays):\n     raise ValueError(\"All arrays must be the same length.\")\n   if not random_state:\n     random_state = np.random\n   order = random_state.permutation(len(arrays[0]))\n   return [a[order] for a in arrays]", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/editor.py#L53-L59", "method_name": "unzip", "code": "def unzip(self, in_file, out_file):\n         with ZipFile(in_file) as zf:\n             zf.extract(\"collection.anki2\", path=self.tempdir)\n         shutil.move(os.path.join(self.tempdir, \"collection.anki2\"),\n                     out_file)\n         return out_file", "query": "unzipping large files"}
{"id": "https://github.com/galaxyproject/pulsar/blob/9ab6683802884324652da0a9f0808c7eb59d3ab4/pulsar/client/client.py#L392-L395", "method_name": "_copy", "code": "def _copy(from_path, to_path):\n     message = \"Copying path [%s] to [%s]\"\n     log.debug(message, from_path, to_path)\n     copy(from_path, to_path)", "query": "copying a file to a path"}
{"id": "https://github.com/divio/django-filer/blob/946629087943d41eff290f07bfdf240b8853dd88/filer/admin/folderadmin.py#L1028-L1045", "method_name": "_copy_file", "code": "def _copy_file(self, file_obj, destination, suffix, overwrite):\n         if overwrite:\n             raise NotImplementedError\n         filename = self._generate_new_filename(file_obj.file.name, suffix)\n         file_obj.pk = None\n         file_obj.id = None\n         file_obj.save()\n         file_obj.folder = destination\n         file_obj._file_data_changed_hint = False  \n         file_obj.file = file_obj._copy_file(filename)\n         file_obj.original_filename = self._generate_new_filename(file_obj.original_filename, suffix)\n         file_obj.save()", "query": "copying a file to a path"}
{"id": "https://github.com/wtsi-hgi/python-hgijson/blob/6e8ccb562eabcaa816a136268a16504c2e0d4664/hgijson/json_converters/_converters.py#L61-L69", "method_name": "deserialize", "code": "def deserialize(self, deserializable: PrimitiveJsonType) -> Optional[SerializableType]:\n         if not isinstance(self._decoder, ParsedJSONDecoder):\n             json_as_string = json.dumps(deserializable)\n             return self._decoder.decode(json_as_string)\n         else:\n             return self._decoder.decode_parsed(deserializable)", "query": "deserialize json"}
{"id": "https://github.com/s1s1ty/py-jsonq/blob/9625597a2578bddcbed4e540174d5253b1fc3b75/pyjsonq/query.py#L46-L60", "method_name": "__parse_json_file", "code": "def __parse_json_file(self, file_path):\n         if file_path == \"\" or os.path.splitext(file_path)[1] != \".json\":\n             raise IOError(\"Invalid Json file\")\n         with open(file_path) as json_file:\n             self._raw_data = json.load(json_file)\n         self._json_data = copy.deepcopy(self._raw_data)", "query": "parse json file"}
{"id": "https://github.com/limodou/uliweb/blob/34472f25e4bc0b954a35346672f94e84ef18b076/uliweb/core/template.py#L489-L524", "method_name": "reindent", "code": "def reindent(text, filename): \n     new_lines=[] \n     k=0 \n     c=0 \n     for n, raw_line in enumerate(text.splitlines()): \n         line=raw_line.strip() \n         if not line or line[0]==\"\n             new_lines.append(line) \n             continue \n         line3 = line[:3] \n         line4 = line[:4] \n         line5 = line[:5] \n         line6 = line[:6] \n         line7 = line[:7] \n         if line3==\"if \" or line4 in (\"def \", \"for \", \"try:\") or\\ \n             line6==\"while \" or line6==\"class \" or line5==\"with \": \n             new_lines.append(\"    \"*k+line) \n             k += 1 \n             continue \n         elif line5==\"elif \" or line5==\"else:\" or    \\ \n             line7==\"except:\" or line7==\"except \" or \\ \n             line7==\"finally:\": \n                 c = k-1 \n                 if c<0: \n                     raise ParseError(\"Extra pass founded on line %s:%d\" % (filename, n)) \n                 new_lines.append(\"    \"*c+line) \n                 continue \n         else: \n             new_lines.append(\"    \"*k+line) \n         if line==\"pass\" or line5==\"pass \": \n             k-=1 \n         if k<0: k = 0 \n     text=\"\n\".join(new_lines) \n     return text", "query": "read text file line by line"}
{"id": "https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/tools/model_dump.py#L99-L127", "method_name": "read_properties", "code": "def read_properties(entry):\n     stream = entry.get(\"properties\")\n     if stream is None:\n         raise Exception(\"can not find properties\")\n     s = stream.open()\n     f = BytesIO(s.read())\n     byte_order = read_u8(f)\n     if byte_order != 0x4c:\n         raise NotImplementedError(\"be byteorder\")\n     version = read_u8(f)\n     entry_count = read_u16le(f)\n     props = []\n     for i in range(entry_count):\n         pid = read_u16le(f)\n         format = read_u16le(f)\n         byte_size = read_u16le(f)\n         props.append([pid, format, byte_size])\n     property_entries = {}\n     for pid, format, byte_size in props:\n         data = f.read(byte_size)\n         property_entries[pid] = data\n     return property_entries", "query": "read properties file"}
{"id": "https://github.com/frictionlessdata/datapackage-pipelines/blob/3a34bbdf042d13c3bec5eef46ff360ee41403874/datapackage_pipelines/lib/dump/to_path.py#L16-L25", "method_name": "write_file_to_output", "code": "def write_file_to_output(self, filename, path):\n         path = os.path.join(self.out_path, path)\n         if self.add_filehash_to_path and os.path.exists(path):\n             return\n         path_part = os.path.dirname(path)\n         PathDumper.__makedirs(path_part)\n         shutil.copy(filename, path)\n         os.chmod(path, 0o666)\n         return path", "query": "copying a file to a path"}
{"id": "https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L37-L57", "method_name": "write_csv_header", "code": "def write_csv_header(mol, csv_writer):\n     line = []\n     line.append(\"id\")\n     line.append(\"status\")\n     queryList = mol.properties.keys()\n     for queryLabel in queryList:\n         line.append(queryLabel)\n     csv_writer.writerow(line)", "query": "write csv"}
{"id": "https://github.com/scheibler/khard/blob/0f69430c2680f1ff5f073a977a3c5b753b96cc17/khard/helpers.py#L81-L107", "method_name": "string_to_date", "code": "def string_to_date(input):\n     for format_string in (\"--%m%d\", \"--%m-%d\", \"%Y%m%d\", \"%Y-%m-%d\",\n                           \"%Y%m%dT%H%M%S\", \"%Y-%m-%dT%H:%M:%S\",\n                           \"%Y%m%dT%H%M%SZ\", \"%Y-%m-%dT%H:%M:%SZ\"):\n         try:\n             return datetime.strptime(input, format_string)\n         except ValueError:\n             pass\n     for format_string in (\"%Y%m%dT%H%M%S%z\", \"%Y-%m-%dT%H:%M:%S%z\"):\n         try:\n             return datetime.strptime(\"\".join(input.rsplit(\":\", 1)),\n                                      format_string)\n         except ValueError:\n             pass\n     raise ValueError", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/tasks.py#L4149-L4153", "method_name": "set_workdir", "code": "def set_workdir(self, workdir, chroot=False):\n         super().set_workdir(workdir, chroot=chroot)\n         self.output_file = self.log_file", "query": "set working directory"}
{"id": "https://github.com/couchbase/couchbase-python-client/blob/a7bada167785bf79a29c39f820d932a433a6a535/couchbase/connstr.py#L126-L143", "method_name": "encode", "code": "def encode(self):\n         opt_dict = {}\n         for k, v in self.options.items():\n             opt_dict[k] = v[0]\n         ss = \"{0}://{1}\".format(self.scheme, \",\".join(self.hosts))\n         if self.bucket:\n             ss += \"/\" + self.bucket\n         ss += \"?\" + urlencode(opt_dict).replace(\"%2F\", \"/\")\n         return ss", "query": "encode url"}
{"id": "https://github.com/rckclmbr/pyportify/blob/696a1caad8a47b191f3bec44cc8fc3c437779512/pyportify/google.py#L98-L112", "method_name": "_http_post", "code": "async def _http_post(self, url, data):\n         data = json.dumps(data)\n         headers = {\"Authorization\": \"GoogleLogin auth={0}\".format(self.token),\n                    \"Content-type\": \"application/json\"}\n         res = await self.session.request(\n             \"POST\",\n             FULL_SJ_URL + url,\n             data=data,\n             headers=headers,\n             params={\"tier\": \"aa\",\n                     \"hl\": \"en_US\",\n                     \"dv\": 0,\n                     \"alt\": \"json\"})\n         ret = await res.json()\n         return ret", "query": "httpclient post json"}
{"id": "https://github.com/DeepHorizons/iarm/blob/b913c9fd577b793a6bbced78b78a5d8d7cd88de4/iarm/arm_instructions/_meta.py#L126-L132", "method_name": "convert_to_integer", "code": "def convert_to_integer(self, str):\n         if str.startswith(\"0x\") or str.startswith(\"0X\"):\n             return int(str, 16)\n         elif str.startswith(\"2_\"):\n             return int(str[2:], 2)\n         else:\n             return int(str)", "query": "convert int to string"}
{"id": "https://github.com/jcassee/django-geckoboard/blob/6ebdaa86015fe645360abf1ba1290132de4cf6d6/django_geckoboard/decorators.py#L446-L460", "method_name": "_encrypt", "code": "def _encrypt(data):\n     BS = AES.block_size\n     def pad(s):\n         n = BS - len(s) % BS\n         char = chr(n).encode(\"utf8\")\n         return s + n * char\n     password = settings.GECKOBOARD_PASSWORD\n     salt = Random.new().read(BS - len(\"Salted__\"))\n     key, iv = _derive_key_and_iv(password, salt, 32, BS)\n     cipher = AES.new(key, AES.MODE_CBC, iv)\n     encrypted = b\"Salted__\" + salt + cipher.encrypt(pad(data))\n     return base64.b64encode(encrypted)", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/whiteclover/dbpy/blob/3d9ce85f55cfb39cced22081e525f79581b26b3a/db/pymysql/connection.py#L53-L56", "method_name": "connect", "code": "def connect(self):\n         self.close()\n         self._connect = pymysql.connect(**self._db_options)\n         self._connect.autocommit(True)", "query": "connect to sql"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568", "method_name": "assert_checked_checkbox", "code": "def assert_checked_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     assert check_box.is_selected(), \"Check box should be selected.\"", "query": "check if a checkbox is checked"}
{"id": "https://github.com/pricingassistant/mongokat/blob/61eaf4bc1c4cc359c6f9592ec97b9a04d9561411/mongokat/collection.py#L136-L137", "method_name": "distinct", "code": "def distinct(self, *args, **kwargs):\n        return self._collection_with_options(kwargs).distinct(*args, **kwargs)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/sunlightlabs/django-mediasync/blob/aa8ce4cfff757bbdb488463c64c0863cca6a1932/mediasync/__init__.py#L33-L38", "method_name": "compress", "code": "def compress(s):\n     zbuf = cStringIO.StringIO()\n     zfile = gzip.GzipFile(mode=\"wb\", compresslevel=6, fileobj=zbuf)\n     zfile.write(s)\n     zfile.close()\n     return zbuf.getvalue()", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/FaradayRF/faradayio/blob/6cf3af88bb4a83e5d2036e5cbdfaf8f0f01500bb/faradayio/faraday.py#L32-L56", "method_name": "send", "code": "def send(self, msg):\n         slipDriver = sliplib.Driver()\n         slipData = slipDriver.send(msg)\n         res = self._serialPort.write(slipData)\n         return res", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/ToucanToco/toucan-data-sdk/blob/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a/toucan_data_sdk/sdk.py#L173-L191", "method_name": "extract_zip", "code": "def extract_zip(zip_file_path):\n     dfs = {}\n     with zipfile.ZipFile(zip_file_path, mode=\"r\") as z_file:\n         names = z_file.namelist()\n         for name in names:\n             content = z_file.read(name)\n             _, tmp_file_path = tempfile.mkstemp()\n             try:\n                 with open(tmp_file_path, \"wb\") as tmp_file:\n                     tmp_file.write(content)\n                 dfs[name] = joblib.load(tmp_file_path)\n             finally:\n                 shutil.rmtree(tmp_file_path, ignore_errors=True)\n     return dfs", "query": "extract zip file recursively"}
{"id": "https://github.com/mapillary/mapillary_tools/blob/816785e90c589cae6e8e34a5530ce8417d29591c/mapillary_tools/process_csv.py#L175-L184", "method_name": "read_csv", "code": "def read_csv(csv_path, delimiter=\",\", header=False):\n     csv_data = []\n     with open(csv_path, \"r\") as csvfile:\n         csvreader = csv.reader(csvfile, delimiter=delimiter)\n         if header:\n             next(csvreader, None)\n         csv_data = zip(*csvreader)\n     return csv_data", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/mothur.py#L391-L399", "method_name": "_set_WorkingDir", "code": "def _set_WorkingDir(self, path):\n         self._curr_working_dir = path\n         try:\n             mkdir(self.WorkingDir)\n         except OSError:\n             pass", "query": "set working directory"}
{"id": "https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/api/model.py#L37-L51", "method_name": "summary", "code": "def summary(self, input_size=None, hashsummary=False):\n         if input_size is None:\n             print(self)\n             print(\"-\" * 120)\n             number = sum(p.numel() for p in self.model.parameters())\n             print(\"Number of model parameters: {:,}\".format(number))\n             print(\"-\" * 120)\n         else:\n             summary(self, input_size)\n         if hashsummary:\n             for idx, hashvalue in enumerate(self.hashsummary()):\n                 print(f\"{idx}: {hashvalue}\")", "query": "print model summary"}
{"id": "https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L275-L283", "method_name": "get_prob", "code": "def get_prob(self):\n         if self.total < 5:\n             return 0.\n         a, b = self.dup + 1, self.nodup + 1\n         n = a + b\n         p = a / n\n         q = b / n\n         return p - 1.96 * math.sqrt(p * q / n)", "query": "binomial distribution"}
{"id": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/custom_destinations.py#L85-L209", "method_name": "ParseFileObject", "code": "def ParseFileObject(self, parser_mediator, file_object):\n     file_entry = parser_mediator.GetFileEntry()\n     display_name = parser_mediator.GetDisplayName()\n     file_header_map = self._GetDataTypeMap(\"custom_file_header\")\n     try:\n       file_header, file_offset = self._ReadStructureFromFileObject(\n           file_object, 0, file_header_map)\n     except (ValueError, errors.ParseError) as exception:\n       raise errors.UnableToParseFile((\n           \"Invalid Custom Destination: {0:s} - unable to parse file header \"\n           \"with error: {1!s}\").format(display_name, exception))\n     if file_header.unknown1 != 2:\n       raise errors.UnableToParseFile((\n           \"Unsupported Custom Destination file: {0:s} - invalid unknown1: \"\n           \"{1:d}.\").format(display_name, file_header.unknown1))\n     if file_header.header_values_type > 2:\n       raise errors.UnableToParseFile((\n           \"Unsupported Custom Destination file: {0:s} - invalid header value \"\n           \"type: {1:d}.\").format(display_name, file_header.header_values_type))\n     if file_header.header_values_type == 0:\n       data_map_name = \"custom_file_header_value_type_0\"\n     else:\n       data_map_name = \"custom_file_header_value_type_1_or_2\"\n     file_header_value_map = self._GetDataTypeMap(data_map_name)\n     try:\n       _, value_data_size = self._ReadStructureFromFileObject(\n           file_object, file_offset, file_header_value_map)\n     except (ValueError, errors.ParseError) as exception:\n       raise errors.UnableToParseFile((\n           \"Invalid Custom Destination: {0:s} - unable to parse file header \"\n           \"value with error: {1!s}\").format(display_name, exception))\n     file_offset += value_data_size\n     file_size = file_object.get_size()\n     remaining_file_size = file_size - file_offset\n     entry_header_map = self._GetDataTypeMap(\"custom_entry_header\")\n     file_footer_map = self._GetDataTypeMap(\"custom_file_footer\")\n     first_guid_checked = False\n     while remaining_file_size > 4:\n       try:\n         entry_header, entry_data_size = self._ReadStructureFromFileObject(\n             file_object, file_offset, entry_header_map)\n       except (ValueError, errors.ParseError) as exception:\n         if not first_guid_checked:\n           raise errors.UnableToParseFile((\n               \"Invalid Custom Destination file: {0:s} - unable to parse \"\n               \"entry header with error: {1!s}\").format(\n                   display_name, exception))\n         parser_mediator.ProduceExtractionWarning(\n             \"unable to parse entry header with error: {0!s}\".format(\n                 exception))\n         break\n       if entry_header.guid != self._LNK_GUID:\n         if not first_guid_checked:\n           raise errors.UnableToParseFile((\n               \"Unsupported Custom Destination file: {0:s} - invalid entry \"\n               \"header signature offset: 0x{1:08x}.\").format(\n                   display_name, file_offset))\n         try:\n           file_footer, _ = self._ReadStructureFromFileObject(\n               file_object, file_offset, file_footer_map)\n           if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n             parser_mediator.ProduceExtractionWarning(\n                 \"invalid entry header signature at offset: 0x{0:08x}\".format(\n                     file_offset))\n         except (ValueError, errors.ParseError) as exception:\n           parser_mediator.ProduceExtractionWarning((\n               \"unable to parse footer at offset: 0x{0:08x} with error: \"\n               \"{1!s}\").format(file_offset, exception))\n           break\n         break\n       first_guid_checked = True\n       file_offset += entry_data_size\n       remaining_file_size -= entry_data_size\n       lnk_file_size = self._ParseLNKFile(\n           parser_mediator, file_entry, file_offset, remaining_file_size)\n       file_offset += lnk_file_size\n       remaining_file_size -= lnk_file_size\n     try:\n       file_footer, _ = self._ReadStructureFromFileObject(\n           file_object, file_offset, file_footer_map)\n       if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n         parser_mediator.ProduceExtractionWarning(\n             \"invalid footer signature at offset: 0x{0:08x}\".format(file_offset))\n     except (ValueError, errors.ParseError) as exception:\n       parser_mediator.ProduceExtractionWarning((\n           \"unable to parse footer at offset: 0x{0:08x} with error: \"\n           \"{1!s}\").format(file_offset, exception))", "query": "parse binary file to custom class"}
{"id": "https://github.com/djf604/chunky-pipes/blob/cd5b2a31ded28ab949da6190854228f1c8897882/chunkypipes/util/decorators/__init__.py#L6-L14", "method_name": "timed", "code": "def timed(func):\n     @functools.wraps(func)\n     def timer(*args, **kwargs):\n         start_time = time()\n         result = func(*args, **kwargs)\n         elapsed_time = str(timedelta(seconds=int(time() - start_time)))\n         print(\"Elapsed time is {}\".format(elapsed_time))\n         return result\n     return timer", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/bram85/topydo/blob/b59fcfca5361869a6b78d4c9808c7c6cd0a18b58/topydo/lib/Utils.py#L28-L46", "method_name": "date_string_to_date", "code": "def date_string_to_date(p_date):\n     result = None\n     if p_date:\n         parsed_date = re.match(r\"(\\d{4})-(\\d{2})-(\\d{2})\", p_date)\n         if parsed_date:\n             result = date(\n                 int(parsed_date.group(1)),  \n                 int(parsed_date.group(2)),  \n                 int(parsed_date.group(3))   \n             )\n         else:\n             raise ValueError\n     return result", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/sparklingpandas/sparklingpandas/blob/7d549df4348c979042b683c355aa778fc6d3a768/sparklingpandas/pcontext.py#L68-L155", "method_name": "read_csv", "code": "def read_csv(self, file_path, use_whole_file=False, names=None, skiprows=0,\n                  *args, **kwargs):\n         def csv_file(partition_number, files):\n             file_count = 0\n             for _, contents in files:\n                 if partition_number == 0 and file_count == 0 and _skiprows > 0:\n                     yield pandas.read_csv(\n                         sio(contents), *args,\n                         header=None,\n                         names=mynames,\n                         skiprows=_skiprows,\n                         **kwargs)\n                 else:\n                     file_count += 1\n                     yield pandas.read_csv(\n                         sio(contents), *args,\n                         header=None,\n                         names=mynames,\n                         **kwargs)\n         def csv_rows(partition_number, rows):\n             in_str = \"\n\".join(rows)\n             if partition_number == 0:\n                 return iter([\n                     pandas.read_csv(\n                         sio(in_str), *args, header=None,\n                         names=mynames,\n                         skiprows=_skiprows,\n                         **kwargs)])\n             else:\n                 return iter([pandas.read_csv(sio(in_str), *args, header=None,\n                                              names=mynames, **kwargs)])\n         mynames = None\n         _skiprows = skiprows\n         if names:\n             mynames = names\n         else:\n             first_line = self.spark_ctx.textFile(file_path).first()\n             frame = pandas.read_csv(sio(first_line), **kwargs)\n             mynames = list(frame.columns)\n             _skiprows += 1\n         if use_whole_file:\n             return self.from_pandas_rdd(\n                 self.spark_ctx.wholeTextFiles(file_path)\n                 .mapPartitionsWithIndex(csv_file))\n         else:\n             return self.from_pandas_rdd(\n                 self.spark_ctx.textFile(file_path)\n                     .mapPartitionsWithIndex(csv_rows))", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/date_interval.py#L266-L267", "method_name": "to_string", "code": "def to_string(self):\n        return \"-\".join([d.strftime(\"%Y-%m-%d\") for d in (self.date_a, self.date_b)])", "query": "string to date"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/rnc_db.py#L1649-L1908", "method_name": "_connect", "code": "def _connect(self,\n                  engine: str = None,\n                  interface: str = None,\n                  host: str = None,\n                  port: int = None,\n                  database: str = None,\n                  driver: str = None,\n                  dsn: str = None,\n                  odbc_connection_string: str = None,\n                  user: str = None,\n                  password: str = None,\n                  autocommit: bool = True,\n                  charset: str = \"utf8\",\n                  use_unicode: bool = True) -> bool:\n         if engine == ENGINE_MYSQL:\n             self.flavour = MySQL()\n             self.schema = database\n         elif engine == ENGINE_SQLSERVER:\n             self.flavour = SQLServer()\n             if database:\n                 self.schema = database\n             else:\n                 self.schema = \"dbo\"  \n         elif engine == ENGINE_ACCESS:\n             self.flavour = Access()\n             self.schema = \"dbo\"  \n         else:\n             raise ValueError(\"Unknown engine\")\n         if interface is None:\n             if engine == ENGINE_MYSQL:\n                 interface = INTERFACE_MYSQL\n             else:\n                 interface = INTERFACE_ODBC\n         if port is None:\n             if engine == ENGINE_MYSQL:\n                 port = 3306\n             elif engine == ENGINE_SQLSERVER:\n                 port = 1433\n         if driver is None:\n             if engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n                 driver = \"{MySQL ODBC 5.1 Driver}\"\n         self._engine = engine\n         self._interface = interface\n         self._server = host\n         self._port = port\n         self._database = database\n         self._user = user\n         self._password = password\n         self._charset = charset\n         self._use_unicode = use_unicode\n         self.autocommit = autocommit\n         log.info(\n             \"Opening database: engine={e}, interface={i}, \"\n             \"use_unicode={u}, autocommit={a}\".format(\n                 e=engine, i=interface, u=use_unicode, a=autocommit))\n         if interface == INTERFACE_MYSQL:\n             if pymysql:\n                 self.db_pythonlib = PYTHONLIB_PYMYSQL\n             elif MySQLdb:\n                 self.db_pythonlib = PYTHONLIB_MYSQLDB\n             else:\n                 raise ImportError(_MSG_MYSQL_DRIVERS_UNAVAILABLE)\n         elif interface == INTERFACE_ODBC:\n             if not pyodbc:\n                 raise ImportError(_MSG_PYODBC_UNAVAILABLE)\n             self.db_pythonlib = PYTHONLIB_PYODBC\n         elif interface == INTERFACE_JDBC:\n             if not jaydebeapi:\n                 raise ImportError(_MSG_JDBC_UNAVAILABLE)\n             if host is None:\n                 raise ValueError(\"Missing host parameter\")\n             if port is None:\n                 raise ValueError(\"Missing port parameter\")\n             if user is None:\n                 raise ValueError(\"Missing user parameter\")\n             self.db_pythonlib = PYTHONLIB_JAYDEBEAPI\n         else:\n             raise ValueError(\"Unknown interface\")\n         if engine == ENGINE_MYSQL and interface == INTERFACE_MYSQL:\n             datetimetype = datetime.datetime  \n             converters = mysql.converters.conversions.copy()\n             converters[datetimetype] = datetime2literal_rnc\n             log.info(\n                 \"{i} connect: host={h}, port={p}, user={u}, \"\n                 \"database={d}\".format(\n                     i=interface, h=host, p=port, u=user, d=database))\n             self.db = mysql.connect(\n                 host=host,\n                 port=port,\n                 user=user,\n                 passwd=password,\n                 db=database,\n                 charset=charset,\n                 use_unicode=use_unicode,\n                 conv=converters\n             )\n             self.db.autocommit(autocommit)\n         elif engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n             log.info(\n                 \"ODBC connect: DRIVER={dr};SERVER={s};PORT={p};\"\n                 \"DATABASE={db};USER={u};PASSWORD=[censored]\".format(\n                     dr=driver, s=host, p=port,\n                     db=database, u=user))\n             dsn = (\n                 \"DRIVER={0};SERVER={1};PORT={2};DATABASE={3};\"\n                 \"USER={4};PASSWORD={5}\".format(driver, host, port, database,\n                                                user, password)\n             )\n             self.db = pyodbc.connect(dsn)\n             self.db.autocommit = autocommit\n         elif engine == ENGINE_MYSQL and interface == INTERFACE_JDBC:\n             jclassname = \"com.mysql.jdbc.Driver\"\n             url = \"jdbc:mysql://{host}:{port}/{database}\".format(\n                 host=host, port=port, database=database)\n             driver_args = [url, user, password]\n             jars = None\n             libs = None\n             log.info(\n                 \"JDBC connect: jclassname={jclassname}, \"\n                 \"url={url}, user={user}, password=[censored]\".format(\n                     jclassname=jclassname,\n                     url=url,\n                     user=user,\n                 )\n             )\n             self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n         elif engine == ENGINE_SQLSERVER and interface == INTERFACE_ODBC:\n             if odbc_connection_string:\n                 log.info(\"Using raw ODBC connection string [censored]\")\n                 connectstring = odbc_connection_string\n             elif dsn:\n                 log.info(\n                     \"ODBC connect: DSN={dsn};UID={u};PWD=[censored]\".format(\n                         dsn=dsn, u=user))\n                 connectstring = \"DSN={};UID={};PWD={}\".format(dsn, user,\n                                                               password)\n             else:\n                 log.info(\n                     \"ODBC connect: DRIVER={dr};SERVER={s};DATABASE={db};\"\n                     \"UID={u};PWD=[censored]\".format(\n                         dr=driver, s=host, db=database, u=user))\n                 connectstring = (\n                     \"DRIVER={};SERVER={};DATABASE={};UID={};PWD={}\".format(\n                         driver, host, database, user, password)\n                 )\n             self.db = pyodbc.connect(connectstring, unicode_results=True)\n             self.db.autocommit = autocommit\n         elif engine == ENGINE_SQLSERVER and interface == INTERFACE_JDBC:\n             jclassname = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n             urlstem = \"jdbc:sqlserver://{host}:{port};\".format(\n                 host=host,\n                 port=port\n             )\n             nvp = {}\n             if database:\n                 nvp[\"databaseName\"] = database\n             nvp[\"user\"] = user\n             nvp[\"password\"] = password\n             nvp[\"responseBuffering\"] = \"adaptive\"  \n             nvp[\"selectMethod\"] = \"cursor\"  \n             url = urlstem + \";\".join(\n                 \"{}={}\".format(x, y) for x, y in nvp.items())\n             nvp[\"password\"] = \"[censored]\"\n             url_censored = urlstem + \";\".join(\n                 \"{}={}\".format(x, y) for x, y in nvp.items())\n             log.info(\n                 \"jdbc connect: jclassname={jclassname}, url = {url}\".format(\n                     jclassname=jclassname,\n                     url=url_censored\n                 )\n             )\n             driver_args = [url]\n             jars = None\n             libs = None\n             self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n         elif engine == ENGINE_ACCESS and interface == INTERFACE_ODBC:\n             dsn = \"DSN={}\".format(dsn)\n             log.info(\"ODBC connect: DSN={}\", dsn)\n             self.db = pyodbc.connect(dsn)\n             self.db.autocommit = autocommit\n         else:\n             raise ValueError(\n                 \"Unknown \"engine\"/\"interface\" combination: {}/{}\".format(\n                     engine, interface\n                 )\n             )\n         return True", "query": "connect to sql"}
{"id": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341", "method_name": "check_checkbox", "code": "def check_checkbox(step, value):\n     with AssertContextManager(step):\n         check_box = find_field(world.browser, \"checkbox\", value)\n         if not check_box.is_selected():\n             check_box.click()", "query": "check if a checkbox is checked"}
{"id": "https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/operation/datashader.py#L932-L936", "method_name": "uint32_to_uint8", "code": "def uint32_to_uint8(cls, img):\n         return np.flipud(img.view(dtype=np.uint8).reshape(img.shape + (4,)))", "query": "converting uint8 array to image"}
{"id": "https://github.com/ChrisCummins/labm8/blob/dd10d67a757aefb180cb508f86696f99440c94f5/text.py#L40-L52", "method_name": "get_substring_idxs", "code": "def get_substring_idxs(substr, string):\n     return [match.start() for match in re.finditer(substr, string)]", "query": "positions of substrings in string"}
{"id": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/utils/http.py#L175-L196", "method_name": "setcookie", "code": "def setcookie(self, key, value, max_age=None, expires=None, path=\"/\", domain=None, secure=None, httponly=False):\n         newcookie = Morsel()\n         newcookie.key = key\n         newcookie.value = value\n         newcookie.coded_value = value\n         if max_age is not None:\n             newcookie[\"max-age\"] = max_age\n         if expires is not None:\n             newcookie[\"expires\"] = expires\n         if path is not None:\n             newcookie[\"path\"] = path\n         if domain is not None:\n             newcookie[\"domain\"] = domain\n         if secure:\n             newcookie[\"secure\"] = secure\n         if httponly:\n             newcookie[\"httponly\"] = httponly\n         self.sent_cookies = [c for c in self.sent_cookies if c.key != key]\n         self.sent_cookies.append(newcookie)", "query": "create cookie"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547", "method_name": "check_checkbox", "code": "def check_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     if not check_box.is_selected():\n         check_box.click()", "query": "make the checkbox checked"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/referenceanalysis.py#L85-L88", "method_name": "toPdf", "code": "def toPdf(self):\n         html = safe_unicode(self.template()).encode(\"utf-8\")\n         pdf_data = createPdf(html)\n         return pdf_data", "query": "html encode string"}
{"id": "https://github.com/molmod/molmod/blob/a7b5b4364ed514ad4c465856c05b5eda1cb561e0/molmod/io/cp2k.py#L204-L213", "method_name": "readline", "code": "def readline(self, f):\n         while True:\n             line = f.readline()\n             if len(line) == 0:\n                 raise EOFError\n             line = line[:line.find(\"\n             line = line.strip()\n             if len(line) > 0:\n                 return line", "query": "read text file line by line"}
{"id": "https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/data/bcolz_daily_bars.py#L209-L237", "method_name": "write_csvs", "code": "def write_csvs(self,\n                    asset_map,\n                    show_progress=False,\n                    invalid_data_behavior=\"warn\"):\n         read = partial(\n             read_csv,\n             parse_dates=[\"day\"],\n             index_col=\"day\",\n             dtype=self._csv_dtypes,\n         )\n         return self.write(\n             ((asset, read(path)) for asset, path in iteritems(asset_map)),\n             assets=viewkeys(asset_map),\n             show_progress=show_progress,\n             invalid_data_behavior=invalid_data_behavior,\n         )", "query": "write csv"}
{"id": "https://github.com/profusion/sgqlc/blob/684afb059c93f142150043cafac09b7fd52bfa27/examples/github/github-agile-dashboard.py#L35-L39", "method_name": "json_pretty_print", "code": "def json_pretty_print(d, file=None):\n     args = {\"sort_keys\": True, \"indent\": 2, \"separators\": (\",\", \": \")}\n     if file:\n         return json.dump(d, file, **args)\n     return json.dumps(d, **args)", "query": "pretty print json"}
{"id": "https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/plugin_html.py#L177-L183", "method_name": "tag_to_dict", "code": "def tag_to_dict(html):\n     element = document_fromstring(html).xpath(\"//html/body/child::*\")[0]\n     attributes = dict(element.attrib)\n     attributes[\"text\"] = element.text_content()\n     return attributes", "query": "reading element from html - <td>"}
{"id": "https://github.com/limpyd/redis-limpyd-jobs/blob/264c71029bad4377d6132bf8bb9c55c44f3b03a2/limpyd_jobs/workers.py#L492-L504", "method_name": "requeue_job", "code": "def requeue_job(self, job, queue, priority, delayed_for=None):\n         job.requeue(queue_name=queue._cached_name,\n                     priority=priority,\n                     delayed_for=delayed_for,\n                     queue_model=self.queue_model)\n         if hasattr(job, \"on_requeued\"):\n             job.on_requeued(queue)\n         self.log(self.job_requeue_message(job, queue))", "query": "priority queue"}
{"id": "https://github.com/jaredLunde/vital-tools/blob/ea924c9bbb6ec22aa66f8095f018b1ee0099ac04/vital/security/__init__.py#L79-L98", "method_name": "aes_encrypt", "code": "def aes_encrypt(value, secret, block_size=AES.block_size):\n     iv = os.urandom(block_size * 2)\n     cipher = AES.new(secret[:32], AES.MODE_CFB, iv[:block_size])\n     return b\"%s%s\" % (iv, cipher.encrypt(value))", "query": "aes encryption"}
{"id": "https://github.com/hackedd/gw2api/blob/5543a78e6e3ed0573b7e84c142c44004b4779eac/gw2api/map.py#L50-L105", "method_name": "maps", "code": "def maps(map_id=None, lang=\"en\"):\n     if map_id:\n         cache_name = \"maps.%s.%s.json\" % (map_id, lang)\n         params = {\"map_id\": map_id, \"lang\": lang}\n     else:\n         cache_name = \"maps.%s.json\" % lang\n         params = {\"lang\": lang}\n     data = get_cached(\"maps.json\", cache_name, params=params).get(\"maps\")\n     return data.get(str(map_id)) if map_id else data", "query": "map to json"}
{"id": "https://github.com/sarugaku/virtenv/blob/fc42a9d8dc9f1821d3893899df78e08a081f6ca3/virtenv_cli.py#L20-L28", "method_name": "which", "code": "def which(name):\n     for p in os.environ[\"PATH\"].split(os.pathsep):\n         exe = os.path.join(p, name)\n         if is_executable(exe):\n             return os.path.abspath(exe)\n         for ext in [\"\"] + os.environ.get(\"PATHEXT\", \"\").split(os.pathsep):\n             exe = \"{}{}\".format(exe, ext.lower())\n             if is_executable(exe):\n                 return os.path.abspath(exe)", "query": "get executable path"}
{"id": "https://github.com/dwillis/python-espncricinfo/blob/96469e39f309e28586fcec40cc6a20b2fddacff7/espncricinfo/player.py#L44-L50", "method_name": "get_html", "code": "def get_html(self):\n         r = requests.get(self.url)\n         if r.status_code == 404:\n             raise PlayerNotFoundError\n         else:\n             soup = BeautifulSoup(r.text, \"html.parser\")\n             return soup.find(\"div\", class_=\"pnl490M\")", "query": "get html of website"}
{"id": "https://github.com/google/grumpy/blob/3ec87959189cfcdeae82eb68a47648ac25ceb10b/third_party/pypy/_struct.py#L101-L108", "method_name": "pack_unsigned_int", "code": "def pack_unsigned_int(number, size, le):\n   if not isinstance(number, int):\n     raise StructError(\"argument for i,I,l,L,q,Q,h,H must be integer\")\n   if number < 0:\n     raise TypeError(\"can\"t convert negative long to unsigned\")\n   if number > (1 << (8 * size)) - 1:\n     raise OverflowError(\"Number:%i too large to convert\" % number)\n   return pack_int(number, size, le)", "query": "convert string to number"}
{"id": "https://github.com/alejandroautalan/pygubu/blob/41c8fb37ef973736ec5d68cbe1cd4ecb78712e40/pygubudesigner/main.py#L491-L495", "method_name": "do_save", "code": "def do_save(self, fname):\n         self.save_file(fname)\n         self.currentfile = fname\n         self.is_changed = False\n         logger.info(_(\"Project saved to {0}\").format(fname))", "query": "save list to file"}
{"id": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/engine/zeromq_queue.py#L161-L171", "method_name": "_SetSocketTimeouts", "code": "def _SetSocketTimeouts(self):\n     timeout = int(self.timeout_seconds * 1000)\n     receive_timeout = min(\n         self._ZMQ_SOCKET_RECEIVE_TIMEOUT_MILLISECONDS, timeout)\n     send_timeout = min(self._ZMQ_SOCKET_SEND_TIMEOUT_MILLISECONDS, timeout)\n     self._zmq_socket.setsockopt(zmq.RCVTIMEO, receive_timeout)\n     self._zmq_socket.setsockopt(zmq.SNDTIMEO, send_timeout)", "query": "socket recv timeout"}
{"id": "https://github.com/noahmorrison/chevron/blob/78f1a384eddef16906732d8db66deea6d37049b7/chevron/renderer.py#L34-L47", "method_name": "_html_escape", "code": "def _html_escape(string):\n     html_codes = {\n         \"\"\": \"&quot;\",\n         \"<\": \"&lt;\",\n         \">\": \"&gt;\",\n     }\n     string = string.replace(\"&\", \"&amp;\")\n     for char in html_codes:\n         string = string.replace(char, html_codes[char])\n     return string", "query": "html encode string"}
{"id": "https://github.com/szairis/sakmapper/blob/ac462fd2674e6aa1aa3b209222d8ac4e9268a790/sakmapper/network.py#L111-L170", "method_name": "optimal_clustering", "code": "def optimal_clustering(df, patch, method=\"kmeans\", statistic=\"gap\", max_K=5):\n     if len(patch) == 1:\n         return [patch]\n     if statistic == \"db\":\n         if method == \"kmeans\":\n             if len(patch) <= 5:\n                 K_max = 2\n             else:\n                 K_max = min(len(patch) / 2, max_K)\n             clustering = {}\n             db_index = []\n             X = df.ix[patch, :]\n             for k in range(2, K_max + 1):\n                 kmeans = cluster.KMeans(n_clusters=k).fit(X)\n                 clustering[k] = pd.DataFrame(kmeans.predict(X), index=patch)\n                 dist_mu = squareform(pdist(kmeans.cluster_centers_))\n                 sigma = []\n                 for i in range(k):\n                     points_in_cluster = clustering[k][clustering[k][0] == i].index\n                     sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n                 db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n             db_index = np.array(db_index)\n             k_optimal = np.argmin(db_index) + 2\n             return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n         elif method == \"agglomerative\":\n             if len(patch) <= 5:\n                 K_max = 2\n             else:\n                 K_max = min(len(patch) / 2, max_K)\n             clustering = {}\n             db_index = []\n             X = df.ix[patch, :]\n             for k in range(2, K_max + 1):\n                 agglomerative = cluster.AgglomerativeClustering(n_clusters=k, linkage=\"average\").fit(X)\n                 clustering[k] = pd.DataFrame(agglomerative.fit_predict(X), index=patch)\n                 tmp = [list(clustering[k][clustering[k][0] == i].index) for i in range(k)]\n                 centers = np.array([np.mean(X.ix[c, :], axis=0) for c in tmp])\n                 dist_mu = squareform(pdist(centers))\n                 sigma = []\n                 for i in range(k):\n                     points_in_cluster = clustering[k][clustering[k][0] == i].index\n                     sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n                 db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n             db_index = np.array(db_index)\n             k_optimal = np.argmin(db_index) + 2\n             return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n     elif statistic == \"gap\":\n         X = np.array(df.ix[patch, :])\n         if method == \"kmeans\":\n             f = cluster.KMeans\n         gaps = gap(X, ks=range(1, min(max_K, len(patch))), method=f)\n         k_optimal = list(gaps).index(max(gaps))+1\n         clustering = pd.DataFrame(f(n_clusters=k_optimal).fit_predict(X), index=patch)\n         return [list(clustering[clustering[0] == i].index) for i in range(k_optimal)]\n     else:\n         raise \"error: only db and gat statistics are supported\"", "query": "k means clustering"}
{"id": "https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/tasks.py#L4149-L4153", "method_name": "set_workdir", "code": "def set_workdir(self, workdir, chroot=False):\n         super().set_workdir(workdir, chroot=chroot)\n         self.output_file = self.log_file", "query": "set working directory"}
{"id": "https://github.com/radujica/baloo/blob/f6e05e35b73a75e8a300754c6bdc575e5f2d53b9/baloo/weld/weld_ops.py#L529-L569", "method_name": "weld_unique", "code": "def weld_unique(array, weld_type):\n     obj_id, weld_obj = create_weld_object(array)\n     weld_template = \n     weld_obj.weld_code = weld_template.format(array=obj_id,\n                                               type=weld_type)\n     return weld_obj", "query": "unique elements"}
{"id": "https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/parse.py#L551-L636", "method_name": "date_convert", "code": "def date_convert(string, match, ymd=None, mdy=None, dmy=None,\n         d_m_y=None, hms=None, am=None, tz=None, mm=None, dd=None):\n     groups = match.groups()\n     time_only = False\n     if mm and dd:\n         y=datetime.today().year\n         m=groups[mm]\n         d=groups[dd]\n     elif ymd is not None:\n         y, m, d = re.split(r\"[-/\\s]\", groups[ymd])\n     elif mdy is not None:\n         m, d, y = re.split(r\"[-/\\s]\", groups[mdy])\n     elif dmy is not None:\n         d, m, y = re.split(r\"[-/\\s]\", groups[dmy])\n     elif d_m_y is not None:\n         d, m, y = d_m_y\n         d = groups[d]\n         m = groups[m]\n         y = groups[y]\n     else:\n         time_only = True\n     H = M = S = u = 0\n     if hms is not None and groups[hms]:\n         t = groups[hms].split(\":\")\n         if len(t) == 2:\n             H, M = t\n         else:\n             H, M, S = t\n             if \".\" in S:\n                 S, u = S.split(\".\")\n                 u = int(float(\".\" + u) * 1000000)\n             S = int(S)\n         H = int(H)\n         M = int(M)\n     if am is not None:\n         am = groups[am]\n         if am:\n             am = am.strip()\n         if am == \"AM\" and H == 12:\n             H -= 12\n         elif am == \"PM\" and H == 12:\n             pass\n         elif am == \"PM\":\n             H += 12\n     if tz is not None:\n         tz = groups[tz]\n     if tz == \"Z\":\n         tz = FixedTzOffset(0, \"UTC\")\n     elif tz:\n         tz = tz.strip()\n         if tz.isupper():\n             pass\n         else:\n             sign = tz[0]\n             if \":\" in tz:\n                 tzh, tzm = tz[1:].split(\":\")\n             elif len(tz) == 4:  \n                 tzh, tzm = tz[1], tz[2:4]\n             else:\n                 tzh, tzm = tz[1:3], tz[3:5]\n             offset = int(tzm) + int(tzh) * 60\n             if sign == \"-\":\n                 offset = -offset\n             tz = FixedTzOffset(offset, tz)\n     if time_only:\n         d = time(H, M, S, u, tzinfo=tz)\n     else:\n         y = int(y)\n         if m.isdigit():\n             m = int(m)\n         else:\n             m = MONTHS_MAP[m]\n         d = int(d)\n         d = datetime(y, m, d, H, M, S, u, tzinfo=tz)\n     return d", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/ml/algorithms/clustering.py#L307-L350", "method_name": "elbow_method", "code": "def elbow_method(data, k_min, k_max, distance=\"euclidean\"):\n         k_range = range(k_min, k_max)\n         k_means_var = [Clustering.kmeans(k).fit(data) for k in k_range]\n         centroids = [X.model.cluster_centers_ for X in k_means_var]\n         k_euclid = [cdist(data, cent, distance) for cent in centroids]\n         dist = [np.min(ke, axis=1) for ke in k_euclid]\n         wcss = [sum(d ** 2) for d in dist]\n         tss = sum(pdist(data) ** 2) / data.shape[0]\n         bss = tss - wcss\n         fig = plt.figure()\n         ax = fig.add_subplot(111)\n         ax.plot(k_range, bss / tss * 100, \"b*-\")\n         ax.set_ylim((0, 100))\n         plt.grid(True)\n         plt.xlabel(\"n_clusters\")\n         plt.ylabel(\"Percentage of variance explained\")\n         plt.title(\"Variance Explained vs. k\")\n         plt.show()", "query": "k means clustering"}
{"id": "https://github.com/limodou/uliweb/blob/34472f25e4bc0b954a35346672f94e84ef18b076/uliweb/core/template.py#L489-L524", "method_name": "reindent", "code": "def reindent(text, filename): \n     new_lines=[] \n     k=0 \n     c=0 \n     for n, raw_line in enumerate(text.splitlines()): \n         line=raw_line.strip() \n         if not line or line[0]==\"\n             new_lines.append(line) \n             continue \n         line3 = line[:3] \n         line4 = line[:4] \n         line5 = line[:5] \n         line6 = line[:6] \n         line7 = line[:7] \n         if line3==\"if \" or line4 in (\"def \", \"for \", \"try:\") or\\ \n             line6==\"while \" or line6==\"class \" or line5==\"with \": \n             new_lines.append(\"    \"*k+line) \n             k += 1 \n             continue \n         elif line5==\"elif \" or line5==\"else:\" or    \\ \n             line7==\"except:\" or line7==\"except \" or \\ \n             line7==\"finally:\": \n                 c = k-1 \n                 if c<0: \n                     raise ParseError(\"Extra pass founded on line %s:%d\" % (filename, n)) \n                 new_lines.append(\"    \"*c+line) \n                 continue \n         else: \n             new_lines.append(\"    \"*k+line) \n         if line==\"pass\" or line5==\"pass \": \n             k-=1 \n         if k<0: k = 0 \n     text=\"\n\".join(new_lines) \n     return text", "query": "read text file line by line"}
{"id": "https://github.com/milinda/htrc-sdk-for-python/blob/d08dbba1a441dcb2bade47deaebc07be68c3a173/htrc/utils.py#L45-L62", "method_name": "unzip", "code": "def unzip(zip_content, dest_dir):\n     with zipfile.ZipFile(zip_content, \"r\") as zf:\n         for member in zf.infolist():\n             words = member.filename.split(\"/\")\n             path = dest_dir\n             for word in words[:-1]:\n                 drive, word = os.path.splitdrive(word)\n                 head, word = os.path.split(word)\n                 if word in (os.curdir, os.pardir, \"\"):\n                     continue\n                 path = os.path.join(path, word)\n             zf.extract(member, path)", "query": "unzipping large files"}
{"id": "https://github.com/char16t/wa/blob/ee28bf47665ea57f3a03a08dfc0a5daaa33d8121/wa/api.py#L152-L174", "method_name": "replace_f", "code": "def replace_f(self, path, arg_name=None):\n         root, file = os.path.split(path)\n         pattern = re.compile(r\"(\\<\\<\\<)([A-Za-z_]+)(\\>\\>\\>)\")\n         file_path = path\n         fh, abs_path = mkstemp()\n         with open(abs_path, \"w\") as new_file:\n             with open(file_path) as old_file:\n                 for line in old_file:\n                     for (o, var_name, c) in re.findall(pattern, line):\n                         line = self.handle_args(line, var_name, arg_name)\n                     new_file.write(line)\n         os.close(fh)\n         os.remove(file_path)\n         shutil.move(abs_path, file_path)\n         pattern = re.compile(r\"(\\[\\[)([A-Za-z_]+)(\\]\\])\")\n         for (o, var_name, c) in re.findall(pattern, file):\n             file = self.handle_args(file, var_name, isfilename=True)\n         os.rename(path, os.path.join(root, file))", "query": "replace in file"}
{"id": "https://github.com/zkbt/the-friendly-stars/blob/50d3f979e79e63c66629065c75595696dc79802e/thefriendlystars/constellations/constellation.py#L281-L314", "method_name": "plot", "code": "def plot(self, sizescale=10, color=None, alpha=0.5, label=None, edgecolor=\"none\", **kw):\n         size = np.maximum(sizescale*(1 + self.magnitudelimit - self.magnitude), 1)\n         scatter = plt.scatter(self.ra, self.dec,\n                                     s=size,\n                                     color=color or self.color,\n                                     label=label or \"{} ({:.1f})\".format(self.name, self.epoch),\n                                     alpha=alpha,\n                                     edgecolor=edgecolor,\n                                     **kw)\n         return scatter", "query": "scatter plot"}
{"id": "https://github.com/ajyoon/blur/blob/25fcf083af112bb003956a7a7e1c6ff7d8fef279/blur/rand.py#L252-L300", "method_name": "normal_distribution", "code": "def normal_distribution(mean, variance,\n                         minimum=None, maximum=None, weight_count=23):\n     standard_deviation = math.sqrt(variance)\n     min_x = (standard_deviation * -5) + mean\n     max_x = (standard_deviation * 5) + mean\n     step = (max_x - min_x) / weight_count\n     current_x = min_x\n     weights = []\n     while current_x < max_x:\n         weights.append(\n             (current_x, _normal_function(current_x, mean, variance))\n         )\n         current_x += step\n     if minimum is not None or maximum is not None:\n         return bound_weights(weights, minimum, maximum)\n     else:\n         return weights", "query": "normal distribution"}
{"id": "https://github.com/mattharrison/rst2odp/blob/4adbf29b28c8207ec882f792ded07e98b1d3e7d0/odplib/preso.py#L170-L175", "method_name": "sub_el", "code": "def sub_el(parent, tag, attrib=None):\n     attrib = attrib or {}\n     tag = get_nstag(tag)\n     attrib = update_attrib(attrib)\n     el = et.SubElement(parent, tag, attrib)  \n     return el", "query": "set file attrib hidden"}
{"id": "https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/mit_stats.py#L48-L88", "method_name": "runningMedian", "code": "def runningMedian(seq, M):\n     seq = iter(seq)\n     s = []\n     m = M // 2 \n     s = [item for item in islice(seq,M)]\n     d = deque(s)\n     median = lambda : s[m] if bool(M&1) else (s[m-1]+s[m])*0.5\n     s.sort()\n     medians = [median()]\n     for item in seq:\n         old = d.popleft()          \n         d.append(item)             \n         del s[bisect_left(s, old)] \n         insort(s, item)            \n         medians.append(median())\n     return medians", "query": "deducting the median from each column"}
{"id": "https://github.com/appknox/vendor/blob/0b85d3c8328c5102cdcb1f619cbff215d7b5f228/ak_vendor/report.py#L31-L34", "method_name": "to_html_file", "code": "def to_html_file(self, path=\"\"):\n         with open(\"{}/output.html\".format(path), \"w\") as file:\n             tpl = self.to_html()\n             file.write(tpl)", "query": "output to html file"}
{"id": "https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L136-L140", "method_name": "__mul__", "code": "def __mul__(self, other):\n         if isinstance(other, Matrix):\n             return Matrix(self.matrix.dot(other.matrix))\n         else:\n             return Matrix(self.matrix * other)", "query": "matrix multiply"}
{"id": "https://github.com/jordanjoz1/flickr-views-counter/blob/cba89285823ab39afd9f40a19db82371d45bd830/count_views.py#L130-L138", "method_name": "write_to_csv", "code": "def write_to_csv(fname, header, rows):\n     with open(fname, \"wb\") as csvfile:\n         csvwriter = csv.writer(csvfile, delimiter=\",\", quotechar=\" \",\n                                quoting=csv.QUOTE_MINIMAL)\n         csvwriter.writerow(header)\n         for row in rows:\n             csvwriter.writerow(\n                 [s.encode(\"utf-8\").replace(\",\", \"\").replace(\"\n\", \"\")\n                  for s in row])", "query": "write csv"}
{"id": "https://github.com/tensorpack/tensorpack/blob/d7a13cb74c9066bc791d7aafc3b744b60ee79a9f/examples/CaffeModels/load-cpm.py#L27-L32", "method_name": "colorize", "code": "def colorize(img, heatmap):\n     heatmap = viz.intensity_to_rgb(heatmap, cmap=\"jet\")[:, :, ::-1]\n     return img * 0.5 + heatmap * 0.5", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/rfinnie/dsari/blob/cd7b07c30876467393e0ec18f1ff45d86bcb1676/dsari/render.py#L75-L86", "method_name": "read_output", "code": "def read_output(filename):\n     if os.path.isfile(filename):\n         with open(filename, \"rb\") as f:\n             return f.read().decode(\"utf-8\")\n     elif os.path.isfile(\"{}.gz\".format(filename)):\n         with gzip.open(\"{}.gz\".format(filename), \"rb\") as f:\n             return f.read().decode(\"utf-8\")\n     elif HAS_LZMA and os.path.isfile(\"{}.xz\".format(filename)):\n         with open(\"{}.xz\".format(filename), \"rb\") as f:\n             return lzma.LZMADecompressor().decompress(f.read()).decode(\"utf-8\")\n     else:\n         return None", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/phoopy/phoopy-console/blob/d38ec0eb952e79239699a0f855c07437a34024b0/phoopy/console/helper/string_helper.py#L41-L43", "method_name": "get_most_used_words", "code": "def get_most_used_words(words, stopwords):\n        valid_words = [word.lower() for word in words if StringHelper.is_valid_word(word, stopwords)]\n        return dict(Counter(valid_words).most_common(5))", "query": "determine a string is a valid word"}
{"id": "https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/distribution.py#L274-L319", "method_name": "anderson", "code": "def anderson(*args, dist=\"norm\"):\n     from scipy.stats import anderson as ads\n     k = len(args)\n     from_dist = np.zeros(k, \"bool\")\n     sig_level = np.zeros(k)\n     for j in range(k):\n         st, cr, sig = ads(args[j], dist=dist)\n         from_dist[j] = True if (st > cr).any() else False\n         sig_level[j] = sig[np.argmin(np.abs(st - cr))]\n     if k == 1:\n         from_dist = bool(from_dist)\n         sig_level = float(sig_level)\n     return from_dist, sig_level", "query": "binomial distribution"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/rotate_poscar.py#L7-L13", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Rotates the cell lattice in VASP POSCAR files\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\" )\n     parser.add_argument( \"-a\", \"--axis\", nargs=3, type=float, help=\"vector for rotation axis\", required=True )\n     parser.add_argument( \"-d\", \"--degrees\", type=int, help=\"rotation angle in degrees\", required=True )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L62-L67", "method_name": "pick", "code": "def pick(self):\n         while True:\n             idx = random.randint(0, len(self.values) - 1)\n             v, p = self.values[idx]\n             if p >= random.uniform(0, 1):\n                 return v", "query": "randomly pick a number"}
{"id": "https://github.com/wolfhong/formic/blob/0d81eb88dcbb6fa705194fc6ccf2993f4abbaa76/formic/formic.py#L182-L187", "method_name": "match", "code": "def match(self, string):\n         if self.casesensitive:\n             return self.pattern == os.path.normcase(string)\n         else:\n             return self.pattern.lower() == os.path.normcase(string).lower()", "query": "regex case insensitive"}
{"id": "https://github.com/googledatalab/pydatalab/blob/d9031901d5bca22fe0d5925d204e6698df9852e1/solutionbox/structured_data/mltoolbox/_structured_data/prediction/predict.py#L307-L346", "method_name": "expand", "code": "def expand(self, datasets):\n     import json\n     tf_graph_predictions, errors = datasets\n     if self._output_format == \"json\":\n       (tf_graph_predictions  \n        \"Write Raw JSON\" >>\n        beam.io.textio.WriteToText(os.path.join(self._output_dir, \"predictions\"),\n                                   file_name_suffix=\".json\",\n                                   coder=RawJsonCoder(),\n                                   shard_name_template=self._shard_name_template))\n     elif self._output_format == \"csv\":\n       header = [col[\"name\"] for col in self._schema]\n       csv_coder = CSVCoder(header)\n       (tf_graph_predictions.pipeline  \n        \"Make CSV Header\" >>\n        beam.Create([json.dumps(self._schema, indent=2)])  \n        \"Write CSV Schema File\" >>\n        beam.io.textio.WriteToText(os.path.join(self._output_dir, \"csv_schema\"),\n                                   file_name_suffix=\".json\",\n                                   shard_name_template=\"\"))\n       (tf_graph_predictions  \n        \"Write CSV\" >>\n        beam.io.textio.WriteToText(os.path.join(self._output_dir, \"predictions\"),\n                                   file_name_suffix=\".csv\",\n                                   coder=csv_coder,\n                                   shard_name_template=self._shard_name_template))\n     else:\n       raise ValueError(\"FormatAndSave: unknown format %s\", self._output_format)\n     (errors  \n      \"Write Errors\" >>\n      beam.io.textio.WriteToText(os.path.join(self._output_dir, \"errors\"),\n                                 file_name_suffix=\".txt\",\n                                 shard_name_template=self._shard_name_template))", "query": "write csv"}
{"id": "https://github.com/codeforamerica/three/blob/67b4a4b233a57aa7995d01f6b0f69c2e85aea6c0/three/core.py#L158-L170", "method_name": "convert", "code": "def convert(self, content, conversion):\n         if not conversion:\n             data = content\n         elif self.format == \"json\":\n             data = json.loads(content)\n         elif self.format == \"xml\":\n             content = xml(content)\n             first = list(content.keys())[0]\n             data = content[first]\n         else:\n             data = content\n         return data", "query": "json to xml conversion"}
{"id": "https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660", "method_name": "checkbox_check", "code": "def checkbox_check(self, force_check=False):\n         if not self.get_attribute(\"checked\"):\n             self.click(force_click=force_check)", "query": "make the checkbox checked"}
{"id": "https://github.com/totalgood/pugnlp/blob/c43445b14afddfdeadc5f3076675c9e8fc1ee67c/src/pugnlp/stats.py#L536-L562", "method_name": "from_existing", "code": "def from_existing(cls, confusion, *args, **kwargs):\n         df = []\n         for t, p in product(confusion.index.values, confusion.columns.values):\n             df += [[t, p]] * confusion[p][t]\n         if confusion.index.name is not None and confusion.columns.name is not None:\n             return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name]))\n         return Confusion(pd.DataFrame(df))", "query": "confusion matrix"}
{"id": "https://github.com/mozilla/Marketplace.Python/blob/88176b12201f766b6b96bccc1e4c3e82f0676283/example/main.py#L29-L61", "method_name": "main", "code": "def main():\n     parser = argparse.ArgumentParser(\n         description=\"Command line Marketplace client\")\n     parser.add_argument(\"method\", type=str,\n                         help=\"command to be run on arguments\",\n                         choices=COMMANDS.keys())\n     parser.add_argument(\"attrs\", metavar=\"attr\", type=str, nargs=\"*\",\n                         help=\"command arguments\")\n     parser.add_argument(\"-v\", action=\"store_true\", default=False,\n                         dest=\"verbose\",\n                         help=\"Switch to verbose mode\")\n     args = parser.parse_args()\n     client = marketplace.Client(\n         domain=config.MARKETPLACE_DOMAIN,\n         protocol=config.MARKETPLACE_PROTOCOL,\n         port=config.MARKETPLACE_PORT,\n         consumer_key=config.CONSUMER_KEY,\n         consumer_secret=config.CONSUMER_SECRET)\n     if args.verbose:\n         logger.setLevel(logging.DEBUG)\n     if args.attrs:\n         result = COMMANDS[args.method](client, *args.attrs)\n     else:\n         result = COMMANDS[args.method](client)\n     if result[\"success\"]:\n         sys.stdout.write(\"%s\n\" % result[\"message\"])\n     else:\n         sys.stderr.write(\"%s\n\" % result[\"message\"])\n         sys.exit(1)", "query": "parse command line argument"}
{"id": "https://github.com/Fizzadar/pyinfra/blob/006f751f7db2e07d32522c0285160783de2feb79/pyinfra/modules/postgresql.py#L48-L123", "method_name": "role", "code": "def role(\n     state, host, name,\n     present=True,\n     password=None, login=True, superuser=False, inherit=False,\n     createdb=False, createrole=False, replication=False, connection_limit=None,\n     postgresql_user=None, postgresql_password=None,\n     postgresql_host=None, postgresql_port=None,\n ):\n     roles = host.fact.postgresql_roles(\n         postgresql_user, postgresql_password,\n         postgresql_host, postgresql_port,\n     )\n     is_present = name in roles\n     if not present:\n         if is_present:\n             yield make_execute_psql_command(\n                 \"DROP ROLE {0}\".format(name),\n                 user=postgresql_user,\n                 password=postgresql_password,\n                 host=postgresql_host,\n                 port=postgresql_port,\n             )\n         return\n     if not is_present:\n         sql_bits = [\"CREATE ROLE {0}\".format(name)]\n         for key, value in (\n             (\"LOGIN\", login),\n             (\"SUPERUSER\", superuser),\n             (\"INHERIT\", inherit),\n             (\"CREATEDB\", createdb),\n             (\"CREATEROLE\", createrole),\n             (\"REPLICATION\", replication),\n         ):\n             if value:\n                 sql_bits.append(key)\n         if connection_limit:\n             sql_bits.append(\"CONNECTION LIMIT {0}\".format(connection_limit))\n         if password:\n             sql_bits.append(\"PASSWORD \"{0}\"\".format(password))\n         yield make_execute_psql_command(\n             \" \".join(sql_bits),\n             user=postgresql_user,\n             password=postgresql_password,\n             host=postgresql_host,\n             port=postgresql_port,\n         )", "query": "postgresql connection"}
{"id": "https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/client/archive.py#L93-L104", "method_name": "copy_dir", "code": "def copy_dir(self, path):\n         for directory in path:\n             if os.path.isdir(path):\n                 full_path = os.path.join(self.archive_dir, directory.lstrip(\"/\"))\n                 logger.debug(\"Copying %s to %s\", directory, full_path)\n                 shutil.copytree(directory, full_path)\n             else:\n                 logger.debug(\"Not a directory: %s\", directory)\n         return path", "query": "copying a file to a path"}
{"id": "https://github.com/fridiculous/estimators/blob/ab5b3d70f16f8372ae1114ac7e54e7791631eb74/estimators/hashing.py#L120-L127", "method_name": "memoize", "code": "def memoize(self, obj):\n         if isinstance(obj, _bytes_or_unicode):\n             return\n         Pickler.memoize(self, obj)", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/openvax/pyensembl/blob/4b995fb72e848206d6fbf11950cf30964cd9b3aa/pyensembl/memory_cache.py#L64-L72", "method_name": "_read_csv", "code": "def _read_csv(self, csv_path):\n         logger.info(\"Reading Dataframe from %s\", csv_path)\n         df = pd.read_csv(csv_path)\n         if \"seqname\" in df:\n             df[\"seqname\"] = df[\"seqname\"].map(str)\n         return df", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/raymontag/kppy/blob/a43f1fff7d49da1da4b3d8628a1b3ebbaf47f43a/kppy/database.py#L861-L871", "method_name": "_cbc_encrypt", "code": "def _cbc_encrypt(self, content, final_key):\n         aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv)\n         padding = (16 - len(content) % AES.block_size)\n         for _ in range(padding):\n             content += chr(padding).encode()\n         temp = bytes(content)\n         return aes.encrypt(temp)", "query": "aes encryption"}
{"id": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/functions.py#L1634-L1643", "method_name": "levenshtein", "code": "def levenshtein(left, right):\n     sc = SparkContext._active_spark_context\n     jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))\n     return Column(jc)", "query": "string similarity levenshtein"}
{"id": "https://github.com/merenlab/illumina-utils/blob/246d0611f976471783b83d2aba309b0cb57210f6/IlluminaUtils/lib/fastalib.py#L111-L128", "method_name": "init_unique_hash", "code": "def init_unique_hash(self):\n         while self.next_regular():\n             hash = hashlib.sha1(self.seq.upper().encode(\"utf-8\")).hexdigest()\n             if hash in self.unique_hash_dict:\n                 self.unique_hash_dict[hash][\"ids\"].append(self.id)\n                 self.unique_hash_dict[hash][\"count\"] += 1\n             else:\n                 self.unique_hash_dict[hash] = {\"id\": self.id,\n                                                \"ids\": [self.id],\n                                                \"seq\": self.seq,\n                                                \"count\": 1}\n         self.unique_hash_list = [i[1] for i in sorted([(self.unique_hash_dict[hash][\"count\"], hash)\\\n                         for hash in self.unique_hash_dict], reverse=True)]\n         self.total_unique = len(self.unique_hash_dict)\n         self.reset()", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/vstinner/perf/blob/cf096c0c0c955d0aa1c893847fa6393ba4922ada/perf/_utils.py#L481-L484", "method_name": "median_abs_dev", "code": "def median_abs_dev(values):\n     median = float(statistics.median(values))\n     return statistics.median([abs(median - sample) for sample in values])", "query": "deducting the median from each column"}
{"id": "https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L297-L304", "method_name": "read_csv", "code": "def read_csv(filename, has_header=True):\n     with open(filename) as fh:\n         csv_reader = csv.reader(fh)\n         header = None\n         if has_header:\n             header = csv_reader.next()\n         rows = [row for row in csv_reader]\n     return header, rows", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/utils/dialog.py#L233-L234", "method_name": "get_checkbox_state_by_name", "code": "def get_checkbox_state_by_name(self, checkbox_text):\n        return [checkbox.get_active() for checkbox in self.checkboxes if checkbox.get_label() == checkbox_text]", "query": "check if a checkbox is checked"}
{"id": "https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/date_interval.py#L266-L267", "method_name": "to_string", "code": "def to_string(self):\n        return \"-\".join([d.strftime(\"%Y-%m-%d\") for d in (self.date_a, self.date_b)])", "query": "string to date"}
{"id": "https://github.com/pedroburon/tbk/blob/ecd6741e0bae06269eb4ac885c3ffcb7902ee40e/tbk/webpay/encryption.py#L36-L42", "method_name": "encrypt_message", "code": "def encrypt_message(self, signed_message, message, key, iv):\n         raw = signed_message + message\n         block_size = AES.block_size\n         pad = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size).encode(\"utf-8\")\n         message_to_encrypt = pad(raw)\n         cipher = AES.new(key, AES.MODE_CBC, iv)\n         return cipher.encrypt(message_to_encrypt)", "query": "aes encryption"}
{"id": "https://github.com/wandb/client/blob/7d08954ed5674fee223cd85ed0d8518fe47266b2/wandb/data_types.py#L727-L746", "method_name": "to_uint8", "code": "def to_uint8(self, data):\n         np = util.get_module(\n             \"numpy\", required=\"wandb.Image requires numpy if not supplying PIL Images: pip install numpy\")\n         dmin = np.min(data)\n         if dmin < 0:\n             data = (data - np.min(data)) / np.ptp(data)\n         if np.max(data) <= 1.0:\n             data = (data * 255).astype(np.int32)\n         return data.clip(0, 255).astype(np.uint8)", "query": "converting uint8 array to image"}
{"id": "https://github.com/sony/nnabla/blob/aaf3d33b7cbb38f2a03aa754178ba8f7c8481320/python/src/nnabla/utils/image_utils/common.py#L62-L66", "method_name": "upscale_float_image", "code": "def upscale_float_image(img, as_uint16):\n     if as_uint16:\n         return np.asarray((img * 65535), np.uint16)\n     return np.asarray((img * 255), np.uint8)", "query": "converting uint8 array to image"}
{"id": "https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/internet/web.py#L171-L180", "method_name": "get_html_source", "code": "def get_html_source(self):\n         req = urllib.request.Request(self.url)\n         req.add_header(\"user-agent\", random.choice(USER_AGENTS))\n         req_text = urllib.request.urlopen(req).read()\n         self.source = str(req_text)\n         self.soup = BeautifulSoup(self.source, \"html.parser\")\n         return self.source", "query": "get html of website"}
{"id": "https://github.com/sony/nnabla/blob/aaf3d33b7cbb38f2a03aa754178ba8f7c8481320/python/benchmark/function/function_benchmark.py#L224-L234", "method_name": "_calc_benchmark_stat", "code": "def _calc_benchmark_stat(self, f):\n         timer = Timer()\n         i = 0\n         while True:\n             f()\n             i += 1\n             if i >= self.min_run:\n                 _, elapsed = timer.lap()\n                 if elapsed > self.min_time:\n                     break\n         return BenchmarkStat(elapsed / i, i)", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/components/rom.py#L101-L134", "method_name": "extract_zip", "code": "def extract_zip(self):\n         assert self.FILE_COUNT>0\n         try:\n             with zipfile.ZipFile(self.archive_path, \"r\") as zip:\n                 namelist = zip.namelist()\n                 print(\"namelist():\", namelist)\n                 if len(namelist) != self.FILE_COUNT:\n                     msg = (\n                         \"Wrong archive content?!?\"\n                         \" There exists %i files, but it should exist %i.\"\n                         \"Existing names are: %r\"\n                     ) % (len(namelist), self.FILE_COUNT, namelist)\n                     log.error(msg)\n                     raise RuntimeError(msg)\n                 for filename in namelist:\n                     content = zip.read(filename)\n                     dst = self.file_rename(filename)\n                     out_filename=os.path.join(self.ROM_PATH, dst)\n                     with open(out_filename, \"wb\") as f:\n                         f.write(content)\n                     if dst == filename:\n                         print(\"%r extracted\" % out_filename)\n                     else:\n                         print(\"%r extracted to %r\" % (filename, out_filename))\n                     self.post_processing(out_filename)\n         except BadZipFile as err:\n             msg = \"Error extracting archive %r: %s\" % (self.archive_path, err)\n             log.error(msg)\n             raise BadZipFile(msg)", "query": "extract zip file recursively"}
{"id": "https://github.com/joshburnett/scanf/blob/52f8911581c1590a3dcc6f17594eeb7b39716d42/scanf.py#L158-L184", "method_name": "extractdata", "code": "def extractdata(pattern, text=None, filepath=None):\n     y = []\n     if text is None:\n         textsource = open(filepath, \"r\")\n     else:\n         textsource = text.splitlines()\n     for line in textsource:\n         match = scanf(pattern, line)\n         if match:\n             if len(y) == 0:\n                 y = [[s] for s in match]\n             else:\n                 for i, ydata in enumerate(y):\n                     ydata.append(match[i])\n     if text is None:\n         textsource.close()\n     return y", "query": "extracting data from a text file"}
{"id": "https://github.com/bxlab/bx-python/blob/09cb725284803df90a468d910f2274628d8647de/lib/bx/align/lav.py#L287-L296", "method_name": "fetch_line", "code": "def fetch_line(self,strip=True,requireLine=True,report=\"\"):\n \t\tif   (strip == None): line = self.file.readline()\n \t\telif (strip == True): line = self.file.readline().strip()\n \t\telse:                 line = self.file.readline().strip().strip(strip)\n \t\tself.lineNumber += 1\n \t\tif (requireLine):\n \t\t\tassert (line), \\\n \t\t\t       \"unexpected blank line or end of file%s (line %d)\" \\\n \t\t\t     % (report,self.lineNumber)\n \t\treturn line", "query": "read text file line by line"}
{"id": "https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hazardlib/valid.py#L437-L446", "method_name": "lon_lat", "code": "def lon_lat(value):\n     lon, lat = value.split()\n     return longitude(lon), latitude(lat)", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/anthonynguyen/pyrcon/blob/278cba95dd4d53a347d37acfce556ad375370e15/pyrcon/rcon.py#L97-L122", "method_name": "recvall", "code": "def recvall(self, timeout=0.5):\n         response = \"\"\n         self.socket.setblocking(False)\n         start = time.time()\n         while True:\n             if response and time.time() - start > timeout:\n                 break\n             elif time.time() - start > timeout * 2:\n                 break\n             try:\n                 data = self.socket.recv(4096)\n                 if data:\n                     response += data.replace(self._rconreplystring, \"\")\n                     start = time.time()\n                 else:\n                     time.sleep(0.1)\n             except socket.error:\n                 pass\n         return response.strip()", "query": "socket recv timeout"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mssql_hook.py#L38-L49", "method_name": "get_conn", "code": "def get_conn(self):\n         conn = self.get_connection(self.mssql_conn_id)\n         conn = pymssql.connect(\n             server=conn.host,\n             user=conn.login,\n             password=conn.password,\n             database=self.schema or conn.schema,\n             port=conn.port)\n         return conn", "query": "postgresql connection"}
{"id": "https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/web_node.py#L305-L311", "method_name": "innerHTML", "code": "def innerHTML(self, html: str) -> None:  \n         df = self._parse_html(html)\n         if self.connected:\n             self._set_inner_html_web(df.html)\n         self._empty()\n         self._append_child(df)", "query": "get inner html"}
{"id": "https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP/blob/84dc8114bf8a79c3acb3f7f59128247b9fc97243/numpy_indexed/grouping.py#L343-L382", "method_name": "median", "code": "def median(self, values, axis=0, average=True):\n         mid_2 = self.index.start + self.index.stop\n         hi = (mid_2    ) // 2\n         lo = (mid_2 - 1) // 2\n         sorted_group_rank_per_key = self.index.sorted_group_rank_per_key\n         def median1d(slc):\n             slc    = slc[self.index.sorter]\n             sorter = np.lexsort((slc, sorted_group_rank_per_key))\n             slc    = slc[sorter]\n             return (slc[lo]+slc[hi]) / 2 if average else slc[hi]\n         values = np.asarray(values)\n         if values.ndim>1:   \n             values = np.apply_along_axis(median1d, axis, values)\n         else:\n             values = median1d(values)\n         return self.unique, values", "query": "deducting the median from each column"}
{"id": "https://github.com/piglei/uwsgi-sloth/blob/2834ac5ed17d89ca5f19151c649ac610f6f37bd1/uwsgi_sloth/tailer.py#L45-L51", "method_name": "read", "code": "def read(self, read_size=None):\n         if read_size:\n             read_str = self.file.read(read_size)\n         else:\n             read_str = self.file.read()\n         return len(read_str), read_str", "query": "read properties file"}
{"id": "https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/graph/cluster.py#L16-L20", "method_name": "unique", "code": "def unique(list):\n     unique = []; [unique.append(x) for x in list if x not in unique]\n     return unique", "query": "unique elements"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/jsons.py#L231-L247", "method_name": "get_csv_from_json", "code": "def get_csv_from_json(d):\n     logger_jsons.info(\"enter get_csv_from_json\")\n     csv_data = OrderedDict()\n     if \"paleoData\" in d:\n         csv_data = _get_csv_from_section(d, \"paleoData\", csv_data)\n     if \"chronData\" in d:\n         csv_data = _get_csv_from_section(d, \"chronData\", csv_data)\n     logger_jsons.info(\"exit get_csv_from_json\")\n     return csv_data", "query": "convert json to csv"}
{"id": "https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_bin_profile.py#L259-L268", "method_name": "print_permutations", "code": "def print_permutations(self):\n         index = 0\n         permutations = []\n         for p in self._input_permutations:\n             permutations.append({\"index\": index, \"args\": p})\n             index += 1\n         with open(\"permutations.json\", \"w\") as fh:\n             json.dump(permutations, fh, indent=2)\n         print(\"All permutations written to the \"permutations.json\" file.\")", "query": "all permutations of a list"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mssql_hook.py#L38-L49", "method_name": "get_conn", "code": "def get_conn(self):\n         conn = self.get_connection(self.mssql_conn_id)\n         conn = pymssql.connect(\n             server=conn.host,\n             user=conn.login,\n             password=conn.password,\n             database=self.schema or conn.schema,\n             port=conn.port)\n         return conn", "query": "postgresql connection"}
{"id": "https://github.com/DataBiosphere/dsub/blob/443ce31daa6023dc2fd65ef2051796e19d18d5a7/dsub/providers/google.py#L466-L473", "method_name": "_datetime_to_utc_int", "code": "def _datetime_to_utc_int(date):\n     if date is None:\n       return None\n     epoch = dsub_util.replace_timezone(datetime.utcfromtimestamp(0), pytz.utc)\n     return (date - epoch).total_seconds()", "query": "convert a utc time to epoch"}
{"id": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/converter/tmdd.py#L9-L15", "method_name": "tmdd_to_json", "code": "def tmdd_to_json(doc):\n     converters = TMDDEventConverter.list_from_document(doc)\n     events = [converter.to_json() for converter in converters]\n     return {\n         \"meta\": dict(version=\"v1\"),\n         \"events\": events\n     }", "query": "json to xml conversion"}
{"id": "https://github.com/dsoprea/PathManifest/blob/0f5cfd4925a61cc0eac150ff354200392d07ec74/pm/manifest.py#L239-L271", "method_name": "__inject_files_to_staging", "code": "def __inject_files_to_staging(self, rel_filepaths, temp_path):\n         patch_files = {}\n         for rel_filepath in rel_filepaths:\n             from_filepath = os.path.join(self.__root_path, rel_filepath)\n             to_filepath = os.path.join(temp_path, rel_filepath)\n             _LOGGER.debug(\"Copying file to patch path: [%s] => [%s]\", \n                           from_filepath, to_filepath)\n             to_path = os.path.dirname(to_filepath)\n             if os.path.exists(to_path) is False:\n                 os.makedirs(to_path)\n             with open(from_filepath, \"rb\") as f:\n                 with open(to_filepath, \"wb\") as g:\n                     shutil.copyfileobj(f, g)\n             s = os.stat(from_filepath)\n             mtime_epoch = int(s.st_mtime)\n             filesize_b = s.st_size\n             os.utime(to_filepath, (mtime_epoch, mtime_epoch))\n             hash_ = self.__get_md5_for_rel_filepath(rel_filepath)\n             patch_files[rel_filepath] = {\n                 \"mtime_epoch\": mtime_epoch,\n                 \"filesize_b\": filesize_b,\n                 \"hash_md5\": hash_,\n             }\n         return patch_files", "query": "copying a file to a path"}
{"id": "https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/graph.py#L58-L65", "method_name": "get_edges", "code": "def get_edges(self, node):\n         return [\n             (node, child)\n             for child in self.get_children(node)\n         ] + [\n             (parent, node)\n             for parent in self.get_parents(node)\n         ]", "query": "get all parents of xml node"}
{"id": "https://github.com/mbedmicro/pyOCD/blob/41a174718a9739f3cbe785c2ba21cb7fd1310c6f/pyocd/debug/svd/parser.py#L82-L88", "method_name": "_parse_enumerated_value", "code": "def _parse_enumerated_value(self, enumerated_value_node):\n         return SVDEnumeratedValue(\n             name=_get_text(enumerated_value_node, \"name\"),\n             description=_get_text(enumerated_value_node, \"description\"),\n             value=_get_int(enumerated_value_node, \"value\"),\n             is_default=_get_int(enumerated_value_node, \"isDefault\")\n         )", "query": "get name of enumerated value"}
{"id": "https://github.com/Deathnerd/pyterp/blob/baf2957263685f03873f368226f5752da4e51f08/pyterp/__init__.py#L53-L65", "method_name": "_load_file", "code": "def _load_file(self, filename):\n         try:\n             read_file = os.path.realpath(os.path.join(os.curdir, filename))\n             with open(read_file, \"rb\") as file:\n                 temp_program = \"\"\n                 for line in file:\n                     temp_program += line.strip().replace(\" \", \"\").replace(\"\n\", \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\")\n         except IOError:  \n             print \"Cannot open file {}\".format(filename)\n             sys.exit(1)\n         return self._parse_program(temp_program)", "query": "replace in file"}
{"id": "https://github.com/dshean/pygeotools/blob/5ac745717c0098d01eb293ff1fe32fd7358c76ab/pygeotools/lib/iolib.py#L604-L626", "method_name": "readcsv", "code": "def readcsv(fn):\n     import csv\n     with open(fn, \"r\") as f:\n         reader = csv.DictReader(f)\n         hdr = reader.fieldnames\n     skiprows = 1\n     if np.all(f.isdigit() for f in hdr):\n         hdr = None\n         skiprows = 0\n     pts = np.loadtxt(fn, delimiter=\",\", skiprows=skiprows, dtype=None)\n     return pts", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/rehive/rehive-python/blob/a7452a9cfecf76c5c8f0d443f122ed22167fb164/rehive/api/client.py#L50-L51", "method_name": "post", "code": "def post(self, path, data, json=True, **kwargs):\n        return self._request(\"post\", path, data, json=json, **kwargs)", "query": "httpclient post json"}
{"id": "https://github.com/sendgrid/sendgrid-python/blob/266c2abde7a35dfcce263e06bedc6a0bbdebeac9/examples/helpers/stats/stats_example.py#L13-L14", "method_name": "pprint_json", "code": "def pprint_json(json_raw):\n    print(json.dumps(json.loads(json_raw), indent=2, sort_keys=True))", "query": "pretty print json"}
{"id": "https://github.com/pyGrowler/Growler/blob/90c923ff204f28b86a01d741224987a22f69540f/growler/middleware/cookieparser.py#L34-L56", "method_name": "__call__", "code": "def __call__(self, req, res):\n         if hasattr(req, \"cookies\"):\n             return\n         req.cookies, res.cookies = SimpleCookie(), SimpleCookie()\n         log.info(\"{:d} built with {}\", id(self), json.dumps(self.opts))\n         req.cookies.load(req.headers.get(\"COOKIE\", \"\"))\n         def _gen_cookie():\n             if res.cookies:\n                 cookie_string = res.cookies.output(header=\"\", sep=res.EOL)\n                 return cookie_string\n         res.headers[\"Set-Cookie\"] = _gen_cookie", "query": "create cookie"}
{"id": "https://github.com/HacKanCuBa/passphrase-py/blob/219d6374338ed9a1475b4f09b0d85212376f11e0/passphrase/secrets.py#L63-L84", "method_name": "randbelow", "code": "def randbelow(num: int) -> int:\n     if not isinstance(num, int):\n         raise TypeError(\"number must be an integer\")\n     if num <= 0:\n         raise ValueError(\"number must be greater than zero\")\n     if num == 1:\n         return 0\n     nbits = num.bit_length()    \n     randnum = random_randint(nbits)    \n     while randnum >= num:\n         randnum = random_randint(nbits)\n     return randnum", "query": "randomly pick a number"}
{"id": "https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/wrappers/abstract_wrapper.py#L161-L176", "method_name": "_search_for_executable", "code": "def _search_for_executable(self, executable):\n         if os.path.isfile(executable):\n             return os.path.abspath(executable)\n         else:\n             envpath = os.getenv(\"PATH\")\n             if envpath is None:\n                 return\n             for path in envpath.split(os.pathsep):\n                 exe = os.path.join(path, executable)\n                 if os.path.isfile(exe):\n                     return os.path.abspath(exe)", "query": "get executable path"}
{"id": "https://github.com/mfitzp/biocyc/blob/2fe81971687e4dcf1fcf869af0e7b3549be535b1/biocyc/biocyc.py#L566-L571", "method_name": "_import_parents_from_xml", "code": "def _import_parents_from_xml(self, xml):\n         parents = xml.iterfind(\"parent\")\n         for p in parents:\n             for o in p:\n                 self._parents.append( o.attrib[\"frameid\"] ) ", "query": "get all parents of xml node"}
{"id": "https://github.com/PySimpleGUI/PySimpleGUI/blob/08184197f5bd4580ab5e5aca28bdda30f87b86fc/PySimpleGUI27.py#L486-L493", "method_name": "CheckboxHandler", "code": "def CheckboxHandler(self):\n         if self.Key is not None:\n             self.ParentForm.LastButtonClicked = self.Key\n         else:\n             self.ParentForm.LastButtonClicked = \"\"\n         self.ParentForm.FormRemainedOpen = True\n         if self.ParentForm.CurrentlyRunningMainloop:\n             self.ParentForm.TKroot.quit()", "query": "check if a checkbox is checked"}
{"id": "https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_to_csv.py#L28-L47", "method_name": "json_to_csv", "code": "def json_to_csv(json_input):\n     try:\n         json_input = json.loads(json_input)\n     except:\n         pass \n     headers = set()\n     for json_row in json_input:\n         headers.update(json_row.keys())\n     csv_io = StringIO.StringIO()\n     csv_out = csv.DictWriter(csv_io,headers)\n     csv_out.writeheader()\n     for json_row in json_input:\n         csv_out.writerow(json_row)\n     csv_io.seek(0)\n     return csv_io.read()", "query": "convert json to csv"}
{"id": "https://github.com/guilhermechapiewski/simple-db-migrate/blob/7ea6ffd0c58f70079cc344eae348430c7bdaaab3/simple_db_migrate/mysql.py#L30-L40", "method_name": "__mysql_connect", "code": "def __mysql_connect(self, connect_using_database_name=True):\n         try:\n             conn = self.__mysql_driver.connect(host=self.__mysql_host, port=self.__mysql_port, user=self.__mysql_user, passwd=self.__mysql_passwd)\n             conn.set_character_set(self.__mysql_encoding)\n             if connect_using_database_name:\n                 conn.select_db(self.__mysql_db)\n             return conn\n         except Exception as e:\n             raise Exception(\"could not connect to database: %s\" % e)", "query": "connect to sql"}
{"id": "https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/dataset.py#L296-L412", "method_name": "get_time_array", "code": "def get_time_array(self,\n                        datetime_simulation_start=None,\n                        simulation_time_step_seconds=None,\n                        return_datetime=False,\n                        time_index_array=None):\n         if datetime_simulation_start is not None:\n             self.datetime_simulation_start = datetime_simulation_start\n         if simulation_time_step_seconds is not None:\n             self.simulation_time_step_seconds = simulation_time_step_seconds\n         epoch = datetime.datetime(1970, 1, 1, tzinfo=utc)\n         time_units = \"seconds since {0}\".format(epoch)\n         if self.is_time_variable_valid():\n             time_array = self.qout_nc.variables[\"time\"][:]\n             if self.qout_nc.variables[\"time\"].units:\n                 time_units = self.qout_nc.variables[\"time\"].units\n         elif self._is_legacy_time_valid():\n             initial_time_seconds = ((self.datetime_simulation_start\n                                     .replace(tzinfo=utc) - epoch)\n                                     .total_seconds() +\n                                     self.simulation_time_step_seconds)\n             final_time_seconds = (initial_time_seconds +\n                                   self.size_time *\n                                   self.simulation_time_step_seconds)\n             time_array = np.arange(initial_time_seconds,\n                                    final_time_seconds,\n                                    self.simulation_time_step_seconds)\n         else:\n             raise ValueError(\"This file does not contain the time\"\n                              \" variable. To get time array, add\"\n                              \" datetime_simulation_start and\"\n                              \" simulation_time_step_seconds\")\n         if time_index_array is not None:\n             time_array = time_array[time_index_array]\n         if return_datetime:\n             time_array = num2date(time_array, time_units)\n             if self.out_tzinfo is not None:\n                 for i in xrange(len(time_array)):\n                     time_array[i] = utc.localize(time_array[i]) \\\n                                        .astimezone(self.out_tzinfo) \\\n                                        .replace(tzinfo=None)\n         return time_array", "query": "convert a utc time to epoch"}
{"id": "https://github.com/Riffstation/flask-philo/blob/76c9d562edb4a77010c8da6dfdb6489fa29cbc9e/flask_philo/db/postgresql/connection.py#L58-L102", "method_name": "initialize", "code": "def initialize(g, app):\n     if \"DATABASES\" in app.config and \"POSTGRESQL\" in app.config[\"DATABASES\"]:\n         for k, v in app.config[\"DATABASES\"][\"POSTGRESQL\"].items():\n             init_db_conn(k, v)\n         if \"test\" not in sys.argv:\n             @app.before_request\n             def before_request():\n                 from flask import _app_ctx_stack\n                 for k, v in app.config[\"DATABASES\"][\"POSTGRESQL\"].items():\n                     init_db_conn(k, v, scopefunc=_app_ctx_stack)\n                 g.postgresql_pool = pool\n             @app.teardown_request\n             def teardown_request(exception):\n                 pool = getattr(g, \"postgresql_pool\", None)\n                 if pool is not None:\n                     for k, v in pool.connections.items():\n                         v.session.remove()\n         else:\n             @app.before_request\n             def before_request():\n                 for k, v in app.config[\"DATABASES\"][\"POSTGRESQL\"].items():\n                     init_db_conn(k, v)\n                 g.postgresql_pool = pool", "query": "postgresql connection"}
{"id": "https://github.com/dustin/twitty-twister/blob/8524750ee73adb57bbe14ef0cfd8aa08e1e59fb3/twittytwister/twitter.py#L171-L177", "method_name": "_urlencode", "code": "def _urlencode(self, h):\n         rv = []\n         for k,v in h.iteritems():\n             rv.append(\"%s=%s\" %\n                 (urllib.quote(k.encode(\"utf-8\")),\n                 urllib.quote(v.encode(\"utf-8\"))))\n         return \"&\".join(rv)", "query": "encode url"}
{"id": "https://github.com/calmjs/calmjs/blob/b9b407c2b6a7662da64bccba93bb8d92e7a5fafd/src/calmjs/utils.py#L128-L173", "method_name": "which", "code": "def which(cmd, mode=os.F_OK   os.X_OK, path=None):\n     if os.path.dirname(cmd):\n         if os.path.isfile(cmd) and os.access(cmd, mode):\n             return cmd\n     if path is None:\n         path = os.environ.get(\"PATH\", defpath)\n     if not path:\n         return None\n     paths = path.split(pathsep)\n     if sys.platform == \"win32\":\n         if curdir not in paths:\n             paths = [curdir] + paths\n         pathext = os.environ.get(\"PATHEXT\", \"\").split(pathsep)\n         if any(cmd.lower().endswith(ext.lower()) for ext in pathext):\n             files = [cmd]\n         else:\n             files = [cmd + ext for ext in pathext]\n     else:\n         files = [cmd]\n     seen = set()\n     for p in paths:\n         normpath = normcase(p)\n         if normpath in seen:\n             continue\n         seen.add(normpath)\n         for f in files:\n             fn = os.path.join(p, f)\n             if os.path.isfile(fn) and os.access(fn, mode):\n                 return fn\n     return None", "query": "get executable path"}
{"id": "https://github.com/atarashansky/self-assembling-manifold/blob/4db4793f65af62047492327716932ba81a67f679/SAM.py#L1318-L1350", "method_name": "kmeans_clustering", "code": "def kmeans_clustering(self, numc, X=None, npcs=15):\n         from sklearn.cluster import KMeans\n         if X is None:\n             D_sub = self.adata.uns[\"X_processed\"]\n             X = (\n                 D_sub -\n                 D_sub.mean(0)).dot(\n                 self.adata.uns[\"pca_obj\"].components_[\n                     :npcs,\n                     :].T)\n             save = True\n         else:\n             save = False\n         cl = KMeans(n_clusters=numc).fit_predict(Normalizer().fit_transform(X))\n         if save:\n             self.adata.obs[\"kmeans_clusters\"] = pd.Categorical(cl)\n         else:\n             return cl", "query": "k means clustering"}
{"id": "https://github.com/StanfordBioinformatics/loom/blob/db2031a1a87124fee1aeb7414a668c03d774a698/worker/loomengine_worker/outputs.py#L28-L35", "method_name": "save", "code": "def save(self):\n         filename_list = self.output[\"source\"][\"filenames\"]\n         file_path_list = [\n             os.path.join(\n                 self.working_dir, filename)\n             for filename in filename_list]\n         self.import_manager.import_result_file_list(\n             self.output, file_path_list, retry=True)", "query": "save list to file"}
{"id": "https://github.com/raymontag/kppy/blob/a43f1fff7d49da1da4b3d8628a1b3ebbaf47f43a/kppy/database.py#L861-L871", "method_name": "_cbc_encrypt", "code": "def _cbc_encrypt(self, content, final_key):\n         aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv)\n         padding = (16 - len(content) % AES.block_size)\n         for _ in range(padding):\n             content += chr(padding).encode()\n         temp = bytes(content)\n         return aes.encrypt(temp)", "query": "aes encryption"}
{"id": "https://github.com/NoviceLive/intellicoder/blob/6cac5ebfce65c370dbebe47756a1789b120ef982/intellicoder/transformers.py#L59-L66", "method_name": "replace_source", "code": "def replace_source(self, body, name):\n         logging.debug(_(\"Processing function body: %s\"), name)\n         replaced = re.sub(\n             self.FUNC_NAME_RE, self._func_replacer, body)\n         replaced = re.sub(\n             self.STR_LITERAL_RE, self._string_replacer, replaced)\n         return self._build_strings() + replaced\n         return replaced", "query": "replace in file"}
{"id": "https://github.com/KarchinLab/probabilistic2020/blob/5d70583b0a7c07cfe32e95f3a70e05df412acb84/prob2020/python/permutation.py#L9-L96", "method_name": "deleterious_permutation", "code": "def deleterious_permutation(obs_del,\n                             context_counts,\n                             context_to_mut,\n                             seq_context,\n                             gene_seq,\n                             num_permutations=10000,\n                             stop_criteria=100,\n                             pseudo_count=0,\n                             max_batch=25000):\n     mycontexts = context_counts.index.tolist()\n     somatic_base = [base\n                     for one_context in mycontexts\n                     for base in context_to_mut[one_context]]\n     max_batch = min(num_permutations, max_batch)\n     num_batches = num_permutations // max_batch\n     remainder = num_permutations % max_batch\n     batch_sizes = [max_batch] * num_batches\n     if remainder:\n         batch_sizes += [remainder]\n     num_sim = 0\n     null_del_ct = 0\n     for j, batch_size in enumerate(batch_sizes):\n         if null_del_ct >= stop_criteria:\n             break\n         tmp_contxt_pos = seq_context.random_pos(context_counts.iteritems(),\n                                                 batch_size)\n         tmp_mut_pos = np.hstack(pos_array for base, pos_array in tmp_contxt_pos)\n         for i, row in enumerate(tmp_mut_pos):\n             tmp_mut_info = mc.get_aa_mut_info(row,\n                                               somatic_base,\n                                               gene_seq)\n             tmp_del_count = cutils.calc_deleterious_info(tmp_mut_info[\"Reference AA\"],\n                                                          tmp_mut_info[\"Somatic AA\"],\n                                                          tmp_mut_info[\"Codon Pos\"])\n             if tmp_del_count >= obs_del: null_del_ct += 1\n             if null_del_ct >= stop_criteria:\n                 break\n         num_sim += i + 1\n     del_pval = float(null_del_ct) / (num_sim)\n     return del_pval", "query": "all permutations of a list"}
{"id": "https://github.com/CivicSpleen/ambry/blob/d7f2be4bf1f7ffd086f3fadd4fcae60c32473e42/ambry/orm/database.py#L785-L793", "method_name": "migrate", "code": "def migrate(self, connection):\n         if connection.engine.name == \"sqlite\":\n             self._migrate_sqlite(connection)\n         elif connection.engine.name == \"postgresql\":\n             self._migrate_postgresql(connection)\n         else:\n             raise DatabaseMissingError(\n                 \"Do not know how to migrate {} engine.\".format(self.connection))", "query": "postgresql connection"}
{"id": "https://github.com/census-instrumentation/opencensus-python/blob/992b223f7e34c5dcb65922b7d5c827e7a1351e7d/opencensus/trace/propagation/binary_format.py#L95-L136", "method_name": "from_header", "code": "def from_header(self, binary):\n         if binary is None:\n             return span_context_module.SpanContext(from_header=False)\n         try:\n             data = Header._make(struct.unpack(BINARY_FORMAT, binary))\n         except struct.error:\n             logging.warning(\n                 \"Cannot parse the incoming binary data {}, \"\n                 \"wrong format. Total bytes length should be {}.\".format(\n                     binary, FORMAT_LENGTH\n                 )\n             )\n             return span_context_module.SpanContext(from_header=False)\n         trace_id = str(binascii.hexlify(data.trace_id).decode(UTF8))\n         span_id = str(binascii.hexlify(data.span_id).decode(UTF8))\n         trace_options = TraceOptions(data.trace_option)\n         span_context = span_context_module.SpanContext(\n                 trace_id=trace_id,\n                 span_id=span_id,\n                 trace_options=trace_options,\n                 from_header=True)\n         return span_context", "query": "parse binary file to custom class"}
{"id": "https://github.com/mirukan/lunafind/blob/77bdfe02df98a7f74d0ae795fee3b1729218995d/lunafind/order.py#L53-L79", "method_name": "sort", "code": "def sort(posts: List[Post], by: str) -> List[Post]:\n     by_val  = by.replace(\"asc_\", \"\").replace(\"desc_\", \"\")\n     in_dict = (ORDER_NUM   if by_val in ORDER_NUM   else\n                ORDER_DATE  if by_val in ORDER_DATE  else\n                ORDER_FUNCS if by_val in ORDER_FUNCS else None)\n     if not in_dict:\n         raise ValueError(\n             f\"Got {by_val!r} as ordering method, must be one of: %s\" %\n             \", \".join(set(ORDER_NUM)   set(ORDER_DATE)   set(ORDER_FUNCS))\n         )\n     if in_dict == ORDER_FUNCS:\n         posts.sort(key=ORDER_FUNCS[by], reverse=(by != \"random\"))\n         return posts\n     by_full = by if by.startswith(\"asc_\") or by.startswith(\"desc_\") else \\\n               f\"%s_{by}\" % in_dict[by][0]\n     def sort_key(post: Post) -> int:\n         key = in_dict[by_val][1]\n         key = post.info[key] if not callable(key) else key(post.info)\n         return pend.parse(key) if in_dict == ORDER_DATE else key\n     posts.sort(key=sort_key, reverse=by_full.startswith(\"desc_\"))\n     return posts", "query": "sort string list"}
{"id": "https://github.com/quintusdias/glymur/blob/8b8fb091130fff00f1028dc82219e69e3f9baf6d/glymur/jp2k.py#L660-L667", "method_name": "_validate_image_datatype", "code": "def _validate_image_datatype(self, img_array):\n         if img_array.dtype != np.uint8 and img_array.dtype != np.uint16:\n             msg = (\"Only uint8 and uint16 datatypes are currently supported \"\n                    \"when writing.\")\n             raise RuntimeError(msg)", "query": "converting uint8 array to image"}
{"id": "https://github.com/saghul/evergreen/blob/22f22f45892f397c23c3e09e6ea1ad4c00b3add8/evergreen/io/stream.py#L96-L102", "method_name": "_read_from_buffer", "code": "def _read_from_buffer(self, delimiter=None, nbytes=None, regex=None):\n         if nbytes is not None:\n             return self._read_buffer.read(nbytes)\n         elif delimiter is not None:\n             return self._read_buffer.read_until(delimiter)\n         elif regex is not None:\n             return self._read_buffer.read_until_regex(regex)", "query": "buffered file reader read text"}
{"id": "https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/core_data.py#L311-L318", "method_name": "find", "code": "def find(self, txt):\n         result = []\n         for e in self.table:\n             print(\"find(self, txt) e = \", e)\n             if txt in str(e):\n                 result.append(e)\n         return result", "query": "find int in string"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L139-L152", "method_name": "get_table_location", "code": "def get_table_location(self, database_name, table_name):\n         table = self.get_table(database_name, table_name)\n         return table[\"StorageDescriptor\"][\"Location\"]", "query": "get database table name"}
{"id": "https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L238-L241", "method_name": "_parse_url", "code": "def _parse_url(url):\n     p = urlsplit(url)\n     query = {k: v[0] for k, v in parse_qs(p.query).items() if len(v) == 1}\n     return \"\".join([p.netloc, p.path]), query", "query": "parse query string in url"}
{"id": "https://github.com/costastf/locationsharinglib/blob/dcd74b0cdb59b951345df84987238763e50ef282/_CI/library/core_library.py#L186-L206", "method_name": "get_binary_path", "code": "def get_binary_path(executable, logging_level=\"INFO\"):\n     if sys.platform == \"win32\":\n         if executable == \"start\":\n             return executable\n         executable = executable + \".exe\"\n         if executable in os.listdir(\".\"):\n             binary = os.path.join(os.getcwd(), executable)\n         else:\n             binary = next((os.path.join(path, executable)\n                            for path in os.environ[\"PATH\"].split(os.pathsep)\n                            if os.path.isfile(os.path.join(path, executable))), None)\n     else:\n         venv_parent = get_venv_parent_path()\n         venv_bin_path = os.path.join(venv_parent, \".venv\", \"bin\")\n         if not venv_bin_path in os.environ.get(\"PATH\"):\n             if logging_level == \"DEBUG\":\n                 print(f\"Adding path {venv_bin_path} to environment PATH variable\")\n             os.environ[\"PATH\"] = os.pathsep.join([os.environ[\"PATH\"], venv_bin_path])\n         binary = shutil.which(executable)\n     return binary if binary else None", "query": "get executable path"}
{"id": "https://github.com/danielfrg/datasciencebox/blob/6b7aa642c6616a46547035fcb815acc1de605a6f/datasciencebox/core/cloud/instance.py#L80-L83", "method_name": "get_ip", "code": "def get_ip(self):\n         if self._ip is None:\n             self._ip = self.fetch_ip()\n         return self._ip", "query": "get current ip address"}
{"id": "https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L52-L57", "method_name": "aes_encrypt", "code": "def aes_encrypt(key, data, mode=\"ECB\", iv=None):\n     aes = AES()\n     aes.mode = mode\n     aes.iv = iv\n     aes.key = key\n     return aes.encrypt(data)", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/djtaylor/python-lsbinit/blob/a41fc551226f61ac2bf1b8b0f3f5395db85e75a2/lsbinit/pid.py#L43-L78", "method_name": "ps", "code": "def ps(self):\n         pid = self.get()\n         parent   = None\n         children = []\n         if pid:\n             proc   = Popen([\"ps\", \"-ef\"], stdout=PIPE)\n             for _line in proc.stdout.readlines():\n                 line = self.unicode(_line.rstrip())\n                 this_pid, this_parent = self._ps_extract_pid(line)\n                 try:\n                     if int(pid) == int(this_parent):\n                         children.append(\"{}; [{}]\".format(this_pid.rstrip(), re.sub(\" +\", \" \", line)))\n                     if int(pid) == int(this_pid):\n                         parent = re.sub(\" +\", \" \", line)\n                 except ValueError:\n                     continue\n         return (parent, children)", "query": "get current process id"}
{"id": "https://github.com/hit9/rux/blob/d7f60722658a3b83ac6d7bb3ca2790ac9c926b59/rux/parser.py#L33-L42", "method_name": "_code_no_lexer", "code": "def _code_no_lexer(self, text):\n         text = text.encode(charset).strip()\n         return(\n              % houdini.escape_html(text)\n         )", "query": "html encode string"}
{"id": "https://github.com/pricingassistant/mongokat/blob/61eaf4bc1c4cc359c6f9592ec97b9a04d9561411/mongokat/collection.py#L136-L137", "method_name": "distinct", "code": "def distinct(self, *args, **kwargs):\n        return self._collection_with_options(kwargs).distinct(*args, **kwargs)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/supercoderz/pyeurofx/blob/3f579bb6e4836dadb187df8c74a9d186ae7e39e7/eurofx/common.py#L88-L98", "method_name": "get_date", "code": "def get_date(date_string):\n     d=None\n     try:\n         d=datetime.datetime.strptime(date_string, \"%d %B %Y\").date()\n     except:\n         d=datetime.datetime.strptime(date_string, \"%Y-%m-%d\").date()\n     if d:\n         return d.strftime(\"%Y-%m-%d\")\n     else:\n         return date_string", "query": "string to date"}
{"id": "https://github.com/gunthercox/ChatterBot/blob/1a03dcb45cba7bdc24d3db5e750582e0cb1518e2/chatterbot/parsing.py#L506-L517", "method_name": "convert_string_to_number", "code": "def convert_string_to_number(value):\n     if value is None:\n         return 1\n     if isinstance(value, int):\n         return value\n     if value.isdigit():\n         return int(value)\n     num_list = map(lambda s: NUMBERS[s], re.findall(numbers + \"+\", value.lower()))\n     return sum(num_list)", "query": "convert string to number"}
{"id": "https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/scheduler/garbage_collector.py#L81-L98", "method_name": "_flush_queue", "code": "def _flush_queue(self, q, ignore_priority=False):\n         assert isinstance(q, PriorityQueue)\n         current_timestamp = compute_release_time(lag_in_minutes=0)\n         for _ in range(len(q)):\n             entry = q.pop()\n             assert isinstance(entry, PriorityEntry)\n             if ignore_priority or entry.release_time < current_timestamp:\n                 self._resubmit_uow(entry.entry)\n             else:\n                 q.put(entry)\n                 break", "query": "priority queue"}
{"id": "https://github.com/alejoe91/MEAutility/blob/7c2b0da52c2752a3baf04e8e248e26b0769cd088/MEAutility/core.py#L326-L338", "method_name": "_set_normal", "code": "def _set_normal(self, normal):\n         for i, el in enumerate(self.electrodes):\n             el.normal = normal/np.linalg.norm(normal)", "query": "normal distribution"}
{"id": "https://github.com/LasLabs/python-helpscout/blob/84bf669417d72ca19641a02c9a660e1ae4271de4/helpscout/request_paginator/__init__.py#L114-L123", "method_name": "post", "code": "def post(self, json=None):\n         return self._call(\"post\", url=self.endpoint, json=json)", "query": "httpclient post json"}
{"id": "https://github.com/espressif/esptool/blob/c583756c118039cfcfe256f7a3285618914d16a5/ecdsa/ecdsa.py#L169-L175", "method_name": "string_to_int", "code": "def string_to_int( s ):\n   result = 0\n   for c in s:\n     if not isinstance(c, int): c = ord( c )\n     result = 256 * result + c\n   return result", "query": "convert int to string"}
{"id": "https://github.com/zeekay/soundcloud-cli/blob/8a83013683e1acf32f093239bbb6d3c02bc50b37/soundcloud_cli/utils.py#L7-L23", "method_name": "copy_to_clipboard", "code": "def copy_to_clipboard(text):\n     if sys.platform == \"darwin\":\n         os.system(\"echo \"{0}\"   pbcopy\".format(text))\n         return\n     try:\n         from Tkinter import Tk\n     except ImportError:\n         return\n     r = Tk()\n     r.withdraw()\n     r.clipboard_clear()\n     r.clipboard_append(text.encode(\"ascii\"))\n     r.destroy()", "query": "copy to clipboard"}
{"id": "https://github.com/marvin-ai/marvin-python-toolbox/blob/7c95cb2f9698b989150ab94c1285f3a9eaaba423/marvin_python_toolbox/common/utils.py#L286-L295", "method_name": "url_encode", "code": "def url_encode(url):\n     if isinstance(url, text_type):\n         url = url.encode(\"utf8\")\n     return quote(url, \":/%?&=\")", "query": "encode url"}
{"id": "https://github.com/EntilZha/PyFunctional/blob/ac04e4a8552b0c464a7f492f7c9862424867b63e/functional/io.py#L98-L113", "method_name": "__iter__", "code": "def __iter__(self):\n         if \"t\" in self.mode:\n             with gzip.GzipFile(self.path, compresslevel=self.compresslevel) as gz_file:\n                 gz_file.read1 = gz_file.read\n                 with io.TextIOWrapper(gz_file,\n                                       encoding=self.encoding,\n                                       errors=self.errors,\n                                       newline=self.newline) as file_content:\n                     for line in file_content:\n                         yield line\n         else:\n             with gzip.open(self.path,\n                            mode=self.mode,\n                            compresslevel=self.compresslevel) as file_content:\n                 for line in file_content:\n                     yield line", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/mbakker7/timml/blob/91e99ad573cb8a9ad8ac1fa041c3ca44520c2390/timml/linesink.py#L651-L670", "method_name": "initialize", "code": "def initialize(self):\n         LineSinkStringBase.initialize(self)\n         self.aq.add_element(self)\n         if len(self.hls) == 1:\n             self.pc = self.hls * self.aq.T[self.layers] * np.ones(self.nparam)\n         elif len(self.hls) == self.nls:  \n             self.pc = (self.hls[:, np.newaxis] * self.aq.T[self.layers]).flatten()\n         elif len(self.hls) == 2:\n             L = np.array([ls.L for ls in self.lslist])\n             Ltot = np.sum(L)\n             xp = np.zeros(self.nls)\n             xp[0] = 0.5 * L[0]\n             for i in range(1, self.nls):\n                 xp[i] = xp[i - 1] + 0.5 * (L[i - 1] + L[i])\n             self.hls = np.interp(xp, [0, Ltot], self.hls)\n             self.pc = (self.hls[:, np.newaxis] * self.aq.T[self.layers]).flatten()\n         else:\n             print(\"Error: hls entry not supported\")\n         self.resfac = 0.0", "query": "initializing array"}
{"id": "https://github.com/tehmaze/ipcalc/blob/d436b95d2783347c3e0084d76ec3c52d1f5d2f0b/ipcalc.py#L544-L558", "method_name": "to_reverse", "code": "def to_reverse(self):\n         if self.v == 4:\n             return \".\".join(list(self.dq.split(\".\")[::-1]) + [\"in-addr\", \"arpa\"])\n         else:\n             return \".\".join(list(self.hex())[::-1] + [\"ip6\", \"arpa\"])", "query": "reverse a string"}
{"id": "https://github.com/Seeed-Studio/wio-cli/blob/ce83f4c2d30be7f72d1a128acd123dfc5effa563/wio/commands/cmd_setup.py#L217-L384", "method_name": "serial_send", "code": "def serial_send(msvr, msvr_ip, xsvr, xsvr_ip, node_sn, node_key, port):\n     thread = termui.waiting_echo(\"Getting device information...\")\n     thread.daemon = True\n     thread.start()\n     flag = False\n     try:\n         with serial.Serial(port, 115200, timeout=5) as ser:\n             cmd = \"Blank?\\r\n\"\n             ser.write(cmd.encode(\"utf-8\"))\n             if \"Node\" in ser.readline():\n                 flag = True\n     except serial.SerialException as e:\n         thread.stop(\"\")\n         thread.join()\n         click.secho(\">> \", fg=\"red\", nl=False)\n         click.echo(e)\n         if e.errno == 13:\n             click.echo(\"For more information, see https://github.com/Seeed-Studio/wio-cli\n         return None\n     thread.stop(\"\")\n     thread.join()\n     if flag:\n         click.secho(\"> \", fg=\"green\", nl=False)\n         click.secho(\"Found Wio.\", fg=\"green\", bold=True)\n         click.echo()\n     else:\n         click.secho(\"> \", fg=\"green\", nl=False)\n         click.secho(\"No nearby Wio detected.\", fg=\"white\", bold=True)\n         if click.confirm(click.style(\"? \", fg=\"green\") +\n                 click.style(\"Would you like to wait and monitor for Wio entering configure mode\", bold=True),\n                 default=True):\n             thread = termui.waiting_echo(\"Waiting for a wild Wio to appear... (press ctrl + C to exit)\")\n             thread.daemon = True\n             thread.start()\n             flag = False\n             while 1:\n                 with serial.Serial(port, 115200, timeout=5) as ser:\n                     cmd = \"Blank?\\r\n\"\n                     ser.write(cmd.encode(\"utf-8\"))\n                     if \"Node\" in ser.readline():\n                         flag = True\n                         break\n             thread.stop(\"\")\n             thread.join()\n             click.secho(\"> \", fg=\"green\", nl=False)\n             click.secho(\"Found Wio.\", fg=\"green\", bold=True)\n             click.echo()\n         else:\n             click.secho(\"> \", fg=\"green\", nl=False)\n             click.secho(\"\nQuit wio setup!\", bg=\"white\", bold=True)\n     while 1:\n         if not click.confirm(click.style(\"? \", fg=\"green\") +\n                     click.style(\"Would you like to manually enter your Wi-Fi network configuration?\", bold=True),\n                     default=False):\n             thread = termui.waiting_echo(\"Asking the Wio to scan for nearby Wi-Fi networks...\")\n             thread.daemon = True\n             thread.start()\n             flag = False\n             with serial.Serial(port, 115200, timeout=3) as ser:\n                 cmd = \"SCAN\\r\n\"\n                 ser.write(cmd.encode(\"utf-8\"))\n                 ssid_list = []\n                 while True:\n                     ssid = ser.readline()\n                     if ssid == \"\\r\n\":\n                         flag = True\n                         break\n                     ssid = ssid.strip(\"\\r\n\")\n                     ssid_list.append(ssid)\n             if flag:\n                 thread.stop(\"\")\n                 thread.join()\n             else:\n                 thread.stop(\"\\rsearch failure...\n\")\n                 return None\n             while 1:\n                 for x in range(len(ssid_list)):\n                     click.echo(\"%s.) %s\" %(x, ssid_list[x]))\n                 click.secho(\"? \", fg=\"green\", nl=False)\n                 value = click.prompt(\n                             click.style(\"Please select the network to which your Wio should connect\", bold=True),\n                             type=int)\n                 if value >= 0 and value < len(ssid_list):\n                     ssid = ssid_list[value]\n                     break\n                 else:\n                     click.echo(click.style(\">> \", fg=\"red\") + \"invalid input, range 0 to %s\" %(len(ssid_list)-1))\n             ap = ssid\n         else:\n             ap = click.prompt(click.style(\"> \", fg=\"green\") +\n                 click.style(\"Please enter the SSID of your Wi-Fi network\", bold=True), type=str)\n         ap_pwd = click.prompt(click.style(\"> \", fg=\"green\") +\n             click.style(\"Please enter your Wi-Fi network password (leave blank for none)\", bold=True),\n             default=\"\", show_default=False)\n         d_name = click.prompt(click.style(\"> \", fg=\"green\") +\n         click.style(\"Please enter the name of a device will be created\", bold=True), type=str)\n         click.echo(click.style(\"> \", fg=\"green\") + \"Here\"s what we\"re going to send to the Wio:\")\n         click.echo()\n         click.echo(click.style(\"> \", fg=\"green\") + \"Wi-Fi network: \" +\n             click.style(ap, fg=\"green\", bold=True))\n         ap_pwd_p = ap_pwd\n         if ap_pwd_p == \"\":\n             ap_pwd_p = \"None\"\n         click.echo(click.style(\"> \", fg=\"green\") + \"Password: \" +\n             click.style(ap_pwd_p, fg=\"green\", bold=True))\n         click.echo(click.style(\"> \", fg=\"green\") + \"Device name: \" +\n             click.style(d_name, fg=\"green\", bold=True))\n         click.echo()\n         if click.confirm(click.style(\"? \", fg=\"green\") +\n             \"Would you like to continue with the information shown above?\", default=True):\n             break\n     click.echo()\n     thread = termui.waiting_echo(\"Sending Wi-Fi information to device...\")\n     thread.daemon = True\n     thread.start()\n     version = 1.1\n     with serial.Serial(port, 115200, timeout=10) as ser:\n         cmd = \"VERSION\\r\n\"\n         ser.write(cmd.encode(\"utf-8\"))\n         res = ser.readline()\n         try:\n             version = float(re.match(r\"([0-9]+.[0-9]+)\", res).group(0))\n         except Exception as e:\n             version = 1.1\n     send_flag = False\n     while 1:\n         with serial.Serial(port, 115200, timeout=10) as ser:\n             if version <= 1.1:\n                 cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\n\" %(ap, ap_pwd, node_key, node_sn, xsvr_ip, msvr_ip)\n             elif version >= 1.2:\n                 cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\n\" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)\n             else:\n                 cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\n\" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)\n             ser.write(cmd.encode(\"utf-8\"))\n             if \"ok\" in ser.readline():\n                 click.echo(click.style(\"\\r> \", fg=\"green\") + \"Send Wi-Fi information to device success.\")\n                 thread.stop(\"\")\n                 thread.join()\n                 send_flag = True\n         if send_flag:\n             break\n     if send_flag:\n         return {\"name\": d_name}\n     else:\n         return None", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/jmbeach/KEP.py/blob/68cda64ab649640a486534867c81274c41e39446/src/keppy/tag.py#L31-L33", "method_name": "name_replace", "code": "def name_replace(self, to_replace, replacement):\n        self.name = self.name.replace(to_replace, replacement)", "query": "replace in file"}
{"id": "https://github.com/internetarchive/warc/blob/8f05a000a23bbd6501217e37cfd862ffdf19da7f/warc/warc.py#L260-L263", "method_name": "reader", "code": "def reader(self):\n         if self._reader is None:\n             self._reader = WARCReader(self.fileobj)\n         return self._reader", "query": "buffered file reader read text"}
{"id": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeregression.py#L292-L310", "method_name": "regression", "code": "def regression(self, slope=None):\n         self._calculate_averages()\n         clock_model = base_regression(self.tree.root.Q, slope)\n         clock_model[\"r_val\"] = self.explained_variance()\n         return clock_model", "query": "linear regression"}
{"id": "https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/processors.py#L91-L95", "method_name": "__call__", "code": "def __call__(self, value):\n         for substring in self.substrings:\n             if value.startswith(substring):\n                 return value[len(substring):]\n         return value", "query": "positions of substrings in string"}
{"id": "https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xpushbutton.py#L72-L90", "method_name": "setShowRichText", "code": "def setShowRichText(self, state): \n         self._showRichText = state \n         text = self.text() \n         if state: \n             label = self.richTextLabel() \n             label.setText(text) \n             label.show() \n             super(XPushButton, self).setText(\"\") \n         else: \n             if self._richTextLabel: \n                 self._richTextLabel.hide() \n             super(XPushButton, self).setText(text)", "query": "underline text in label widget"}
{"id": "https://github.com/dshean/pygeotools/blob/5ac745717c0098d01eb293ff1fe32fd7358c76ab/pygeotools/lib/malib.py#L653-L667", "method_name": "linreg", "code": "def linreg(self, rsq=False, conf_test=False):\n         model=\"linear\"\n         if not self.datestack:\n             self.compute_dt_stats()\n         if np.isnan(self.min_dt_ptp):\n             max_dt_ptp = calcperc(self.dt_stack_ptp, (4, 96))[1]\n             self.min_dt_ptp = 0.20 * max_dt_ptp\n         if self.robust:\n             model=\"theilsen\"\n         print(\"Compute stack linear trend with model: %s\" % model)\n         self.stack_trend, self.stack_intercept, self.stack_detrended_std = \\\n                 ma_linreg(self.ma_stack, self.date_list, dt_stack_ptp=self.dt_stack_ptp, min_dt_ptp=self.min_dt_ptp, \\\n                 n_thresh=self.n_thresh, model=model, rsq=False, conf_test=False, smooth=False, n_cpu=self.n_cpu)", "query": "linear regression"}
{"id": "https://github.com/DenisCarriere/geocoder/blob/39b9999ec70e61da9fa52fe9fe82a261ad70fa8b/geocoder/opencage.py#L409-L415", "method_name": "_catch_errors", "code": "def _catch_errors(self, json_response):\n         status = json_response.get(\"status\")\n         if status and status.get(\"code\") != 200:\n             self.status_code = status.get(\"code\")\n             self.error = status.get(\"message\")\n         return self.error", "query": "get the description of a http status code"}
{"id": "https://github.com/bokeh/bokeh/blob/dc8cf49e4e4302fd38537ad089ece81fbcca4737/examples/app/stocks/download_sample_data.py#L27-L61", "method_name": "extract_zip", "code": "def extract_zip(zip_name, exclude_term=None):\n     zip_dir = os.path.dirname(os.path.abspath(zip_name))\n     try:\n         with zipfile.ZipFile(zip_name) as z:\n             files = [zip_file for zip_file in z.namelist() if not zip_file.endswith(\"/\")]\n             print(\"Extracting %i files from %r.\" % (len(files), zip_name))\n             for zip_file in files:\n                 if exclude_term:\n                     dest_file = zip_file.replace(exclude_term, \"\")\n                 else:\n                     dest_file = zip_file\n                 dest_file = os.path.normpath(os.path.join(zip_dir, dest_file))\n                 dest_dir = os.path.dirname(dest_file)\n                 if not os.path.isdir(dest_dir):\n                     os.makedirs(dest_dir)\n                 data = z.read(zip_file)\n                 with open(dest_file, \"wb\") as f:\n                     f.write(encode_utf8(data))\n     except zipfile.error as e:\n         print(\"Bad zipfile (%r): %s\" % (zip_name, e))\n         raise e", "query": "extract zip file recursively"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/collectionseditor.py#L1242-L1252", "method_name": "paste", "code": "def paste(self): \n         clipboard = QApplication.clipboard() \n         cliptext = \"\" \n         if clipboard.mimeData().hasText(): \n             cliptext = to_text_string(clipboard.text()) \n         if cliptext.strip(): \n             self.import_from_string(cliptext, title=_(\"Import from clipboard\")) \n         else: \n             QMessageBox.warning(self, _( \"Empty clipboard\"), \n                                 _(\"Nothing to be imported from clipboard.\"))", "query": "copy to clipboard"}
{"id": "https://github.com/buildinspace/peru/blob/76e4012c6c34e85fb53a4c6d85f4ac3633d93f77/peru/resources/plugins/curl/curl_plugin.py#L117-L137", "method_name": "extract_zip", "code": "def extract_zip(archive_path, dest):\n     with zipfile.ZipFile(archive_path) as z:\n         validate_filenames(z.namelist())\n         z.extractall(dest)\n         for info in z.filelist:\n             if not info.filename.endswith(\"/\"):\n                 mode = (info.external_attr >> 16) & 0o777\n                 if mode & stat.S_IXUSR:\n                     os.chmod(os.path.join(dest, info.filename), 0o755)", "query": "extract zip file recursively"}
{"id": "https://github.com/lrq3000/pyFileFixity/blob/fd5ef23bb13835faf1e3baa773619b86a1cc9bdf/pyFileFixity/lib/profilers/pyinstrument/profiler.py#L159-L185", "method_name": "output_html", "code": "def output_html(self, root=False):\n         resources_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources/\")\n         with open(os.path.join(resources_dir, \"style.css\")) as f:\n             css = f.read()\n         with open(os.path.join(resources_dir, \"profile.js\")) as f:\n             js = f.read()\n         with open(os.path.join(resources_dir, \"jquery-1.11.0.min.js\")) as f:\n             jquery_js = f.read()\n         body = self.starting_frame(root).as_html()\n         page = .format(css=css, js=js, jquery_js=jquery_js, body=body)\n         return page", "query": "output to html file"}
{"id": "https://github.com/espressif/esptool/blob/c583756c118039cfcfe256f7a3285618914d16a5/ecdsa/ecdsa.py#L169-L175", "method_name": "string_to_int", "code": "def string_to_int( s ):\n   result = 0\n   for c in s:\n     if not isinstance(c, int): c = ord( c )\n     result = 256 * result + c\n   return result", "query": "convert int to string"}
{"id": "https://github.com/nickoala/telepot/blob/3792fde251d0f1d5a6ca16c8ad1a71f89360c41d/telepot/text.py#L65-L88", "method_name": "apply_entities_as_html", "code": "def apply_entities_as_html(text, entities):\n     escapes = {\"<\": \"&lt;\",\n                \">\": \"&gt;\",\n                \"&\": \"&amp;\",}\n     formatters = {\"bold\":         lambda s,e: \"<b>\"+s+\"</b>\",\n                   \"italic\":       lambda s,e: \"<i>\"+s+\"</i>\",\n                   \"text_link\":    lambda s,e: \"<a href=\"\"+e[\"url\"]+\"\">\"+s+\"</a>\",\n                   \"text_mention\": lambda s,e: \"<a href=\"tg://user?id=\"+str(e[\"user\"][\"id\"])+\"\">\"+s+\"</a>\",\n                   \"code\":         lambda s,e: \"<code>\"+s+\"</code>\",\n                   \"pre\":          lambda s,e: \"<pre>\"+s+\"</pre>\"}\n     return _apply_entities(text, entities, escapes, formatters)", "query": "html entities replace"}
{"id": "https://github.com/proycon/flat/blob/f14eea61edcae8656dadccd9a43481ff7e710ffb/flat/comm.py#L116-L135", "method_name": "postjson", "code": "def postjson( request, url, data):\n     if isinstance(data, dict) or isinstance(data,list) or isinstance(data, tuple):\n         data = json.dumps(data)\n     if url and url[0] == \"/\": url = url[1:]\n     docservereq = Request(\"http://\" + settings.FOLIADOCSERVE_HOST + \":\" + str(settings.FOLIADOCSERVE_PORT) + \"/\" + url + \"/\" + sid) \n     setsid(docservereq, getsid(request))\n     docservereq.add_header(\"Content-Type\", \"application/json\")\n     f = urlopen(docservereq, urlencode(data).encode(\"utf-8\"))\n     if sys.version < \"3\":\n         contents = unicode(f.read(),\"utf-8\")\n     else:\n         contents = str(f.read(),\"utf-8\")\n     f.close()\n     if contents and contents[0] == \"{\":\n         return json.loads(contents)\n     elif contents:\n         return contents\n     else:\n         return None", "query": "httpclient post json"}
{"id": "https://github.com/ml31415/numpy-groupies/blob/0911e9c59b14e11319e82d0876056ad2a17e6568/numpy_groupies/aggregate_numpy.py#L181-L185", "method_name": "_sort", "code": "def _sort(group_idx, a, size=None, fill_value=None, dtype=None, reverse=False):\n     sortidx = np.lexsort((-a if reverse else a, group_idx))\n     revidx = np.argsort(np.argsort(group_idx, kind=\"mergesort\"), kind=\"mergesort\")\n     return a[sortidx][revidx]", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L136-L140", "method_name": "__mul__", "code": "def __mul__(self, other):\n         if isinstance(other, Matrix):\n             return Matrix(self.matrix.dot(other.matrix))\n         else:\n             return Matrix(self.matrix * other)", "query": "matrix multiply"}
{"id": "https://github.com/robinandeer/puzzle/blob/9476f05b416d3a5135d25492cb31411fdf831c58/puzzle/plugins/sql/store.py#L62-L86", "method_name": "connect", "code": "def connect(self, db_uri, debug=False):\n         kwargs = {\"echo\": debug, \"convert_unicode\": True}\n         if \"mysql\" in db_uri:\n             kwargs[\"pool_recycle\"] = 3600\n         elif \"://\" not in db_uri:\n             logger.debug(\"detected sqlite path URI: {}\".format(db_uri))\n             db_path = os.path.abspath(os.path.expanduser(db_uri))\n             db_uri = \"sqlite:///{}\".format(db_path)\n         self.engine = create_engine(db_uri, **kwargs)\n         logger.debug(\"connection established successfully\")\n         BASE.metadata.bind = self.engine\n         self.session = scoped_session(sessionmaker(bind=self.engine))\n         self.query = self.session.query\n         return self", "query": "connect to sql"}
{"id": "https://github.com/fy0/slim/blob/9951a910750888dbe7dd3e98acae9c40efae0689/slim/base/view.py#L128-L136", "method_name": "get_ip", "code": "async def get_ip(self) -> Union[IPv4Address, IPv6Address]:\n         xff = await self.get_x_forwarded_for()\n         if xff: return xff[0]\n         ip_addr = self._request.transport.get_extra_info(\"peername\")[0]\n         return ip_address(ip_addr)", "query": "get current ip address"}
{"id": "https://github.com/poldracklab/niworkflows/blob/254f4b4fcc5e6ecb29d2f4602a30786b913ecce5/niworkflows/interfaces/utils.py#L793-L864", "method_name": "_tsv2json", "code": "def _tsv2json(in_tsv, out_json, index_column, additional_metadata=None,\n               drop_columns=None, enforce_case=True):\n     import pandas as pd\n     re_to_camel = r\"(.*?)_([a-zA-Z0-9])\"\n     re_to_snake = r\"(^.+? .*?)((?<![_A-Z])[A-Z] (?<![_0-9])[0-9]+)\"\n     def snake(match):\n         return \"{}_{}\".format(match.group(1).lower(), match.group(2).lower())\n     def camel(match):\n         return \"{}{}\".format(match.group(1), match.group(2).upper())\n     def less_breakable(a_string):\n         return \"\".join(a_string.split()).strip(\"\n     drop_columns = drop_columns or []\n     additional_metadata = additional_metadata or {}\n     tsv_data = pd.read_csv(in_tsv, \"\\t\")\n     for k, v in additional_metadata.items():\n         tsv_data[k] = v\n     for col in drop_columns:\n         tsv_data.drop(labels=col, axis=\"columns\", inplace=True)\n     tsv_data.set_index(index_column, drop=True, inplace=True)\n     if enforce_case:\n         tsv_data.index = [re.sub(re_to_snake, snake,\n                                  less_breakable(i), 0).lower()\n                           for i in tsv_data.index]\n         tsv_data.columns = [re.sub(re_to_camel, camel,\n                                    less_breakable(i).title(), 0)\n                             for i in tsv_data.columns]\n     json_data = tsv_data.to_json(orient=\"index\")\n     json_data = json.JSONDecoder(\n         object_pairs_hook=OrderedDict).decode(json_data)\n     if out_json is None:\n         return json_data\n     with open(out_json, \"w\") as f:\n         json.dump(json_data, f, indent=4)\n     return out_json", "query": "convert json to csv"}
{"id": "https://github.com/fermiPy/fermipy/blob/9df5e7e3728307fd58c5bba36fd86783c39fbad4/fermipy/timing.py#L14-L21", "method_name": "elapsed_time", "code": "def elapsed_time(self):\n         if self._t0 is not None:\n             return self._time + self._get_time()\n         else:\n             return self._time", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/filt_obj.py#L319-L344", "method_name": "fuzzmatch", "code": "def fuzzmatch(self, fuzzkey, multi=False):\n         keys, ratios = np.array([(f, seqm(None, fuzzkey, f).ratio()) for f in self.components.keys()]).T\n         mratio = max(ratios)\n         if multi:\n             return keys[ratios == mratio]\n         else:\n             if sum(ratios == mratio) == 1:\n                 return keys[ratios == mratio][0]\n             else:\n                 raise ValueError(\"\nThe filter key provided (\"{:}\") matches two or more filter names equally well:\n\".format(fuzzkey) + \", \".join(keys[ratios == mratio]) + \"\nPlease be more specific!\")", "query": "fuzzy match ranking"}
{"id": "https://github.com/cloudendpoints/endpoints-python/blob/00dd7c7a52a9ee39d5923191c2604b8eafdb3f24/endpoints/errors.py#L239-L253", "method_name": "_get_status_code", "code": "def _get_status_code(self, http_status):\n     try:\n       return int(http_status.split(\" \", 1)[0])\n     except TypeError:\n       _logger.warning(\"Unable to find status code in HTTP status %r.\",\n                       http_status)\n     return 500", "query": "get the description of a http status code"}
{"id": "https://github.com/django-extensions/django-extensions/blob/7e0bef97ea6cb7f9eea5e2528e3a985a83a7b9b8/django_extensions/management/commands/sqldsn.py#L100-L141", "method_name": "_postgresql", "code": "def _postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):  \n         dsn = []\n         if dsn_style is None or dsn_style == \"all\" or dsn_style == \"keyvalue\":\n             dsnstr = \"host=\"{0}\" dbname=\"{2}\" user=\"{3}\" password=\"{4}\"\"\n             if dbport is not None:\n                 dsnstr += \" port=\"{1}\"\"\n             dsn.append(dsnstr.format(dbhost,\n                                      dbport,\n                                      dbname,\n                                      dbuser,\n                                      dbpass,))\n         if dsn_style == \"all\" or dsn_style == \"kwargs\":\n             dsnstr = \"host=\"{0}\", database=\"{2}\", user=\"{3}\", password=\"{4}\"\"\n             if dbport is not None:\n                 dsnstr += \", port=\"{1}\"\"\n             dsn.append(dsnstr.format(dbhost,\n                                      dbport,\n                                      dbname,\n                                      dbuser,\n                                      dbpass))\n         if dsn_style == \"all\" or dsn_style == \"uri\":\n             dsnstr = \"postgresql://{user}:{password}@{host}/{name}\"\n             dsn.append(dsnstr.format(\n                 host=\"{host}:{port}\".format(host=dbhost, port=dbport) if dbport else dbhost,  \n                 name=dbname, user=dbuser, password=dbpass))\n         if dsn_style == \"all\" or dsn_style == \"pgpass\":\n             dsn.append(\":\".join(map(str, filter(\n                 None, [dbhost, dbport, dbname, dbuser, dbpass]))))\n         return dsn", "query": "postgresql connection"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/convert.py#L43-L73", "method_name": "convert_to_bool", "code": "def convert_to_bool(x: Any, default: bool = None) -> bool:\n     if isinstance(x, bool):\n         return x\n     if not x:  \n         return default\n     try:\n         return int(x) != 0\n     except (TypeError, ValueError):\n         pass\n     try:\n         return float(x) != 0\n     except (TypeError, ValueError):\n         pass\n     if not isinstance(x, str):\n         raise Exception(\"Unknown thing being converted to bool: {!r}\".format(x))\n     x = x.upper()\n     if x in [\"Y\", \"YES\", \"T\", \"TRUE\"]:\n         return True\n     if x in [\"N\", \"NO\", \"F\", \"FALSE\"]:\n         return False\n     raise Exception(\"Unknown thing being converted to bool: {!r}\".format(x))", "query": "convert int to bool"}
{"id": "https://github.com/jbarlow83/OCRmyPDF/blob/79c84eefa353632a3d7ccddbd398c6678c1c1777/src/ocrmypdf/hocrtransform.py#L156-L231", "method_name": "to_pdf", "code": "def to_pdf(\n         self,\n         outFileName,\n         imageFileName=None,\n         showBoundingboxes=False,\n         fontname=\"Helvetica\",\n         invisibleText=False,\n         interwordSpaces=False,\n     ):\n         pdf = Canvas(outFileName, pagesize=(self.width, self.height), pageCompression=1)\n         pdf.setStrokeColorRGB(0, 1, 1)\n         pdf.setFillColorRGB(0, 1, 1)\n         pdf.setLineWidth(0)  \n         for elem in self.hocr.findall(\".//%sp[@class=\"%s\"]\" % (self.xmlns, \"ocr_par\")):\n             elemtxt = self._get_element_text(elem).rstrip()\n             if len(elemtxt) == 0:\n                 continue\n             pxl_coords = self.element_coordinates(elem)\n             pt = self.pt_from_pixel(pxl_coords)\n             if showBoundingboxes:\n                 pdf.rect(\n                     pt.x1, self.height - pt.y2, pt.x2 - pt.x1, pt.y2 - pt.y1, fill=1\n                 )\n         found_lines = False\n         for line in self.hocr.findall(\n             \".//%sspan[@class=\"%s\"]\" % (self.xmlns, \"ocr_line\")\n         ):\n             found_lines = True\n             self._do_line(\n                 pdf,\n                 line,\n                 \"ocrx_word\",\n                 fontname,\n                 invisibleText,\n                 interwordSpaces,\n                 showBoundingboxes,\n             )\n         if not found_lines:\n             root = self.hocr.find(\".//%sdiv[@class=\"%s\"]\" % (self.xmlns, \"ocr_page\"))\n             self._do_line(\n                 pdf,\n                 root,\n                 \"ocrx_word\",\n                 fontname,\n                 invisibleText,\n                 interwordSpaces,\n                 showBoundingboxes,\n             )\n         if imageFileName is not None:\n             pdf.drawImage(imageFileName, 0, 0, width=self.width, height=self.height)\n         pdf.showPage()\n         pdf.save()", "query": "convert html to pdf"}
{"id": "https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/structural/cnvkit.py#L633-L646", "method_name": "_add_plots_to_output", "code": "def _add_plots_to_output(out, data):\n     out[\"plot\"] = {}\n     diagram_plot = _add_diagram_plot(out, data)\n     if diagram_plot:\n         out[\"plot\"][\"diagram\"] = diagram_plot\n     scatter = _add_scatter_plot(out, data)\n     if scatter:\n         out[\"plot\"][\"scatter\"] = scatter\n     scatter_global = _add_global_scatter_plot(out, data)\n     if scatter_global:\n         out[\"plot\"][\"scatter_global\"] = scatter_global\n     return out", "query": "scatter plot"}
{"id": "https://github.com/pgjones/quart/blob/7cb2d3bd98e8746025764f2b933abc12041fa175/quart/utils.py#L29-L59", "method_name": "create_cookie", "code": "def create_cookie(\n         key: str,\n         value: str=\"\",\n         max_age: Optional[Union[int, timedelta]]=None,\n         expires: Optional[Union[int, float, datetime]]=None,\n         path: str=\"/\",\n         domain: Optional[str]=None,\n         secure: bool=False,\n         httponly: bool=False,\n ) -> SimpleCookie:\n     cookie = SimpleCookie()\n     cookie[key] = value\n     cookie[key][\"path\"] = path\n     cookie[key][\"httponly\"] = httponly  \n     cookie[key][\"secure\"] = secure  \n     if isinstance(max_age, timedelta):\n         cookie[key][\"max-age\"] = f\"{max_age.total_seconds():d}\"\n     if isinstance(max_age, int):\n         cookie[key][\"max-age\"] = str(max_age)\n     if expires is not None and isinstance(expires, (int, float)):\n         cookie[key][\"expires\"] = format_date_time(int(expires))\n     elif expires is not None and isinstance(expires, datetime):\n         cookie[key][\"expires\"] = format_date_time(expires.replace(tzinfo=timezone.utc).timestamp())\n     if domain is not None:\n         cookie[key][\"domain\"] = domain\n     return cookie", "query": "create cookie"}
{"id": "https://github.com/collectiveacuity/labPack/blob/52949ece35e72e3cc308f54d9ffa6bfbd96805b8/labpack/records/time.py#L181-L206", "method_name": "fromEpoch", "code": "def fromEpoch(cls, epoch_time): \n         title = \"Epoch time input for labDT.fromEpoch\" \n         if not isinstance(epoch_time, float) and not isinstance(epoch_time, int): \n             raise TypeError(\"\n%s must be an integer or float.\" % title) \n         dT = datetime.utcfromtimestamp(epoch_time).replace(tzinfo=pytz.utc) \n         dt_kwargs = { \n             \"year\": dT.year, \n             \"month\": dT.month, \n             \"day\": dT.day, \n             \"hour\": dT.hour, \n             \"minute\": dT.minute, \n             \"second\": dT.second, \n             \"microsecond\": dT.microsecond, \n             \"tzinfo\": dT.tzinfo \n         } \n         return labDT(**dt_kwargs)", "query": "convert a utc time to epoch"}
{"id": "https://github.com/mapillary/mapillary_tools/blob/816785e90c589cae6e8e34a5530ce8417d29591c/mapillary_tools/gpmf.py#L72-L133", "method_name": "parse_bin", "code": "def parse_bin(path):\n     f = open(path, \"rb\")\n     s = {}  \n     output = []\n     methods = {\n         \"GPS5\": parse_gps,\n         \"GPSU\": parse_time,\n         \"GPSF\": parse_fix,\n         \"GPSP\": parse_precision,\n         \"ACCL\": parse_accl,\n         \"GYRO\": parse_gyro,\n     }\n     d = {\"gps\": []}  \n     while True:\n         label = f.read(4)\n         if not label:  \n             break\n         desc = f.read(4)\n         if \"00\" == binascii.hexlify(desc[0]):\n             continue\n         val_size = struct.unpack(\">b\", desc[1])[0]\n         num_values = struct.unpack(\">h\", desc[2:4])[0]\n         length = val_size * num_values\n         if label == \"DVID\":\n             if len(d[\"gps\"]):  \n                 output.append(d)\n             d = {\"gps\": []}  \n         for i in range(num_values):\n             data = f.read(val_size)\n             if label in methods:\n                 methods[label](data, d, s)\n             if label == \"SCAL\":\n                 if 2 == val_size:\n                     s[i] = struct.unpack(\">h\", data)[0]\n                 elif 4 == val_size:\n                     s[i] = struct.unpack(\">i\", data)[0]\n                 else:\n                     raise Exception(\"unknown scal size\")\n         mod = length % 4\n         if mod != 0:\n             seek = 4 - mod\n             f.read(seek)  \n     return output", "query": "parse binary file to custom class"}
{"id": "https://github.com/UpCloudLtd/upcloud-python-api/blob/954b0ad7c4b932b2be31a95d88975f6b0eeac8ed/upcloud_api/cloud_manager/ip_address_mixin.py#L16-L23", "method_name": "get_ip", "code": "def get_ip(self, address):\n         res = self.get_request(\"/ip_address/\" + address)\n         return IPAddress(cloud_manager=self, **res[\"ip_address\"])", "query": "get current ip address"}
{"id": "https://github.com/LIVVkit/LIVVkit/blob/680120cd437e408673e62e535fc0a246c7fc17db/livvkit/components/validation.py#L141-L149", "method_name": "_print_summary", "code": "def _print_summary(module, case, summary):\n     try:\n         try:\n             module.print_summary(summary[case])\n         except TypeError:\n             module.print_summary(case, summary[case])\n     except (NotImplementedError, AttributeError):\n         print(\"    Ran \" + case + \"!\")\n         print(\"\")", "query": "print model summary"}
{"id": "https://github.com/dade-ai/snipy/blob/408520867179f99b3158b57520e2619f3fecd69b/snipy/img/imageutil.py#L479-L482", "method_name": "_convert_uint8", "code": "def _convert_uint8(im):\n     if im.dtype != np.uint8:\n         im = np.uint8(im * 255)\n     return im", "query": "converting uint8 array to image"}
{"id": "https://github.com/lappis-unb/salic-ml/blob/1b3ebc4f8067740999897ccffd9892dc94482a93/src/salicml/utils/read_csv.py#L10-L15", "method_name": "read_csv", "code": "def read_csv(csv_name, usecols=None):\n     csv_path = os.path.join(DATA_FOLDER, csv_name)\n     csv = pd.read_csv(csv_path, low_memory=False,\n                       usecols=usecols, encoding=\"utf-8\")\n     return csv", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/src/external/coremltools_wrap/coremltools/deps/protobuf/python/google/protobuf/descriptor.py#L321-L337", "method_name": "EnumValueName", "code": "def EnumValueName(self, enum, value):\n     return self.enum_types_by_name[enum].values_by_number[value].name", "query": "get name of enumerated value"}
{"id": "https://github.com/housecanary/hc-api-python/blob/2bb9e2208b34e8617575de45934357ee33b8531c/housecanary/excel/__init__.py#L18-L28", "method_name": "export_analytics_data_to_excel", "code": "def export_analytics_data_to_excel(data, output_file_name, result_info_key, identifier_keys):\n     workbook = create_excel_workbook(data, result_info_key, identifier_keys)\n     workbook.save(output_file_name)\n     print(\"Saved Excel file to {}\".format(output_file_name))", "query": "export to excel"}
{"id": "https://github.com/JIC-CSB/jicbioimage.transform/blob/494c282d964c3a9b54c2a1b3730f5625ea2a494b/appveyor/install.py#L19-L23", "method_name": "unzip_file", "code": "def unzip_file(zip_fname):\n     print(\"Unzipping {}\".format(zip_fname))\n     with zipfile.ZipFile(zip_fname) as zf:\n         zf.extractall()", "query": "unzipping large files"}
{"id": "https://github.com/uuazed/numerapi/blob/fc9dcc53b32ede95bfda1ceeb62aec1d67d26697/numerapi/numerapi.py#L76-L88", "method_name": "_unzip_file", "code": "def _unzip_file(self, src_path, dest_path, filename):\n         self.logger.info(\"unzipping file...\")\n         unzip_path = os.path.join(dest_path, filename)\n         utils.ensure_directory_exists(unzip_path)\n         with zipfile.ZipFile(src_path, \"r\") as z:\n             z.extractall(unzip_path)\n         return True", "query": "unzipping large files"}
{"id": "https://github.com/EconForge/dolo/blob/d91ddf148b009bf79852d9aec70f3a1877e0f79a/dolo/compiler/symbolic.py#L10-L17", "method_name": "std_tsymbol", "code": "def std_tsymbol(tsymbol):\n     s, date = tsymbol\n     if date == 0:\n         return \"_{}_\".format(s)\n     elif date <= 0:\n         return \"_{}_m{}_\".format(s, str(-date))\n     elif date >= 0:\n         return \"_{}__{}_\".format(s, str(date))", "query": "format date"}
{"id": "https://github.com/robgolding/tasklib/blob/0ad882377639865283021041f19add5aeb10126a/tasklib/serializing.py#L71-L79", "method_name": "timestamp_serializer", "code": "def timestamp_serializer(self, date):\n         if not date:\n             return \"\"\n         date = date.astimezone(pytz.utc)\n         return date.strftime(DATE_FORMAT)", "query": "string to date"}
{"id": "https://github.com/frawau/aioblescan/blob/02d12e90db3ee6df7be6513fec171f20dc533de3/aioblescan/plugins/eddystone.py#L105-L151", "method_name": "url_encoder", "code": "def url_encoder(self):\n         encodedurl = []\n         encodedurl.append(aios.IntByte(\"Tx Power\",self.power))\n         asisurl=\"\"\n         myurl = urlparse(self.type_payload)\n         myhostname = myurl.hostname\n         mypath = myurl.path\n         if (myurl.scheme,myhostname.startswith(\"www.\")) in url_schemes:\n             encodedurl.append(aios.IntByte(\"URL Scheme\",\n                                            url_schemes.index((myurl.scheme,myhostname.startswith(\"www.\")))))\n             if myhostname.startswith(\"www.\"):\n                 myhostname = myhostname[4:]\n         extval=None\n         if myhostname.split(\".\")[-1] in url_domain:\n             extval = url_domain.index(myhostname.split(\".\")[-1])\n             myhostname = \".\".join(myhostname.split(\".\")[:-1])\n         if extval is not None and not mypath.startswith(\"/\"):\n             extval+=7\n         else:\n             if myurl.port is None:\n                 if extval is not None:\n                     mypath = mypath[1:]\n             else:\n                 extval += 7\n         encodedurl.append(aios.String(\"URL string\"))\n         encodedurl[-1].val = myhostname\n         if extval is not None:\n             encodedurl.append(aios.IntByte(\"URL Extention\",extval))\n         if myurl.port:\n             asisurl += \":\"+str(myurl.port)+mypath\n         asisurl += mypath\n         if myurl.params:\n             asisurl += \";\"+myurl.params\n         if myurl.query:\n             asisurl += \"?\"+myurl.query\n         if myurl.fragment:\n             asisurl += \"\n         encodedurl.append(aios.String(\"Rest of URL\"))\n         encodedurl[-1].val = asisurl\n         tlength=0\n         for x in encodedurl: \n             tlength += len(x)\n         if tlength > 19: \n             raise Exception(\"Encoded url too long (max 18 bytes)\")\n         self.service_data_length.val += tlength \n         return encodedurl", "query": "encode url"}
{"id": "https://github.com/theonion/django-bulbs/blob/0c0e6e3127a7dc487b96677fab95cacd2b3806da/bulbs/utils/methods.py#L66-L68", "method_name": "datetime_to_epoch_seconds", "code": "def datetime_to_epoch_seconds(value):\n    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=timezone.utc)\n    return (value - epoch).total_seconds()", "query": "convert a utc time to epoch"}
{"id": "https://github.com/coderholic/pyradio/blob/c5219d350bccbccd49dbd627c1f886a952ea1963/pyradio/config.py#L258-L306", "method_name": "save_playlist_file", "code": "def save_playlist_file(self, stationFile=\"\"):\n         if self._playlist_format_changed():\n             self.dirty_playlist = True\n             self.new_format = not self.new_format\n         if stationFile:\n             st_file = stationFile\n         else:\n             st_file = self.stations_file\n         if not self.dirty_playlist:\n             if logger.isEnabledFor(logging.DEBUG):\n                 logger.debug(\"Playlist not modified...\")\n             return 0\n         st_new_file = st_file.replace(\".csv\", \".txt\")\n         tmp_stations = self.stations[:]\n         tmp_stations.reverse()\n         if self.new_format:\n             tmp_stations.append([ \"\n         else:\n             tmp_stations.append([ \"\n         tmp_stations.reverse()\n         try:\n             with open(st_new_file, \"w\") as cfgfile:\n                 writter = csv.writer(cfgfile)\n                 for a_station in tmp_stations:\n                     writter.writerow(self._format_playlist_row(a_station))\n         except:\n             if logger.isEnabledFor(logging.DEBUG):\n                 logger.debug(\"Cannot open playlist file for writing,,,\")\n             return -1\n         try:\n             move(st_new_file, st_file)\n         except:\n             if logger.isEnabledFor(logging.DEBUG):\n                 logger.debug(\"Cannot rename playlist file...\")\n             return -2\n         self.dirty_playlist = False\n         return 0", "query": "save list to file"}
{"id": "https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/widgets/image_cleaner.py#L209-L218", "method_name": "write_csv", "code": "def write_csv(self):\n         csv_path = self._path/\"cleaned.csv\"\n         with open(csv_path, \"w\") as f:\n             csv_writer = csv.writer(f)\n             csv_writer.writerow([\"name\",\"label\"])\n             for pair in self._csv_dict.items():\n                 pair = [os.path.relpath(pair[0], self._path), pair[1]]\n                 csv_writer.writerow(pair)\n         return csv_path", "query": "write csv"}
{"id": "https://github.com/mapillary/mapillary_tools/blob/816785e90c589cae6e8e34a5530ce8417d29591c/mapillary_tools/gpmf.py#L72-L133", "method_name": "parse_bin", "code": "def parse_bin(path):\n     f = open(path, \"rb\")\n     s = {}  \n     output = []\n     methods = {\n         \"GPS5\": parse_gps,\n         \"GPSU\": parse_time,\n         \"GPSF\": parse_fix,\n         \"GPSP\": parse_precision,\n         \"ACCL\": parse_accl,\n         \"GYRO\": parse_gyro,\n     }\n     d = {\"gps\": []}  \n     while True:\n         label = f.read(4)\n         if not label:  \n             break\n         desc = f.read(4)\n         if \"00\" == binascii.hexlify(desc[0]):\n             continue\n         val_size = struct.unpack(\">b\", desc[1])[0]\n         num_values = struct.unpack(\">h\", desc[2:4])[0]\n         length = val_size * num_values\n         if label == \"DVID\":\n             if len(d[\"gps\"]):  \n                 output.append(d)\n             d = {\"gps\": []}  \n         for i in range(num_values):\n             data = f.read(val_size)\n             if label in methods:\n                 methods[label](data, d, s)\n             if label == \"SCAL\":\n                 if 2 == val_size:\n                     s[i] = struct.unpack(\">h\", data)[0]\n                 elif 4 == val_size:\n                     s[i] = struct.unpack(\">i\", data)[0]\n                 else:\n                     raise Exception(\"unknown scal size\")\n         mod = length % 4\n         if mod != 0:\n             seek = 4 - mod\n             f.read(seek)  \n     return output", "query": "parse binary file to custom class"}
{"id": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341", "method_name": "check_checkbox", "code": "def check_checkbox(step, value):\n     with AssertContextManager(step):\n         check_box = find_field(world.browser, \"checkbox\", value)\n         if not check_box.is_selected():\n             check_box.click()", "query": "make the checkbox checked"}
{"id": "https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/operation/datashader.py#L1058-L1069", "method_name": "uint8_to_uint32", "code": "def uint8_to_uint32(self, element):\n         img = np.dstack([element.dimension_values(d, flat=False)\n                          for d in element.vdims])\n         if img.shape[2] == 3: \n             alpha = np.ones(img.shape[:2])\n             if img.dtype.name == \"uint8\":\n                 alpha = (alpha*255).astype(\"uint8\")\n             img = np.dstack([img, alpha])\n         if img.dtype.name != \"uint8\":\n             img = (img*255).astype(np.uint8)\n         N, M, _ = img.shape\n         return img.view(dtype=np.uint32).reshape((N, M))", "query": "converting uint8 array to image"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/utils/stringmatching.py#L18-L47", "method_name": "get_search_regex", "code": "def get_search_regex(query, ignore_case=True):\n     regex_text = [char for char in query if char != \" \"]\n     regex_text = \".*\".join(regex_text)\n     regex = r\"({0})\".format(regex_text)\n     if ignore_case:\n         pattern = re.compile(regex, re.IGNORECASE)\n     else:\n         pattern = re.compile(regex)\n     return pattern", "query": "regex case insensitive"}
{"id": "https://github.com/Adyen/adyen-python-api-library/blob/928f6409ab6e2fac300b9fa29d89f3f508b23445/Adyen/httpclient.py#L282-L316", "method_name": "request", "code": "def request(self, url,\n                 json=\"\",\n                 data=\"\",\n                 username=\"\",\n                 password=\"\",\n                 headers=None,\n                 timout=30):\n         raise NotImplementedError(\"request of HTTPClient should have been \"\n                                   \"overridden on initialization. \"\n                                   \"Otherwise, can be overridden to \"\n                                   \"supply your own post method\")", "query": "httpclient post json"}
{"id": "https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/utils.py#L1300-L1316", "method_name": "find_windows_executable", "code": "def find_windows_executable(bin_path, exe_name):\n     requested_path = get_windows_path(bin_path, exe_name)\n     if os.path.isfile(requested_path):\n         return requested_path\n     try:\n         pathext = os.environ[\"PATHEXT\"]\n     except KeyError:\n         pass\n     else:\n         for ext in pathext.split(os.pathsep):\n             path = get_windows_path(bin_path, exe_name + ext.strip().lower())\n             if os.path.isfile(path):\n                 return path\n     return find_executable(exe_name)", "query": "get executable path"}
{"id": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/engine/zeromq_queue.py#L161-L171", "method_name": "_SetSocketTimeouts", "code": "def _SetSocketTimeouts(self):\n     timeout = int(self.timeout_seconds * 1000)\n     receive_timeout = min(\n         self._ZMQ_SOCKET_RECEIVE_TIMEOUT_MILLISECONDS, timeout)\n     send_timeout = min(self._ZMQ_SOCKET_SEND_TIMEOUT_MILLISECONDS, timeout)\n     self._zmq_socket.setsockopt(zmq.RCVTIMEO, receive_timeout)\n     self._zmq_socket.setsockopt(zmq.SNDTIMEO, send_timeout)", "query": "socket recv timeout"}
{"id": "https://github.com/lepture/flask-oauthlib/blob/9e6f152a5bb360e7496210da21561c3e6d41b0e1/flask_oauthlib/provider/oauth1.py#L902-L905", "method_name": "_error_response", "code": "def _error_response(e):\n     res = make_response(e.urlencoded, e.status_code)\n     res.headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n     return res", "query": "custom http error response"}
{"id": "https://github.com/lappis-unb/salic-ml/blob/1b3ebc4f8067740999897ccffd9892dc94482a93/src/salicml/utils/read_csv.py#L10-L15", "method_name": "read_csv", "code": "def read_csv(csv_name, usecols=None):\n     csv_path = os.path.join(DATA_FOLDER, csv_name)\n     csv = pd.read_csv(csv_path, low_memory=False,\n                       usecols=usecols, encoding=\"utf-8\")\n     return csv", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/aliyun/aliyun-log-python-sdk/blob/ac383db0a16abf1e5ef7df36074374184b43516e/aliyun/log/index_config.py#L64-L72", "method_name": "from_json", "code": "def from_json(self, json_value) : \n         self.index_all = json_value.get(\"index_all\", True)\n         self.max_depth = json_value.get(\"max_depth\", -1)\n         self.alias = None\n         self.json_keys = {}\n         if \"alias\" in json_value:\n             self.alias = json_value[\"alias\"]\n         if \"json_keys\" in json_value:\n             self.json_keys = json_value[\"json_keys\"]", "query": "map to json"}
{"id": "https://github.com/davgeo/clear/blob/5ec85d27efd28afddfcd4c3f44df17f0115a77aa/clear/epguides.py#L219-L245", "method_name": "_ExtractDataFromShowHtml", "code": "def _ExtractDataFromShowHtml(self, html):\n     htmlLines = html.splitlines()\n     for count, line in enumerate(htmlLines):\n       if line.strip() == r\"<pre>\":\n         startLine = count+1\n       if line.strip() == r\"</pre>\":\n         endLine = count\n     try:\n       dataList = htmlLines[startLine:endLine]\n       dataString = \"\n\".join(dataList)\n       return dataString.strip()\n     except:\n       raise Exception(\"Show content not found - check EPGuides html formatting\")", "query": "extract data from html content"}
{"id": "https://github.com/flo-compbio/genometools/blob/dd962bb26d60a0f14ca14d8c9a4dd75768962c7d/genometools/expression/matrix.py#L295-L319", "method_name": "get_figure", "code": "def get_figure(self, heatmap_kw=None, **kwargs):\n         if heatmap_kw is not None:\n             assert isinstance(heatmap_kw, dict)\n         if heatmap_kw is None:\n             heatmap_kw = {}\n         return self.get_heatmap(**heatmap_kw).get_figure(**kwargs)", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/gzip_util.py#L43-L51", "method_name": "open_fileobj", "code": "def open_fileobj(fileobj, mode=\"rb\", compresslevel=9):\n     if hasattr(gzip.GzipFile, \"__enter__\"):\n         return gzip.GzipFile(\n             filename=\"\", mode=mode, fileobj=fileobj,\n             compresslevel=compresslevel\n         )\n     return GzipFile(\n         filename=\"\", mode=mode, fileobj=fileobj, compresslevel=compresslevel\n     )", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/eddiejessup/spatious/blob/b7ae91bec029e85a45a7f303ee184076433723cd/spatious/vector.py#L185-L216", "method_name": "sphere_pick_polar", "code": "def sphere_pick_polar(d, n=1, rng=None):\n     if rng is None:\n         rng = np.random\n     a = np.empty([n, d])\n     if d == 1:\n         a[:, 0] = rng.randint(2, size=n) * 2 - 1\n     elif d == 2:\n         a[:, 0] = 1.0\n         a[:, 1] = rng.uniform(-np.pi, +np.pi, n)\n     elif d == 3:\n         u, v = rng.uniform(0.0, 1.0, (2, n))\n         a[:, 0] = 1.0\n         a[:, 1] = np.arccos(2.0 * v - 1.0)\n         a[:, 2] = 2.0 * np.pi * u\n     else:\n         raise Exception(\"Invalid vector for polar representation\")\n     return a", "query": "randomly pick a number"}
{"id": "https://github.com/totalgood/pugnlp/blob/c43445b14afddfdeadc5f3076675c9e8fc1ee67c/src/pugnlp/stats.py#L536-L562", "method_name": "from_existing", "code": "def from_existing(cls, confusion, *args, **kwargs):\n         df = []\n         for t, p in product(confusion.index.values, confusion.columns.values):\n             df += [[t, p]] * confusion[p][t]\n         if confusion.index.name is not None and confusion.columns.name is not None:\n             return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name]))\n         return Confusion(pd.DataFrame(df))", "query": "confusion matrix"}
{"id": "https://github.com/securestate/termineter/blob/d657d25d97c7739e650b951c396404e857e56625/lib/termineter/core.py#L363-L374", "method_name": "serial_connect", "code": "def serial_connect(self):\n \t\tself.serial_get()\n \t\ttry:\n \t\t\tself.serial_connection.start()\n \t\texcept c1218.errors.C1218IOError as error:\n \t\t\tself.logger.error(\"serial connection has been opened but the meter is unresponsive\")\n \t\t\traise error\n \t\tself._serial_connected = True\n \t\treturn True", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/edublancas/sklearn-evaluation/blob/79ee6e4dfe911b5a5a9b78a5caaed7c73eef6f39/sklearn_evaluation/evaluator.py#L85-L89", "method_name": "confusion_matrix", "code": "def confusion_matrix(self):\n         return plot.confusion_matrix(self.y_true, self.y_pred,\n                                      self.target_names, ax=_gen_ax())", "query": "confusion matrix"}
{"id": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/gurumate-2.8.6-py2.7.egg/gurumate/linux2/procs.py#L43-L46", "method_name": "get_pid", "code": "def get_pid(PROCNAME):\n     for proc in psutil.process_iter():\n         if proc.name == PROCNAME:\n             return proc.pid", "query": "get current process id"}
{"id": "https://github.com/kpdyer/regex2dfa/blob/109f877e60ef0dfcb430f11516d215930b7b9936/third_party/re2/re2/unicode.py#L26-L45", "method_name": "_UInt", "code": "def _UInt(s):\n   try:\n     v = int(s, 16)\n   except ValueError:\n     v = -1\n   if len(s) < 4 or len(s) > 6 or v < 0 or v > _RUNE_MAX:\n     raise InputError(\"invalid Unicode value %s\" % (s,))\n   return v", "query": "convert int to string"}
{"id": "https://github.com/klen/zeta-library/blob/b76f89000f467e10ddcc94aded3f6c6bf4a0e5bd/zetalibrary/scss/__init__.py#L1898-L1910", "method_name": "to_str", "code": "def to_str(num):\n     if isinstance(num, dict):\n         s = sorted(num.items())\n         sp = num.get(\"_\", \"\")\n         return (sp + \" \").join(to_str(v) for n, v in s if n != \"_\")\n     elif isinstance(num, float):\n         num = (\"%0.03f\" % round(num, 3)).rstrip(\"0\").rstrip(\".\")\n         return num\n     elif isinstance(num, bool):\n         return \"true\" if num else \"false\"\n     elif num is None:\n         return \"\"\n     return str(num)", "query": "convert string to number"}
{"id": "https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/htmls.py#L106-L116", "method_name": "get_body", "code": "def get_body(doc):\n     [ elem.drop_tree() for elem in doc.xpath(\".//script   .//link   .//style\") ]\n     raw_html = tostring(doc.body or doc).decode(\"utf-8\")\n     print(raw_html)\n     cleaned = clean_attributes(raw_html)\n     try:\n         return cleaned\n     except Exception: \n---------\n%s\" % (raw_html, cleaned))\n         return raw_html", "query": "reading element from html - <td>"}
{"id": "https://github.com/klen/starter/blob/24a65c10d4ac5a9ca8fc1d8b3d54b3fb13603f5f/starter/core.py#L52-L58", "method_name": "copy_file", "code": "def copy_file(self, from_path, to_path):\n         if not op.exists(op.dirname(to_path)):\n             self.make_directory(op.dirname(to_path))\n         shutil.copy(from_path, to_path)\n         logging.debug(\"File copied: {0}\".format(to_path))", "query": "copying a file to a path"}
{"id": "https://github.com/flowersteam/explauto/blob/cf0f81ecb9f6412f7276a95bd27359000e1e26b6/explauto/experiment/log.py#L67-L93", "method_name": "scatter_plot", "code": "def scatter_plot(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot):\n         plot_specs = {\"marker\": \"o\", \"linestyle\": \"None\"}\n         plot_specs.update(kwargs_plot)\n         data = self.data_t(topic_dims, t)\n         ax.plot(*(data.T), **plot_specs)\n         if ms_limits:\n             ax.axis(self.axes_limits(topic_dims))", "query": "scatter plot"}
{"id": "https://github.com/airbus-cert/mispy/blob/6d523d6f134d2bd38ec8264be74e73b68403da65/mispy/misp.py#L595-L605", "method_name": "date", "code": "def date(self):\n         if self._date:\n             return self._date\n         return datetime.datetime.now().strftime(\"%Y-%m-%d\")", "query": "get current date"}
{"id": "https://github.com/pandas-profiling/pandas-profiling/blob/003d236daee8b7aca39c62708b18d59bced0bc03/pandas_profiling/__init__.py#L106-L122", "method_name": "to_file", "code": "def to_file(self, outputfile=DEFAULT_OUTPUTFILE):\n         if outputfile != NO_OUTPUTFILE:\n             if outputfile == DEFAULT_OUTPUTFILE:\n                 outputfile = \"profile_\" + str(hash(self)) + \".html\"\n             with codecs.open(outputfile, \"w+b\", encoding=\"utf8\") as self.file:\n                 self.file.write(templates.template(\"wrapper\").render(content=self.html))", "query": "output to html file"}
{"id": "https://github.com/msfrank/cifparser/blob/ecd899ba2e7b990e2cec62b115742d830e7e4384/cifparser/converters.py#L23-L29", "method_name": "str_to_bool", "code": "def str_to_bool(s):\n     s = s.lower()\n     if s in (\"true\", \"yes\", \"1\"):\n         return True\n     if s in (\"false\", \"no\", \"0\"):\n         return False\n     raise ConversionError(\"failed to convert {0} to bool\".format(s))", "query": "convert int to bool"}
{"id": "https://github.com/ethereum/pyethereum/blob/b704a5c6577863edc539a1ec3d2620a443b950fb/ethereum/tools/keys.py#L56-L61", "method_name": "aes_ctr_encrypt", "code": "def aes_ctr_encrypt(text, key, params):\n     iv = big_endian_to_int(decode_hex(params[\"iv\"]))\n     ctr = Counter.new(128, initial_value=iv, allow_wraparound=True)\n     mode = AES.MODE_CTR\n     encryptor = AES.new(key, mode, counter=ctr)\n     return encryptor.encrypt(text)", "query": "aes encryption"}
{"id": "https://github.com/thespacedoctor/sherlock/blob/2c80fb6fa31b04e7820e6928e3d437a21e692dd3/sherlock/imports/ned_d.py#L177-L274", "method_name": "_create_dictionary_of_ned_d", "code": "def _create_dictionary_of_ned_d(\n             self):\n         self.log.debug(\n             \"starting the ``_create_dictionary_of_ned_d`` method\")\n         count = 0\n         with open(self.pathToDataFile, \"rb\") as csvFile:\n             csvReader = csv.reader(\n                 csvFile, dialect=\"excel\", delimiter=\",\", quotechar=)\n             theseKeys = []\n             dictList = []\n             for row in csvReader:\n                 if len(theseKeys) == 0:\n                     totalRows -= 1\n                 if \"Exclusion Code\" in row and \"Hubble const.\" in row:\n                     for i in row:\n                         if i == \"redshift (z)\":\n                             theseKeys.append(\"redshift\")\n                         elif i == \"Hubble const.\":\n                             theseKeys.append(\"hubble_const\")\n                         elif i == \"G\":\n                             theseKeys.append(\"galaxy_index_id\")\n                         elif i == \"err\":\n                             theseKeys.append(\"dist_mod_err\")\n                         elif i == \"D (Mpc)\":\n                             theseKeys.append(\"dist_mpc\")\n                         elif i == \"Date (Yr. - 1980)\":\n                             theseKeys.append(\"ref_date\")\n                         elif i == \"REFCODE\":\n                             theseKeys.append(\"ref\")\n                         elif i == \"Exclusion Code\":\n                             theseKeys.append(\"dist_in_ned_flag\")\n                         elif i == \"Adopted LMC modulus\":\n                             theseKeys.append(\"lmc_mod\")\n                         elif i == \"m-M\":\n                             theseKeys.append(\"dist_mod\")\n                         elif i == \"Notes\":\n                             theseKeys.append(\"notes\")\n                         elif i == \"SN ID\":\n                             theseKeys.append(\"dist_derived_from_sn\")\n                         elif i == \"method\":\n                             theseKeys.append(\"dist_method\")\n                         elif i == \"Galaxy ID\":\n                             theseKeys.append(\"primary_ned_id\")\n                         elif i == \"D\":\n                             theseKeys.append(\"dist_index_id\")\n                         else:\n                             theseKeys.append(i)\n                     continue\n                 if len(theseKeys):\n                     count += 1\n                     if count > 1:\n                         sys.stdout.write(\"\\x1b[1A\\x1b[2K\")\n                     if count > totalCount:\n                         count = totalCount\n                     percent = (float(count) / float(totalCount)) * 100.\n                     print \"%(count)s / %(totalCount)s (%(percent)1.1f%%) rows added to memory\" % locals()\n                     rowDict = {}\n                     for t, r in zip(theseKeys, row):\n                         rowDict[t] = r\n                         if t == \"ref_date\":\n                             try:\n                                 rowDict[t] = int(r) + 1980\n                             except:\n                                 rowDict[t] = None\n                     if rowDict[\"dist_index_id\"] != \"999999\":\n                         dictList.append(rowDict)\n         csvFile.close()\n         self.log.debug(\n             \"completed the ``_create_dictionary_of_ned_d`` method\")\n         return dictList", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/nickw444/flask-ldap3-login/blob/3cf0faff52d0e04d4813119a2ba36d706e6fb31f/flask_ldap3_login/__init__.py#L705-L736", "method_name": "connection", "code": "def connection(self):\n         ctx = stack.top\n         if ctx is None:\n             raise Exception(\"Working outside of the Flask application \"\n                             \"context. If you wish to make a connection outside of a flask\"\n                             \" application context, please handle your connections \"\n                             \"and use manager.make_connection()\")\n         if hasattr(ctx, \"ldap3_manager_main_connection\"):\n             return ctx.ldap3_manager_main_connection\n         else:\n             connection = self._make_connection(\n                 bind_user=self.config.get(\"LDAP_BIND_USER_DN\"),\n                 bind_password=self.config.get(\"LDAP_BIND_USER_PASSWORD\"),\n                 contextualise=False\n             )\n             connection.bind()\n             if ctx is not None:\n                 ctx.ldap3_manager_main_connection = connection\n             return connection", "query": "postgresql connection"}
{"id": "https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/src/pyoram/crypto/aes.py#L24-L27", "method_name": "CTREnc", "code": "def CTREnc(key, plaintext):\n         iv = os.urandom(AES.block_size)\n         cipher = _cipher(_aes(key), _ctrmode(iv), backend=_backend).encryptor()\n         return iv + cipher.update(plaintext) + cipher.finalize()", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/client/archive.py#L93-L104", "method_name": "copy_dir", "code": "def copy_dir(self, path):\n         for directory in path:\n             if os.path.isdir(path):\n                 full_path = os.path.join(self.archive_dir, directory.lstrip(\"/\"))\n                 logger.debug(\"Copying %s to %s\", directory, full_path)\n                 shutil.copytree(directory, full_path)\n             else:\n                 logger.debug(\"Not a directory: %s\", directory)\n         return path", "query": "copying a file to a path"}
{"id": "https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/cookie.py#L369-L404", "method_name": "append_cookie", "code": "def append_cookie(self, cookie, name, payload, typ, domain=None, path=None,\n                       timestamp=\"\", max_age=0):\n         timestamp = str(int(time.time()))\n         try:\n             _payload = \"::\".join([payload, timestamp, typ])\n         except TypeError:\n             _payload = \"::\".join([payload[0], timestamp, typ])\n         content = make_cookie_content(name, _payload, self.sign_key, domain=domain,\n                                       path=path, timestamp=timestamp,\n                                       enc_key=self.enc_key, max_age=max_age,\n                                       sign_alg=self.sign_alg)\n         for name, args in content.items():\n             cookie[name] = args[\"value\"]\n             for key, value in args.items():\n                 if key == \"value\":\n                     continue\n                 cookie[name][key] = value\n         return cookie", "query": "create cookie"}
{"id": "https://github.com/QInfer/python-qinfer/blob/8170c84a0be1723f8c6b09e0d3c7a40a886f1fe3/src/qinfer/distributions.py#L1078-L1082", "method_name": "sample", "code": "def sample(self, n=1):\n         p_vals = self._p_dist.rvs(size=n)[:, np.newaxis]\n         return np.random.binomial(self.n, p_vals)", "query": "binomial distribution"}
{"id": "https://github.com/CEA-COSMIC/ModOpt/blob/019b189cb897cbb4d210c44a100daaa08468830c/modopt/math/stats.py#L75-L106", "method_name": "mad", "code": "def mad(data):\n     r\n     return np.median(np.abs(data - np.median(data)))", "query": "deducting the median from each column"}
{"id": "https://github.com/maxweisspoker/simplebitcoinfuncs/blob/ad332433dfcc067e86d2e77fa0c8f1a27daffb63/simplebitcoinfuncs/miscbitcoinfuncs.py#L190-L199", "method_name": "inttoDER", "code": "def inttoDER(a):\n     o = dechex(a,1)\n     if int(o[:2],16) > 127:\n         o = \"00\" + o\n     olen = dechex(len(o)//2,1)\n     return \"02\" + olen + o", "query": "convert decimal to hex"}
{"id": "https://github.com/HeliumEdu/heliumcli/blob/f6c6cc0ed3c335f956afa77eab0228ae23bb5d47/heliumcli/actions/init.py#L86-L97", "method_name": "_replace_in_file", "code": "def _replace_in_file(self, dir_name, filename, args):\n         path = os.path.join(dir_name, filename)\n         with open(path, \"r\") as f:\n             s = f.read()\n         s = s.replace(\"{%PROJECT_ID%}\", args.id)\n         s = s.replace(\"{%PROJECT_ID_UPPER%}\", self._upper_slug)\n         s = s.replace(\"{%PROJECT_ID_LOWER%}\", self._lower_slug)\n         s = s.replace(\"{%PROJECT_NAME%}\", args.name)\n         s = s.replace(\"{%PROJECT_GITHUB_USER%}\", args.github_user)\n         with open(path, \"w\") as f:\n             f.write(s)", "query": "replace in file"}
{"id": "https://github.com/aboSamoor/polyglot/blob/d0d2aa8d06cec4e03bd96618ae960030f7069a17/polyglot/decorators.py#L23-L32", "method_name": "memoize", "code": "def memoize(obj):\n   cache = obj.cache = {}\n   @functools.wraps(obj)\n   def memoizer(*args, **kwargs):\n     key = tuple(list(args) + sorted(kwargs.items()))\n     if key not in cache:\n       cache[key] = obj(*args, **kwargs)\n     return cache[key]\n   return memoizer", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/layers/common_layers.py#L3985-L4004", "method_name": "kl_divergence", "code": "def kl_divergence(mu, log_var, mu_p=0.0, log_var_p=0.0):\n   batch_size = shape_list(mu)[0]\n   prior_distribution = tfp.distributions.Normal(\n       mu_p, tf.exp(tf.multiply(0.5, log_var_p)))\n   posterior_distribution = tfp.distributions.Normal(\n       mu, tf.exp(tf.multiply(0.5, log_var)))\n   kld = tfp.distributions.kl_divergence(posterior_distribution,\n                                         prior_distribution)\n   return tf.reduce_sum(kld) / to_float(batch_size)", "query": "normal distribution"}
{"id": "https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/cli/html_content_extractor.py#L20-L32", "method_name": "run", "code": "def run(args):\n     html_content_extractor = HTMLContentExtractor()\n     with warnings.catch_warnings():\n         warnings.simplefilter(\"ignore\")\n         extractions = html_content_extractor.extract(html_text=args.input_file)\n         for e in extractions:\n             print(e.value)", "query": "extract data from html content"}
{"id": "https://github.com/remix/partridge/blob/0ba80fa30035e5e09fd8d7a7bdf1f28b93d53d03/partridge/gtfs.py#L89-L112", "method_name": "_read_csv", "code": "def _read_csv(self, filename: str) -> pd.DataFrame:\n         path = self._pathmap.get(filename)\n         columns = self._config.nodes.get(filename, {}).get(\"required_columns\", [])\n         if path is None or os.path.getsize(path) == 0:\n             return empty_df(columns)\n         with open(path, \"rb\") as f:\n             encoding = detect_encoding(f)\n         df = pd.read_csv(path, dtype=np.unicode, encoding=encoding, index_col=False)\n         df.rename(columns=lambda x: x.strip(), inplace=True)\n         if not df.empty:\n             for col in df.columns:\n                 df[col] = df[col].str.strip()\n         return df", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/model/multinomial.py#L18-L26", "method_name": "confusion_matrix", "code": "def confusion_matrix(self, data):\n         assert_is_type(data, H2OFrame)\n         j = h2o.api(\"POST /3/Predictions/models/%s/frames/%s\" % (self._id, data.frame_id))\n         return j[\"model_metrics\"][0][\"cm\"][\"table\"]", "query": "confusion matrix"}
{"id": "https://github.com/mclarkk/lifxlan/blob/ead0e3114d6aa2e5e77dab1191c13c16066c32b0/lifxlan/message.py#L126-L130", "method_name": "convert_MAC_to_int", "code": "def convert_MAC_to_int(addr):\n     reverse_bytes_str = addr.split(\":\")\n     reverse_bytes_str.reverse()\n     addr_str = \"\".join(reverse_bytes_str)\n     return int(addr_str, 16)", "query": "reverse a string"}
{"id": "https://github.com/O365/python-o365/blob/02a71cf3775cc6a3c042e003365d6a07c8c75a73/O365/utils/utils.py#L879-L907", "method_name": "_parse_filter_word", "code": "def _parse_filter_word(self, word):\n         if isinstance(word, str):\n             word = \"\"{}\"\".format(word)\n         elif isinstance(word, dt.date):\n             if isinstance(word, dt.datetime):\n                 if word.tzinfo is None:\n                     word = self.protocol.timezone.localize(\n                         word)  \n                 if word.tzinfo != pytz.utc:\n                     word = word.astimezone(\n                         pytz.utc)  \n             if \"/\" in self._attribute:\n                 word = \"\"{}\"\".format(\n                     word.isoformat())  \n             else:\n                 word = \"{}\".format(\n                     word.isoformat())  \n         elif isinstance(word, bool):\n             word = str(word).lower()\n         return word", "query": "determine a string is a valid word"}
{"id": "https://github.com/rkargon/pixelsorter/blob/0775d1e487fbcb023e411e1818ba3290b0e8665e/pixelsorter/util.py#L72-L84", "method_name": "weighted_random_choice", "code": "def weighted_random_choice(items):\n     l = list(items)\n     r = random.random() * sum([i[1] for i in l])\n     for x, p in l:\n         if p > r:\n             return x\n         r -= p\n     return None", "query": "randomly extract x items from a list"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/pdf.py#L161-L186", "method_name": "add_to_writer", "code": "def add_to_writer(self,\n                       writer: PdfFileWriter,\n                       start_recto: bool = True) -> None:\n         if self.is_html:\n             pdf = get_pdf_from_html(\n                 html=self.html,\n                 header_html=self.header_html,\n                 footer_html=self.footer_html,\n                 wkhtmltopdf_filename=self.wkhtmltopdf_filename,\n                 wkhtmltopdf_options=self.wkhtmltopdf_options)\n             append_memory_pdf_to_writer(pdf, writer, start_recto=start_recto)\n         elif self.is_filename:\n             if start_recto and writer.getNumPages() % 2 != 0:\n                 writer.addBlankPage()\n             writer.appendPagesFromReader(PdfFileReader(\n                 open(self.filename, \"rb\")))\n         else:\n             raise AssertionError(\"PdfPlan: shouldn\"t get here!\")", "query": "convert html to pdf"}
{"id": "https://github.com/pymupdf/PyMuPDF/blob/917f2d83482510e26ba0ff01fd2392c26f3a8e90/fitz/fitz.py#L270-L275", "method_name": "__mul__", "code": "def __mul__(self, m):\n         if hasattr(m, \"__float__\"):\n             return Matrix(self.a * m, self.b * m, self.c * m,\n                           self.d * m, self.e * m, self.f * m)\n         m1 = Matrix(1,1)\n         return m1.concat(self, m)", "query": "matrix multiply"}
{"id": "https://github.com/JoseAntFer/pyny3d/blob/fb81684935a24f7e50c975cb4383c81a63ab56df/pyny3d/utils.py#L8-L33", "method_name": "sort_numpy", "code": "def sort_numpy(array, col=0, order_back=False): \n     x = array[:,col] \n     sorted_index = np.argsort(x, kind = \"quicksort\") \n     sorted_array = array[sorted_index] \n     if not order_back: \n         return sorted_array \n     else: \n         n_points = sorted_index.shape[0] \n         order_back = np.empty(n_points, dtype=int) \n         order_back[sorted_index] = np.arange(n_points) \n         return [sorted_array, order_back]", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/DIPSAS/SwarmManagement/blob/c9ef1165b240c145d42e2d363925c8200fc19f43/SwarmManagement/SwarmTools.py#L170-L180", "method_name": "TimeoutCounter", "code": "def TimeoutCounter(secTimeout):\n     startTime = time.time()\n     elapsedTime = time.time() - startTime\n     timeLeft = secTimeout - int(elapsedTime)\n     printedTime = timeLeft\n     while elapsedTime < secTimeout:\n         timeLeft = secTimeout - int(elapsedTime)\n         if timeLeft < printedTime:\n             printedTime = timeLeft\n             print(\"Restarting Swarm in %d seconds\" % printedTime)\n         elapsedTime = time.time() - startTime", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/yunpian/yunpian-python-sdk/blob/405a1196ec83fdf29ff454f74ef036974be11970/yunpian_python_sdk/ypclient.py#L195-L208", "method_name": "urlEncodeAndJoin", "code": "def urlEncodeAndJoin(self, seq, sepr=\",\"):\n         try:\n             from urllib.parse import quote_plus as encode\n             return sepr.join([encode(x, encoding=CHARSET_UTF8) for x in seq])\n         except ImportError:\n             from urllib import quote as encode\n             return sepr.join([i for i in map(lambda x: encode(x), seq)])", "query": "encode url"}
{"id": "https://github.com/honzajavorek/tipi/blob/cbe51192725608b6fba1244a48610ae231b13e08/tipi/repl.py#L65-L74", "method_name": "replace", "code": "def replace(html, replacements=None):\n     if not replacements:\n         return html  \n     html = HTMLFragment(html)\n     for r in replacements:\n         r.replace(html)\n     return unicode(html)", "query": "html entities replace"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547", "method_name": "check_checkbox", "code": "def check_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     if not check_box.is_selected():\n         check_box.click()", "query": "make the checkbox checked"}
{"id": "https://github.com/jbm950/pygame_toolbox/blob/3fe32145fc149e4dd0963c30a2b6a4dddd4fac0e/pygame_toolbox/graphics/widgets.py#L141-L162", "method_name": "__call__", "code": "def __call__(self, *arg):\n         if self.status:\n             self.status = 0\n             self.image.fill((255, 255, 255))\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)\n         else:\n             self.status = 1\n             if self.checktype == \"r\":\n                 self.draw_rect_check()\n             elif self.checktype == \"c\":\n                 self.draw_circle_check()\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)", "query": "check if a checkbox is checked"}
{"id": "https://github.com/pikepdf/pikepdf/blob/07154f4dec007e2e9c0c6a8c07b964fd06bc5f77/src/pikepdf/_methods.py#L76-L83", "method_name": "_single_page_pdf", "code": "def _single_page_pdf(page):\n     pdf = Pdf.new()\n     pdf.pages.append(page)\n     bio = BytesIO()\n     pdf.save(bio)\n     bio.seek(0)\n     return bio.read()", "query": "convert html to pdf"}
{"id": "https://github.com/sunlightlabs/django-mediasync/blob/aa8ce4cfff757bbdb488463c64c0863cca6a1932/mediasync/__init__.py#L33-L38", "method_name": "compress", "code": "def compress(s):\n     zbuf = cStringIO.StringIO()\n     zfile = gzip.GzipFile(mode=\"wb\", compresslevel=6, fileobj=zbuf)\n     zfile.write(s)\n     zfile.close()\n     return zbuf.getvalue()", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/IAMconsortium/pyam/blob/4077929ca6e7be63a0e3ecf882c5f1da97b287bf/pyam/core.py#L1049-L1070", "method_name": "to_excel", "code": "def to_excel(self, excel_writer, sheet_name=\"data\",\n                  iamc_index=False, **kwargs):\n         if not isinstance(excel_writer, pd.ExcelWriter):\n             close = True\n             excel_writer = pd.ExcelWriter(excel_writer)\n         self._to_file_format(iamc_index)\\\n             .to_excel(excel_writer, sheet_name=sheet_name, index=False,\n                       **kwargs)\n         if close:\n             excel_writer.close()", "query": "export to excel"}
{"id": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341", "method_name": "check_checkbox", "code": "def check_checkbox(step, value):\n     with AssertContextManager(step):\n         check_box = find_field(world.browser, \"checkbox\", value)\n         if not check_box.is_selected():\n             check_box.click()", "query": "make the checkbox checked"}
{"id": "https://github.com/bihealth/vcfpy/blob/99e2165df30f11e0c95f3170f31bc5191d9e9e15/vcfpy/header.py#L227-L253", "method_name": "header_without_lines", "code": "def header_without_lines(header, remove):\n     remove = set(remove)\n     lines = []\n     for line in header.lines:\n         if hasattr(line, \"mapping\"):\n             if (line.key, line.mapping.get(\"ID\", None)) in remove:\n                 continue  \n         else:\n             if (line.key, line.value) in remove:\n                 continue  \n         lines.append(line)\n     return Header(lines, header.samples)", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/heatmaps.py#L110-L158", "method_name": "draw", "code": "def draw(self, size=None, cmap=\"jet\"):\n         heatmaps_uint8 = self.to_uint8()\n         heatmaps_drawn = []\n         for c in sm.xrange(heatmaps_uint8.shape[2]):\n             heatmap_c = heatmaps_uint8[..., c:c+1]\n             if size is not None:\n                 heatmap_c_rs = ia.imresize_single_image(heatmap_c, size, interpolation=\"nearest\")\n             else:\n                 heatmap_c_rs = heatmap_c\n             heatmap_c_rs = np.squeeze(heatmap_c_rs).astype(np.float32) / 255.0\n             if cmap is not None:\n                 import matplotlib.pyplot as plt\n                 cmap_func = plt.get_cmap(cmap)\n                 heatmap_cmapped = cmap_func(heatmap_c_rs)\n                 heatmap_cmapped = np.delete(heatmap_cmapped, 3, 2)\n             else:\n                 heatmap_cmapped = np.tile(heatmap_c_rs[..., np.newaxis], (1, 1, 3))\n             heatmap_cmapped = np.clip(heatmap_cmapped * 255, 0, 255).astype(np.uint8)\n             heatmaps_drawn.append(heatmap_cmapped)\n         return heatmaps_drawn", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/markbaas/python-iresolve/blob/ba91e37221e91265e4ac5dbc6e8f5cffa955a04f/iresolve.py#L136-L141", "method_name": "output", "code": "def output(results, output_format=\"pretty\"):\n     if output_format == \"pretty\":\n         for u, meta in results.items():\n             print(\"* {} can be imported from: {}\".format(u, \", \".join(meta[\"paths\"])))\n     elif output_format == \"json\":\n         print(json.dumps(results))", "query": "pretty print json"}
{"id": "https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/internet/web.py#L171-L180", "method_name": "get_html_source", "code": "def get_html_source(self):\n         req = urllib.request.Request(self.url)\n         req.add_header(\"user-agent\", random.choice(USER_AGENTS))\n         req_text = urllib.request.urlopen(req).read()\n         self.source = str(req_text)\n         self.soup = BeautifulSoup(self.source, \"html.parser\")\n         return self.source", "query": "get html of website"}
{"id": "https://github.com/janpipek/physt/blob/6dd441b073514e7728235f50b2352d56aacf38d4/physt/examples/__init__.py#L12-L22", "method_name": "normal_h1", "code": "def normal_h1(size: int = 10000, mean: float = 0, sigma: float = 1) -> Histogram1D:\n     data = np.random.normal(mean, sigma, (size,))\n     return h1(data, name=\"normal\", axis_name=\"x\", title=\"1D normal distribution\")", "query": "normal distribution"}
{"id": "https://github.com/pgjones/quart/blob/7cb2d3bd98e8746025764f2b933abc12041fa175/quart/utils.py#L29-L59", "method_name": "create_cookie", "code": "def create_cookie(\n         key: str,\n         value: str=\"\",\n         max_age: Optional[Union[int, timedelta]]=None,\n         expires: Optional[Union[int, float, datetime]]=None,\n         path: str=\"/\",\n         domain: Optional[str]=None,\n         secure: bool=False,\n         httponly: bool=False,\n ) -> SimpleCookie:\n     cookie = SimpleCookie()\n     cookie[key] = value\n     cookie[key][\"path\"] = path\n     cookie[key][\"httponly\"] = httponly  \n     cookie[key][\"secure\"] = secure  \n     if isinstance(max_age, timedelta):\n         cookie[key][\"max-age\"] = f\"{max_age.total_seconds():d}\"\n     if isinstance(max_age, int):\n         cookie[key][\"max-age\"] = str(max_age)\n     if expires is not None and isinstance(expires, (int, float)):\n         cookie[key][\"expires\"] = format_date_time(int(expires))\n     elif expires is not None and isinstance(expires, datetime):\n         cookie[key][\"expires\"] = format_date_time(expires.replace(tzinfo=timezone.utc).timestamp())\n     if domain is not None:\n         cookie[key][\"domain\"] = domain\n     return cookie", "query": "create cookie"}
{"id": "https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/plugin_html.py#L177-L183", "method_name": "tag_to_dict", "code": "def tag_to_dict(html):\n     element = document_fromstring(html).xpath(\"//html/body/child::*\")[0]\n     attributes = dict(element.attrib)\n     attributes[\"text\"] = element.text_content()\n     return attributes", "query": "reading element from html - <td>"}
{"id": "https://github.com/totalgood/nlpia/blob/efa01126275e9cd3c3a5151a644f1c798a9ec53f/src/nlpia/book/examples/ch05_sms_spam_linear_regression.py#L161-L173", "method_name": "sentiment_scatter", "code": "def sentiment_scatter(sms=sms):\n     plt.figure(figsize=(10, 7.5))\n     ax = plt.subplot(1, 1, 1)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"line\", ax=ax, color=\"g\", marker=\"+\", alpha=.6)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"sgd\", ax=ax, color=\"r\", marker=\"x\", alpha=.4)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"vader\", ax=ax, color=\"k\", marker=\".\", alpha=.3)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"sgd\", ax=ax, color=\"c\", marker=\"s\", alpha=.6)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"pca_lda_spaminess\", ax=ax, color=\"b\", marker=\"o\", alpha=.6)\n     plt.ylabel(\"Sentiment\")\n     plt.xlabel(\"Topic 4\")\n     plt.legend([\"LinearRegressor\", \"SGDRegressor\", \"Vader\", \"OneNeuronRegresor\", \"PCA->LDA->spaminess\"])\n     plt.tight_layout()\n     plt.show()", "query": "scatter plot"}
{"id": "https://github.com/lotrekagency/djlotrek/blob/10a304103768c3d59bdb8859eba86ef3327c9598/djlotrek/templatetags/djlotrek_filters.py#L50-L53", "method_name": "regex_match", "code": "def regex_match(value, regex):\n     pattern = re.compile(regex)\n     if pattern.match(value):\n         return True", "query": "regex case insensitive"}
{"id": "https://github.com/fred49/linshare-api/blob/be646c25aa8ba3718abb6869c620b157d53d6e41/linshareapi/admin/upgradetasks.py#L177-L191", "method_name": "trigger", "code": "def trigger(self, identifier, force=True):\n         self.debug(identifier)\n         url = \"{base}/{identifier}\".format(\n             base=self.local_base_url,\n             identifier=identifier\n         )\n         param = {}\n         if force:\n             param[\"force\"] = force\n         encode = urllib.urlencode(param)\n         if encode:\n             url += \"?\"\n             url += encode\n         return self.core.update(url, {})", "query": "encode url"}
{"id": "https://github.com/maxpowel/scrapium/blob/bc12c425aa5978f953a87d05920ba0f61a00409c/scrapium/scrapium.py#L140-L142", "method_name": "get_html", "code": "def get_html(self, url):\n        r = self.get(url)\n        return self.html(r.text)", "query": "get inner html"}
{"id": "https://github.com/thomasdelaet/python-velbus/blob/af2f8af43f1a24bf854eff9f3126fd7b5c41b3dd/velbus/messages/relay_status.py#L120-L130", "method_name": "data_to_binary", "code": "def data_to_binary(self):\n         return bytes([\n             COMMAND_CODE,\n             self.channels_to_byte([self.channel]),\n             self.disable_inhibit_forced,\n             self.status,\n             self.led_status\n         ]) + struct.pack(\">L\", self.delay_time)[-3:]", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/utiasSTARS/pykitti/blob/d3e1bb81676e831886726cc5ed79ce1f049aef2c/pykitti/downloader/tracking.py#L26-L38", "method_name": "clean_file", "code": "def clean_file(filename):\n     f = open(filename, \"r\")\n     new_lines = []\n     for line in f.readlines():\n         new_lines.append(line.rstrip())\n     f.close()\n     f = open(filename, \"w\")\n     for line in new_lines:\n         f.write(line + \"\n\")\n     f.close()", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/kevinsprong23/aperture/blob/d0420fef3b25d8afc0e5ddcfb6fe5f0ff42b9799/aperture/heatmaps.py#L123-L144", "method_name": "heatmap", "code": "def heatmap(x, y, step=None, min_pt=None, max_pt=None,\n                  colormap=\"Blues\", alpha=1, grid=False,\n                  colorbar=True, scale=\"lin\",\n                  vmax=\"auto\", vmin=\"auto\", crop=True):\n     (x_vec, y_vec, hist_matrix) = calc_2d_hist(x, y, step, min_pt, max_pt)\n     if scale == \"log\":\n         for row in hist_matrix:\n             for i, el in enumerate(row):\n                 row[i] = 0 if row[i] == 0 else log10(row[i])\n     fig = plt.figure()\n     init_heatmap(x_vec, y_vec, hist_matrix, fig, colormap=colormap,\n                  alpha=alpha, grid=grid, colorbar=colorbar,\n                  vmax=vmax, vmin=vmin, crop=crop)\n     return fig", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/python-xlib/python-xlib/blob/8901e831737e79fe5645f48089d70e1d1046d2f2/Xlib/protocol/rq.py#L1135-L1210", "method_name": "parse_binary", "code": "def parse_binary(self, data, display, rawdict = 0):\n         ret = {}\n         val = struct.unpack(self.static_codes, data[:self.static_size])\n         lengths = {}\n         formats = {}\n         vno = 0\n         for f in self.static_fields:\n             if not f.name:\n                 pass\n             elif isinstance(f, LengthField):\n                 f_names = [f.name]\n                 if f.other_fields:\n                     f_names.extend(f.other_fields)\n                 field_val = val[vno]\n                 if f.parse_value is not None:\n                     field_val = f.parse_value(field_val, display)\n                 for f_name in f_names:\n                     lengths[f_name] = field_val\n             elif isinstance(f, FormatField):\n                 formats[f.name] = val[vno]\n             else:\n                 if f.structvalues == 1:\n                     field_val = val[vno]\n                 else:\n                     field_val = val[vno:vno+f.structvalues]\n                 if f.parse_value is not None:\n                     field_val = f.parse_value(field_val, display)\n                 ret[f.name] = field_val\n             vno = vno + f.structvalues\n         data = data[self.static_size:]\n         for f in self.var_fields:\n             ret[f.name], data = f.parse_binary_value(data, display,\n                                                      lengths.get(f.name),\n                                                      formats.get(f.name),\n                                                     )\n         if not rawdict:\n             ret = DictWrapper(ret)\n         return ret, data", "query": "parse binary file to custom class"}
{"id": "https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660", "method_name": "checkbox_check", "code": "def checkbox_check(self, force_check=False):\n         if not self.get_attribute(\"checked\"):\n             self.click(force_click=force_check)", "query": "check if a checkbox is checked"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/collectionseditor.py#L1242-L1252", "method_name": "paste", "code": "def paste(self): \n         clipboard = QApplication.clipboard() \n         cliptext = \"\" \n         if clipboard.mimeData().hasText(): \n             cliptext = to_text_string(clipboard.text()) \n         if cliptext.strip(): \n             self.import_from_string(cliptext, title=_(\"Import from clipboard\")) \n         else: \n             QMessageBox.warning(self, _( \"Empty clipboard\"), \n                                 _(\"Nothing to be imported from clipboard.\"))", "query": "copy to clipboard"}
{"id": "https://github.com/fvalverd/AutoApi-client-Python/blob/a6b04947f80bf988c1515d622c78c587b341b3f4/auto_api_client/__init__.py#L91-L94", "method_name": "post", "code": "def post(self, json=None):\n         response = self._http(requests.post, json=json)\n         if response.status_code == 201:\n             return response.json()", "query": "httpclient post json"}
{"id": "https://github.com/sentinel-hub/eo-learn/blob/b8c390b9f553c561612fe9eb64e720611633a035/ml_tools/eolearn/ml_tools/validator.py#L201-L208", "method_name": "confusion_matrix", "code": "def confusion_matrix(self):\n         confusion_matrix = self.pixel_classification_sum.astype(np.float)\n         confusion_matrix = np.divide(confusion_matrix.T, self.pixel_truth_sum.T).T\n         return confusion_matrix * 100.0", "query": "confusion matrix"}
{"id": "https://github.com/sosreport/sos/blob/2ebc04da53dc871c8dd5243567afa4f8592dca29/sos/plugins/kernel.py#L41-L51", "method_name": "get_bpftool_map_ids", "code": "def get_bpftool_map_ids(self, map_file):\n         out = []\n         try:\n             map_data = json.load(open(map_file))\n         except Exception as e:\n             self._log_info(\"Could not parse bpftool map list as JSON: %s\" % e)\n             return out\n         for item in range(len(map_data)):\n             if \"id\" in map_data[item]:\n                 out.append(map_data[item][\"id\"])\n         return out", "query": "map to json"}
{"id": "https://github.com/ask/ghettoq/blob/22a0fcd865b618cbbbfd102efd88a7983507c24e/ghettoq/backends/beanstalk.py#L32-L34", "method_name": "put", "code": "def put(self, queue, message, priority=0, **kwargs):\n        self.client.use(queue)\n        self.client.put(message, priority=priority)", "query": "priority queue"}
{"id": "https://github.com/FaradayRF/faradayio/blob/6cf3af88bb4a83e5d2036e5cbdfaf8f0f01500bb/faradayio/faraday.py#L32-L56", "method_name": "send", "code": "def send(self, msg):\n         slipDriver = sliplib.Driver()\n         slipData = slipDriver.send(msg)\n         res = self._serialPort.write(slipData)\n         return res", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/__init__.py#L232-L258", "method_name": "levenshtein", "code": "def levenshtein(s1, s2, allow_substring=False):\n     len1, len2 = len(s1), len(s2)\n     lev = []\n     for i in range(len1 + 1):\n         lev.append([0] * (len2 + 1))\n     for i in range(len1 + 1):\n         lev[i][0] = i\n     for j in range(len2 + 1):\n         lev[0][j] = 0 if allow_substring else j\n     for i in range(len1):\n         for j in range(len2):\n             lev[i + 1][j + 1] = min(lev[i][j + 1] + 1, lev[i + 1][j] + 1, lev[i][j] + (s1[i] != s2[j]))\n     return min(lev[len1]) if allow_substring else lev[len1][len2]", "query": "string similarity levenshtein"}
{"id": "https://github.com/nyaruka/smartmin/blob/488a676a4960555e4d216a7b95d6e01a4ad4efd8/smartmin/views.py#L914-L924", "method_name": "derive_readonly", "code": "def derive_readonly(self):\n         readonly = list(self.readonly)\n         for key, value in self.field_config.items():\n             if \"readonly\" in value and value[\"readonly\"]:\n                 readonly.append(key)\n         return readonly", "query": "readonly array"}
{"id": "https://github.com/Turbo87/utm/blob/efdd46ab0a341ce2aa45f8144d8b05a4fa0fd592/utm/conversion.py#L171-L246", "method_name": "from_latlon", "code": "def from_latlon(latitude, longitude, force_zone_number=None, force_zone_letter=None):\n     if not in_bounds(latitude, -80.0, 84.0):\n         raise OutOfRangeError(\"latitude out of range (must be between 80 deg S and 84 deg N)\")\n     if not in_bounds(longitude, -180.0, 180.0):\n         raise OutOfRangeError(\"longitude out of range (must be between 180 deg W and 180 deg E)\")\n     if force_zone_number is not None:\n         check_valid_zone(force_zone_number, force_zone_letter)\n     lat_rad = mathlib.radians(latitude)\n     lat_sin = mathlib.sin(lat_rad)\n     lat_cos = mathlib.cos(lat_rad)\n     lat_tan = lat_sin / lat_cos\n     lat_tan2 = lat_tan * lat_tan\n     lat_tan4 = lat_tan2 * lat_tan2\n     if force_zone_number is None:\n         zone_number = latlon_to_zone_number(latitude, longitude)\n     else:\n         zone_number = force_zone_number\n     if force_zone_letter is None:\n         zone_letter = latitude_to_zone_letter(latitude)\n     else:\n         zone_letter = force_zone_letter\n     lon_rad = mathlib.radians(longitude)\n     central_lon = zone_number_to_central_longitude(zone_number)\n     central_lon_rad = mathlib.radians(central_lon)\n     n = R / mathlib.sqrt(1 - E * lat_sin**2)\n     c = E_P2 * lat_cos**2\n     a = lat_cos * (lon_rad - central_lon_rad)\n     a2 = a * a\n     a3 = a2 * a\n     a4 = a3 * a\n     a5 = a4 * a\n     a6 = a5 * a\n     m = R * (M1 * lat_rad -\n              M2 * mathlib.sin(2 * lat_rad) +\n              M3 * mathlib.sin(4 * lat_rad) -\n              M4 * mathlib.sin(6 * lat_rad))\n     easting = K0 * n * (a +\n                         a3 / 6 * (1 - lat_tan2 + c) +\n                         a5 / 120 * (5 - 18 * lat_tan2 + lat_tan4 + 72 * c - 58 * E_P2)) + 500000\n     northing = K0 * (m + n * lat_tan * (a2 / 2 +\n                                         a4 / 24 * (5 - lat_tan2 + 9 * c + 4 * c**2) +\n                                         a6 / 720 * (61 - 58 * lat_tan2 + lat_tan4 + 600 * c - 330 * E_P2)))\n     if mixed_signs(latitude):\n         raise ValueError(\"latitudes must all have the same sign\")\n     elif negative(latitude):\n         northing += 10000000\n     return easting, northing, zone_number, zone_letter", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/hardware.py#L106-L119", "method_name": "_readline", "code": "def _readline(self, ignore_comments=True):\n         while True:\n             line = self._det_file.readline()\n             if line == \"\":\n                 return line    \n             line = line.strip()\n             if line == \"\":\n                 continue    \n             if line.startswith(\"\n                 if not ignore_comments:\n                     return line\n             else:\n                 return line", "query": "read text file line by line"}
{"id": "https://github.com/deeshugupta/tes/blob/217db49aa211ebca2d9258380765a0c31abfca91/es_commands/base.py#L38-L40", "method_name": "pretty_print", "code": "def pretty_print(response):\n    parsed = json.loads(json.dumps(response))\n    click.echo(json.dumps(parsed, indent=4, sort_keys=True))", "query": "pretty print json"}
{"id": "https://github.com/openstates/billy/blob/5fc795347f12a949e410a8cfad0c911ea6bced67/billy/models/base.py#L414-L416", "method_name": "distinct", "code": "def distinct(self, *args, **kwargs):\n        return CursorWrapper(\n            self.cursor.distinct(*args, **kwargs), self.instance)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/loverajoel/sqlalchemy-elasticquery/blob/4c99b81f59e7bb20eaeedb3adbf5126e62bbc25c/sqlalchemy_elasticquery/elastic_query.py#L159-L167", "method_name": "sort", "code": "def sort(self, sort_list):\n         order = []\n         for sort in sort_list:\n             if sort_list[sort] == \"asc\":\n                 order.append(asc(getattr(self.model, sort, None)))\n             elif sort_list[sort] == \"desc\":\n                 order.append(desc(getattr(self.model, sort, None)))\n         return order", "query": "sort string list"}
{"id": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_decor.py#L601-L651", "method_name": "memoize", "code": "def memoize(func):\n     cache = func._util_decor_memoize_cache = {}\n     def memoizer(*args, **kwargs):\n         key = str(args) + str(kwargs)\n         if key not in cache:\n             cache[key] = func(*args, **kwargs)\n         return cache[key]\n     memoizer = preserve_sig(memoizer, func)\n     memoizer.cache = cache\n     return memoizer", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_formatting.py#L29-L32", "method_name": "format_dateformat", "code": "def format_dateformat(dates):\n     format = dateformat.DateFormat(\"YYYY-MM-DD hh:mm:ss\")\n     for date in dates:\n         assert isinstance(format.format(date), str)", "query": "format date"}
{"id": "https://github.com/StorjOld/heartbeat/blob/4d54f2011f1e9f688073d4347bc51bb7bd682718/heartbeat/PySwizzle/PySwizzle.py#L162-L178", "method_name": "encrypt", "code": "def encrypt(self, key):\n         if (self.encrypted):\n             return\n         self.iv = Random.new().read(AES.block_size)\n         aes = AES.new(key, AES.MODE_CFB, self.iv)\n         self.f_key = aes.encrypt(self.f_key)\n         self.alpha_key = aes.encrypt(self.alpha_key)\n         self.encrypted = True\n         self.hmac = self.get_hmac(key)", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/db/__init__.py#L56-L62", "method_name": "execute", "code": "def execute(self, sql, args=()):\n         if isinstance(sql, (list, tuple)):\n             sql = \" \".join(sql)\n         with sqlite3.connect(self.path) as con:\n             return con.execute(sql, args)", "query": "connect to sql"}
{"id": "https://github.com/jbm950/pygame_toolbox/blob/3fe32145fc149e4dd0963c30a2b6a4dddd4fac0e/pygame_toolbox/graphics/widgets.py#L141-L162", "method_name": "__call__", "code": "def __call__(self, *arg):\n         if self.status:\n             self.status = 0\n             self.image.fill((255, 255, 255))\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)\n         else:\n             self.status = 1\n             if self.checktype == \"r\":\n                 self.draw_rect_check()\n             elif self.checktype == \"c\":\n                 self.draw_circle_check()\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)", "query": "make the checkbox checked"}
{"id": "https://github.com/google/apitools/blob/f3745a7ea535aa0e88b0650c16479b696d6fd446/apitools/base/protorpclite/messages.py#L1786-L1809", "method_name": "validate_default_element", "code": "def validate_default_element(self, value):\n         if isinstance(value, (six.string_types, six.integer_types)):\n             if self.__type:\n                 self.__type(value)\n             return value\n         return super(EnumField, self).validate_default_element(value)", "query": "get name of enumerated value"}
{"id": "https://github.com/zeekay/soundcloud-cli/blob/8a83013683e1acf32f093239bbb6d3c02bc50b37/soundcloud_cli/utils.py#L7-L23", "method_name": "copy_to_clipboard", "code": "def copy_to_clipboard(text):\n     if sys.platform == \"darwin\":\n         os.system(\"echo \"{0}\"   pbcopy\".format(text))\n         return\n     try:\n         from Tkinter import Tk\n     except ImportError:\n         return\n     r = Tk()\n     r.withdraw()\n     r.clipboard_clear()\n     r.clipboard_append(text.encode(\"ascii\"))\n     r.destroy()", "query": "copy to clipboard"}
{"id": "https://github.com/openvax/pyensembl/blob/4b995fb72e848206d6fbf11950cf30964cd9b3aa/pyensembl/memory_cache.py#L64-L72", "method_name": "_read_csv", "code": "def _read_csv(self, csv_path):\n         logger.info(\"Reading Dataframe from %s\", csv_path)\n         df = pd.read_csv(csv_path)\n         if \"seqname\" in df:\n             df[\"seqname\"] = df[\"seqname\"].map(str)\n         return df", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/ewels/MultiQC/blob/2037d6322b2554146a74efbf869156ad20d4c4ec/multiqc/modules/bcl2fastq/bcl2fastq.py#L142-L268", "method_name": "parse_file_as_json", "code": "def parse_file_as_json(self, myfile):\n         try:\n             content = json.loads(myfile[\"f\"])\n         except ValueError:\n             log.warn(\"Could not parse file as json: {}\".format(myfile[\"fn\"]))\n             return\n         runId = content[\"RunId\"]\n         if runId not in self.bcl2fastq_data:\n             self.bcl2fastq_data[runId] = dict()\n         run_data = self.bcl2fastq_data[runId]\n         for conversionResult in content.get(\"ConversionResults\", []):\n             l = conversionResult[\"LaneNumber\"]\n             lane = \"L{}\".format(conversionResult[\"LaneNumber\"])\n             if lane in run_data:\n                 log.debug(\"Duplicate runId/lane combination found! Overwriting: {}\".format(self.prepend_runid(runId, lane)))\n             run_data[lane] = {\n                 \"total\": 0,\n                 \"total_yield\": 0,\n                 \"perfectIndex\": 0,\n                 \"samples\": dict(),\n                 \"yieldQ30\": 0,\n                 \"qscore_sum\": 0\n             }\n             rlane = run_data[lane]\n             try:\n                 unknown_barcode = content[\"UnknownBarcodes\"][l - 1][\"Barcodes\"]\n             except IndexError:\n                 unknown_barcode = next(\n                     (item[\"Barcodes\"] for item in content[\"UnknownBarcodes\"] if item[\"Lane\"] == 8),\n                     None\n                 )\n             run_data[lane][\"unknown_barcodes\"] = unknown_barcode\n             for demuxResult in conversionResult.get(\"DemuxResults\", []):\n                 if demuxResult[\"SampleName\"] == demuxResult[\"SampleName\"]:\n                     sample = demuxResult[\"SampleName\"]\n                 else:\n                     sample = \"{}-{}\".format(demuxResult[\"SampleId\"], demuxResult[\"SampleName\"])\n                 if sample in run_data[lane][\"samples\"]:\n                     log.debug(\"Duplicate runId/lane/sample combination found! Overwriting: {}, {}\".format(self.prepend_runid(runId, lane), sample))\n                 run_data[lane][\"samples\"][sample] = {\n                     \"total\": 0,\n                     \"total_yield\": 0,\n                     \"perfectIndex\": 0,\n                     \"filename\": os.path.join(myfile[\"root\"], myfile[\"fn\"]),\n                     \"yieldQ30\": 0,\n                     \"qscore_sum\": 0\n                 }\n                 lsample = run_data[lane][\"samples\"][sample]\n                 for r in range(1,5):\n                         lsample[\"R{}_yield\".format(r)] = 0\n                         lsample[\"R{}_Q30\".format(r)] = 0\n                         lsample[\"R{}_trimmed_bases\".format(r)] = 0\n                 rlane[\"total\"] += demuxResult[\"NumberReads\"]\n                 rlane[\"total_yield\"] += demuxResult[\"Yield\"]\n                 lsample[\"total\"] += demuxResult[\"NumberReads\"]\n                 lsample[\"total_yield\"] += demuxResult[\"Yield\"]\n                 for indexMetric in demuxResult.get(\"IndexMetrics\", []):\n                     rlane[\"perfectIndex\"] += indexMetric[\"MismatchCounts\"][\"0\"]\n                     lsample[\"perfectIndex\"] += indexMetric[\"MismatchCounts\"][\"0\"]\n                 for readMetric in demuxResult.get(\"ReadMetrics\", []):\n                     r = readMetric[\"ReadNumber\"]\n                     rlane[\"yieldQ30\"] += readMetric[\"YieldQ30\"]\n                     rlane[\"qscore_sum\"] += readMetric[\"QualityScoreSum\"]\n                     lsample[\"yieldQ30\"] += readMetric[\"YieldQ30\"]\n                     lsample[\"qscore_sum\"] += readMetric[\"QualityScoreSum\"]\n                     lsample[\"R{}_yield\".format(r)] += readMetric[\"Yield\"]\n                     lsample[\"R{}_Q30\".format(r)] += readMetric[\"YieldQ30\"]\n                     lsample[\"R{}_trimmed_bases\".format(r)] += readMetric[\"TrimmedBases\"]\n                 for r in range(1,5):\n                     if not lsample[\"R{}_yield\".format(r)] and not lsample[\"R{}_Q30\".format(r)] and not lsample[\"R{}_trimmed_bases\".format(r)]:\n                         lsample.pop(\"R{}_yield\".format(r))\n                         lsample.pop(\"R{}_Q30\".format(r))\n                         lsample.pop(\"R{}_trimmed_bases\".format(r))\n             undeterminedYieldQ30 = 0\n             undeterminedQscoreSum = 0\n             undeterminedTrimmedBases = 0\n             if \"Undetermined\" in conversionResult:\n                 for readMetric in conversionResult[\"Undetermined\"][\"ReadMetrics\"]:\n                     undeterminedYieldQ30 += readMetric[\"YieldQ30\"]\n                     undeterminedQscoreSum += readMetric[\"QualityScoreSum\"]\n                     undeterminedTrimmedBases += readMetric[\"TrimmedBases\"]\n                 run_data[lane][\"samples\"][\"undetermined\"] = {\n                     \"total\": conversionResult[\"Undetermined\"][\"NumberReads\"],\n                     \"total_yield\": conversionResult[\"Undetermined\"][\"Yield\"],\n                     \"perfectIndex\": 0,\n                     \"yieldQ30\": undeterminedYieldQ30,\n                     \"qscore_sum\": undeterminedQscoreSum,\n                     \"trimmed_bases\": undeterminedTrimmedBases\n                 }\n         for lane_id, lane in run_data.items():\n             try:\n                 lane[\"percent_Q30\"] = (float(lane[\"yieldQ30\"])\n                     / float(lane[\"total_yield\"])) * 100.0\n             except ZeroDivisionError:\n                 lane[\"percent_Q30\"] = \"NA\"\n             try:\n                 lane[\"percent_perfectIndex\"] = (float(lane[\"perfectIndex\"])\n                     / float(lane[\"total\"])) * 100.0\n             except ZeroDivisionError:\n                 lane[\"percent_perfectIndex\"] = \"NA\"\n             try:\n                 lane[\"mean_qscore\"] = float(lane[\"qscore_sum\"]) / float(lane[\"total_yield\"])\n             except ZeroDivisionError:\n                 lane[\"mean_qscore\"] = \"NA\"\n             for sample_id, sample in lane[\"samples\"].items():\n                 try:\n                     sample[\"percent_Q30\"] = (float(sample[\"yieldQ30\"])\n                         / float(sample[\"total_yield\"])) * 100.0\n                 except ZeroDivisionError:\n                     sample[\"percent_Q30\"] = \"NA\"\n                 try:\n                     sample[\"percent_perfectIndex\"] = (float(sample[\"perfectIndex\"])\n                         / float(sample[\"total\"])) * 100.0\n                 except ZeroDivisionError:\n                     sample[\"percent_perfectIndex\"] = \"NA\"\n                 try:\n                     sample[\"mean_qscore\"] = float(sample[\"qscore_sum\"]) / float(sample[\"total_yield\"])\n                 except ZeroDivisionError:\n                     sample[\"mean_qscore\"] = \"NA\"", "query": "parse json file"}
{"id": "https://github.com/matthew-brett/delocate/blob/ed48de15fce31c3f52f1a9f32cae1b02fc55aa60/delocate/tools.py#L72-L89", "method_name": "unique_by_index", "code": "def unique_by_index(sequence):\n     uniques = []\n     for element in sequence:\n         if element not in uniques:\n             uniques.append(element)\n     return uniques", "query": "unique elements"}
{"id": "https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/iotools/ecmwf_macc.py#L199-L228", "method_name": "get_nearest_indices", "code": "def get_nearest_indices(self, latitude, longitude):\n         idx_lat = int(round((latitude - 90.0) / self.delta_lat))\n         if idx_lat < 0:\n             idx_lat = 0  \n         elif idx_lat > self.lat_size:\n             idx_lat = self.lat_size  \n         longitude = longitude % 360.0\n         idx_lon = int(round(longitude / self.delta_lon)) % self.lon_size\n         return idx_lat, idx_lon", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/CalebBell/fluids/blob/57f556752e039f1d3e5a822f408c184783db2828/fluids/friction.py#L60-L71", "method_name": "fuzzy_match", "code": "def fuzzy_match(name, strings):\n     global fuzzy_match_fun\n     if fuzzy_match_fun is not None:\n         return fuzzy_match_fun(name, strings)\n     try:\n         from fuzzywuzzy import process, fuzz\n         fuzzy_match_fun = lambda name, strings: process.extractOne(name, strings, scorer=fuzz.partial_ratio)[0]\n     except ImportError: \n         import difflib\n         fuzzy_match_fun = lambda name, strings: difflib.get_close_matches(name, strings, n=1, cutoff=0)[0]\n     return fuzzy_match_fun(name, strings)", "query": "fuzzy match ranking"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/graphics/camera.py#L181-L194", "method_name": "_get_projection_matrix", "code": "def _get_projection_matrix(self):\n         fov = self.fov*np.pi/180.0\n         top = np.tan(fov * 0.5)*self.z_near\n         bottom = -top\n         left = self.aspectratio * bottom\n         right = self.aspectratio * top\n         return clip_matrix(left, right, bottom, top,\n                            self.z_near, self.z_far, perspective=True)", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/parsers/crontab.py#L116-L156", "method_name": "parse_content", "code": "def parse_content(self, content):\n         self.data = []\n         self.environment = {}\n         self.invalid_lines = []\n         nicknames = {\n             \"@yearly\": \"0 0 1 1 *\",\n             \"@annually\": \"0 0 1 1 *\",\n             \"@monthly\": \"0 0 1 * *\",\n             \"@weekly\": \"0 0 * * 0\",\n             \"@daily\": \"0 0 * * *\",\n             \"@hourly\": \"0 * * * *\",\n         }\n         cron_re = re.compile(_make_cron_re(), flags=re.IGNORECASE)\n         env_re = re.compile(r\"^\\s*(?P<key>\\w+)\\s*=\\s*(?P<value>\\S.*)$\")\n         for line in get_active_lines(content):\n             if line.startswith(\"@\"):\n                 if line.startswith(\"@reboot\"):\n                     parts = line.split(None, 2)\n                     self.data.append({\"time\": \"@reboot\", \"command\": parts[1]})\n                     continue\n                 else:\n                     parts = line.split(None, 2)\n                     if parts[0] not in nicknames:\n                         raise ParseException(\n                             \"{n} not recognised as a time specification \"nickname\"\".format(n=parts[0])\n                         )\n                     line = line.replace(parts[0], nicknames[parts[0]])\n             cron_match = cron_re.match(line)\n             env_match = env_re.match(line)\n             if cron_match:\n                 self.data.append(cron_match.groupdict())\n             elif env_match:\n                 self.environment[env_match.group(\"key\")] = env_match.group(\"value\")\n             else:\n                 self.invalid_lines.append(line)", "query": "extract data from html content"}
{"id": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/heatmaps.py#L392-L406", "method_name": "to_uint8", "code": "def to_uint8(self):\n         arr_0to255 = np.clip(np.round(self.arr_0to1 * 255), 0, 255)\n         arr_uint8 = arr_0to255.astype(np.uint8)\n         return arr_uint8", "query": "converting uint8 array to image"}
{"id": "https://github.com/simonw/datasette/blob/11b352b4d52fd02a422776edebb14f12e4994d3b/datasette/app.py#L521-L527", "method_name": "table_metadata", "code": "def table_metadata(self, database, table):\n         \"Fetch table-specific metadata.\"\n         return (self.metadata(\"databases\") or {}).get(database, {}).get(\n             \"tables\", {}\n         ).get(\n             table, {}\n         )", "query": "get database table name"}
{"id": "https://github.com/sprymix/metamagic.json/blob/c95d3cacd641d433af44f0774f51a085cb4888e6/metamagic/json/encoder.py#L156-L178", "method_name": "_encode_str", "code": "def _encode_str(self, obj, escape_quotes=True):\n         def replace(match):\n             s = match.group(0)\n             try:\n                 if escape_quotes:\n                     return ESCAPE_DCT[s]\n                 else:\n                     return BASE_ESCAPE_DCT[s]\n             except KeyError:\n                 n = ord(s)\n                 if n < 0x10000:\n                     return \"\\\\u{0:04x}\".format(n)\n                 else:\n                     n -= 0x10000\n                     s1 = 0xd800   ((n >> 10) & 0x3ff)\n                     s2 = 0xdc00   (n & 0x3ff)\n                     return \"\\\\u{0:04x}\\\\u{1:04x}\".format(s1, s2)\n         if escape_quotes:\n             return \n         else:\n             return BASE_ESCAPE_ASCII.sub(replace, obj)", "query": "html encode string"}
{"id": "https://github.com/AFriemann/simple_tools/blob/27d0f838c23309ebfc8afb59511220c6f8bb42fe/simple_tools/contextmanagers/time.py#L16-L20", "method_name": "timer", "code": "def timer(name):\n     startTime = time.time()\n     yield\n     elapsedTime = time.time() - startTime\n     print(\"[{}] finished in {} ms\".format(name, int(elapsedTime * 1000)))", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/lsst-epo/vela/blob/8e17ebec509be5c3cc2063f4645dfe9e26b49c18/astropixie-widgets/astropixie_widgets/visual.py#L39-L53", "method_name": "_telescope_pointing_widget", "code": "def _telescope_pointing_widget(cluster_name):\n     html = \"<table><thead><tr>\"\n     html += \"<td><b>Telescope pointing</b></td>\"\n     html += \"<td><b>Cluster Name</b></td>\"\n     html += \"<td><b>Image number</b></td>\"\n     html += \"<td><b>Right ascension</b></td>\"\n     html += \"<td><b>Declination</b></td>\"\n     html += \"</tr></thead><tbody><tr>\"\n     html += \"<td><img src=\"http://assets.lsst.rocks/data/sphere.png\"></td>\"\n     html += \"<td>%s</td>\" % cluster_name\n     html += \"<td>20221274993</td>\"\n     html += \"<td>05h 32m 37s</td>\"\n     html += \"<td>+00h 11m 18s</td>\"\n     html += \"</tr></tbody></table>\"\n     return Div(text=html, width=600, height=175)", "query": "reading element from html - <td>"}
{"id": "https://github.com/toomore/goristock/blob/e61f57f11a626cfbc4afbf66337fd9d1c51e3e71/grs/mobileapi.py#L38-L72", "method_name": "output", "code": "def output(self):\n     <table>\n             <tr><td>%(name)s</td><td>%(c)s</td><td>%(range)+.2f(%(pp)+.2f%%)</td></tr>\n             <tr><td>%(stock_no)s</td><td>%(value)s</td><td>%(time)s</td></tr></table>\n     if covstr(self.g[\"range\"]) > 0:\n       css = \"red\"\n     elif covstr(self.g[\"range\"]) < 0:\n       css = \"green\"\n     else:\n       css = \"gray\"\n     re = {\n       \"name\": self.g[\"name\"],\n       \"stock_no\": self.g[\"no\"],\n       \"time\": self.g[\"time\"],\n       \"open\": self.g[\"open\"],\n       \"h\": self.g[\"h\"],\n       \"l\": self.g[\"l\"],\n       \"c\": self.g[\"c\"],\n       \"max\": self.g[\"max\"],\n       \"min\": self.g[\"min\"],\n       \"range\": covstr(self.g[\"range\"]),\n       \"ranges\": self.g[\"ranges\"],\n       \"value\": self.g[\"value\"],\n       \"pvalue\": self.g[\"pvalue\"],\n       \"pp\": covstr(self.g[\"pp\"]),\n       \"top5buy\": self.g[\"top5buy\"],\n       \"top5sell\": self.g[\"top5sell\"],\n       \"crosspic\": self.g[\"crosspic\"],\n       \"css\": css\n     }\n     return re", "query": "reading element from html - <td>"}
{"id": "https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/cookie.py#L299-L342", "method_name": "create_cookie", "code": "def create_cookie(self, value, typ, cookie_name=None, ttl=-1, kill=False):\n         if kill:\n             ttl = -1\n         elif ttl < 0:\n             ttl = self.default_value[\"max_age\"]\n         if cookie_name is None:\n             cookie_name = self.default_value[\"name\"]\n         c_args = {}\n         srvdomain = self.default_value[\"domain\"]\n         if srvdomain and srvdomain not in [\"localhost\", \"127.0.0.1\",\n                                            \"0.0.0.0\"]:\n             c_args[\"domain\"] = srvdomain\n         srvpath = self.default_value[\"path\"]\n         if srvpath:\n             c_args[\"path\"] = srvpath\n         timestamp = str(int(time.time()))\n         try:\n             cookie_payload = \"::\".join([value, timestamp, typ])\n         except TypeError:\n             cookie_payload = \"::\".join([value[0], timestamp, typ])\n         cookie = make_cookie(\n             cookie_name, cookie_payload, self.sign_key,\n             timestamp=timestamp, enc_key=self.enc_key, max_age=ttl,\n             sign_alg=self.sign_alg, **c_args)\n         return cookie", "query": "create cookie"}
{"id": "https://github.com/ianlini/bistiming/blob/46a78ec647723c3516fc4fc73f2619ab41f647f2/examples/stopwatch_examples.py#L107-L119", "method_name": "cumulative_elapsed_time_example", "code": "def cumulative_elapsed_time_example():\n     print(\"[cumulative_elapsed_time_example] use python logging module with different log level\")\n     timer = Stopwatch(\"Waiting\")\n     with timer:\n         sleep(1)\n     sleep(1)\n     with timer:\n         sleep(1)\n         timer.log_elapsed_time(prefix=\"timer.log_elapsed_time(): \")  \n         print(\"timer.get_elapsed_time():\", timer.get_elapsed_time())  \n     print(\"timer.split_elapsed_time:\", timer.split_elapsed_time)\n     print(\"timer.get_cumulative_elapsed_time():\", timer.get_cumulative_elapsed_time())", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/HacKanCuBa/passphrase-py/blob/219d6374338ed9a1475b4f09b0d85212376f11e0/passphrase/secrets.py#L63-L84", "method_name": "randbelow", "code": "def randbelow(num: int) -> int:\n     if not isinstance(num, int):\n         raise TypeError(\"number must be an integer\")\n     if num <= 0:\n         raise ValueError(\"number must be greater than zero\")\n     if num == 1:\n         return 0\n     nbits = num.bit_length()    \n     randnum = random_randint(nbits)    \n     while randnum >= num:\n         randnum = random_randint(nbits)\n     return randnum", "query": "randomly pick a number"}
{"id": "https://github.com/joshburnett/scanf/blob/52f8911581c1590a3dcc6f17594eeb7b39716d42/scanf.py#L158-L184", "method_name": "extractdata", "code": "def extractdata(pattern, text=None, filepath=None):\n     y = []\n     if text is None:\n         textsource = open(filepath, \"r\")\n     else:\n         textsource = text.splitlines()\n     for line in textsource:\n         match = scanf(pattern, line)\n         if match:\n             if len(y) == 0:\n                 y = [[s] for s in match]\n             else:\n                 for i, ydata in enumerate(y):\n                     ydata.append(match[i])\n     if text is None:\n         textsource.close()\n     return y", "query": "extracting data from a text file"}
{"id": "https://github.com/Microsoft/nni/blob/c7cc8db32da8d2ec77a382a55089f4e17247ce41/src/sdk/pynni/nni/curvefitting_assessor/model_factory.py#L204-L221", "method_name": "normal_distribution", "code": "def normal_distribution(self, pos, sample):\n         curr_sigma_sq = self.sigma_sq(sample)\n         delta = self.trial_history[pos - 1] - self.f_comb(pos, sample)\n         return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))", "query": "normal distribution"}
{"id": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L373-L389", "method_name": "multiply", "code": "def multiply(self, matrix):\n         if not isinstance(matrix, DenseMatrix):\n             raise ValueError(\"Only multiplication with DenseMatrix \"\n                              \"is supported.\")\n         j_model = self._java_matrix_wrapper.call(\"multiply\", matrix)\n         return RowMatrix(j_model)", "query": "matrix multiply"}
{"id": "https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/controller/widgets/SignalFrame.py#L31-L33", "method_name": "perform_filter", "code": "def perform_filter(result_array: Array, data, f_low, f_high, filter_bw):\n    result_array = np.frombuffer(result_array.get_obj(), dtype=np.complex64)\n    result_array[:] = Filter.apply_bandpass_filter(data, f_low, f_high, filter_bw=filter_bw)", "query": "filter array"}
{"id": "https://github.com/gambogi/CSHLDAP/blob/09cb754b1e72437834e0d8cb4c7ac1830cfa6829/CSHLDAP.py#L336-L348", "method_name": "dateFromLDAPTimestamp", "code": "def dateFromLDAPTimestamp(timestamp):\n     numberOfCharacters = len(\"YYYYmmdd\")\n     timestamp = timestamp[:numberOfCharacters]\n     try:\n         day = datetime.strptime(timestamp, \"%Y%m%d\")\n         return date(year=day.year, month=day.month, day=day.day)\n     except:\n         print(timestamp)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/textio.py#L121-L140", "method_name": "hexadecimal", "code": "def hexadecimal(token):\n         token = \"\".join([ c for c in token if c.isalnum() ])\n         if len(token) % 2 != 0:\n             raise ValueError(\"Missing characters in hex data\")\n         data = \"\"\n         for i in compat.xrange(0, len(token), 2):\n             x = token[i:i+2]\n             d = int(x, 16)\n             s = struct.pack(\"<B\", d)\n             data += s\n         return data", "query": "convert decimal to hex"}
{"id": "https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L534-L574", "method_name": "copy_file", "code": "def copy_file(src, dst, ignore=None):\n    src = re.sub(\"[^\\w/\\-\\.\\*]\", \"\", src)\n    dst = re.sub(\"[^\\w/\\-\\.\\*]\", \"\", dst)\n    if len(re.sub(\"[\\W]\", \"\", src)) < 5 or len(re.sub(\"[\\W]\", \"\", dst)) < 5:\n       debug.log(\"Error: Copying file failed. Provided paths are invalid! src=\"%s\" dst=\"%s\"\"%(src, dst))\n    else:\n       check = False\n       if dst[-1] == \"/\":\n          if os.path.exists(dst):\n             check = True \n          else:\n             debug.log(\"Error: Copying file failed. Destination directory does not exist (%s)\"%(dst)) \n       elif os.path.exists(dst):\n          if os.path.isdir(dst):\n             check = True \n             dst += \"/\" \n          else:\n             debug.log(\"Error: Copying file failed. %s exists!\"%dst)\n       elif os.path.exists(os.path.dirname(dst)):\n          check = True \n       else:\n          debug.log(\"Error: Copying file failed. %s is an invalid distination!\"%dst)\n       if check:\n          files = glob.glob(src)\n          if ignore is not None: files = [fil for fil in files if not ignore in fil]\n          if len(files) != 0:\n             debug.log(\"Copying File(s)...\", \"Copy from %s\"%src, \"to %s\"%dst) \n             for file_ in files:\n                if os.path.isfile(file_):\n                   debug.log(\"Copying file: %s\"%file_) \n                   shutil.copy(file_, dst)\n                else:\n                   debug.log(\"Error: Copying file failed. %s is not a regular file!\"%file_) \n          else: debug.log(\"Error: Copying file failed. No files were found! (%s)\"%src) ", "query": "copying a file to a path"}
{"id": "https://github.com/bigchaindb/bigchaindb/blob/835fdfcf598918f76139e3b88ee33dd157acaaa7/bigchaindb/commands/utils.py#L53-L76", "method_name": "_convert", "code": "def _convert(value, default=None, convert=None):\n     def convert_bool(value):\n         if value.lower() in (\"true\", \"t\", \"yes\", \"y\"):\n             return True\n         if value.lower() in (\"false\", \"f\", \"no\", \"n\"):\n             return False\n         raise ValueError(\"{} cannot be converted to bool\".format(value))\n     if value == \"\":\n         value = None\n     if convert is None:\n         if default is not None:\n             convert = type(default)\n         else:\n             convert = str\n     if convert == bool:\n         convert = convert_bool\n     if value is None:\n         return default\n     else:\n         return convert(value)", "query": "convert int to bool"}
{"id": "https://github.com/PiotrDabkowski/Js2Py/blob/c0fa43f5679cf91ca8986c5747fcb07a433dc584/js2py/internals/prototypes/jsarray.py#L382-L398", "method_name": "filter", "code": "def filter(this, args):\n         array = to_object(this, args.space)\n         callbackfn = get_arg(args, 0)\n         arr_len = js_arr_length(array)\n         if not is_callable(callbackfn):\n             raise MakeError(\"TypeError\", \"callbackfn must be a function\")\n         _this = get_arg(args, 1)\n         k = 0\n         res = []\n         while k < arr_len:\n             if array.has_property(unicode(k)):\n                 kValue = array.get(unicode(k))\n                 if to_boolean(\n                         callbackfn.call(_this, (kValue, float(k), array))):\n                     res.append(kValue)\n             k += 1\n         return args.space.ConstructArray(res)", "query": "filter array"}
{"id": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/url.py#L75-L125", "method_name": "get", "code": "def get(cls):  \n         if Check().is_url_valid() or PyFunceble.CONFIGURATION[\"local\"]:\n             if \"current_test_data\" in PyFunceble.INTERN:\n                 PyFunceble.INTERN[\"current_test_data\"][\"url_syntax_validation\"] = True\n             PyFunceble.INTERN.update({\"http_code\": HTTPCode().get()})\n             active_list = []\n             active_list.extend(PyFunceble.HTTP_CODE[\"list\"][\"potentially_up\"])\n             active_list.extend(PyFunceble.HTTP_CODE[\"list\"][\"up\"])\n             inactive_list = []\n             inactive_list.extend(PyFunceble.HTTP_CODE[\"list\"][\"potentially_down\"])\n             inactive_list.append(\"*\" * 3)\n             if PyFunceble.INTERN[\"http_code\"] in active_list:\n                 return URLStatus(PyFunceble.STATUS[\"official\"][\"up\"]).handle()\n             if PyFunceble.INTERN[\"http_code\"] in inactive_list:\n                 return URLStatus(PyFunceble.STATUS[\"official\"][\"down\"]).handle()\n         if \"current_test_data\" in PyFunceble.INTERN:\n             PyFunceble.INTERN[\"current_test_data\"][\"url_syntax_validation\"] = False\n         return URLStatus(PyFunceble.STATUS[\"official\"][\"invalid\"]).handle()", "query": "get the description of a http status code"}
{"id": "https://github.com/PiotrDabkowski/Js2Py/blob/c0fa43f5679cf91ca8986c5747fcb07a433dc584/js2py/internals/prototypes/jsarray.py#L157-L183", "method_name": "sort", "code": "def sort(\n             this, args\n     ):  \n         cmpfn = get_arg(args, 0)\n         if not GetClass(this) in (\"Array\", \"Arguments\"):\n             return to_object(this, args.space)  \n         arr_len = js_arr_length(this)\n         if not arr_len:\n             return this\n         arr = [\n             (this.get(unicode(e)) if this.has_property(unicode(e)) else None)\n             for e in xrange(arr_len)\n         ]\n         if not is_callable(cmpfn):\n             cmpfn = None\n         cmp = lambda a, b: sort_compare(a, b, cmpfn)\n         if six.PY3:\n             key = functools.cmp_to_key(cmp)\n             arr.sort(key=key)\n         else:\n             arr.sort(cmp=cmp)\n         for i in xrange(arr_len):\n             if arr[i] is None:\n                 this.delete(unicode(i))\n             else:\n                 this.put(unicode(i), arr[i])\n         return this", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/bulkan/robotframework-requests/blob/11baa3277f1cb728712e26d996200703c15254a8/src/RequestsLibrary/RequestsKeywords.py#L460-L477", "method_name": "to_json", "code": "def to_json(self, content, pretty_print=False):\n         if PY3:\n             if isinstance(content, bytes):\n                 content = content.decode(encoding=\"utf-8\")\n         if pretty_print:\n             json_ = self._json_pretty_print(content)\n         else:\n             json_ = json.loads(content)\n         logger.info(\"To JSON using : content=%s \" % (content))\n         logger.info(\"To JSON using : pretty_print=%s \" % (pretty_print))\n         return json_", "query": "pretty print json"}
{"id": "https://github.com/BerkeleyAutomation/perception/blob/03d9b37dd6b66896cdfe173905c9413c8c3c5df6/perception/image.py#L252-L291", "method_name": "from_array", "code": "def from_array(x, frame=\"unspecified\"):\n         if not Image.can_convert(x):\n             raise ValueError(\"Cannot convert array to an Image!\")\n         dtype = x.dtype\n         height = x.shape[0]\n         width = x.shape[1]\n         channels = 1\n         if len(x.shape) == 3:\n             channels = x.shape[2]\n         if dtype == np.uint8:\n             if channels == 1:\n                 if np.any((x % BINARY_IM_MAX_VAL) > 0):\n                     return GrayscaleImage(x, frame)\n                 return BinaryImage(x, frame)\n             elif channels == 3:\n                 return ColorImage(x, frame)\n             else:\n                 raise ValueError(\n                     \"No available image conversion for uint8 array with 2 channels\")\n         elif dtype == np.uint16:\n             if channels != 1:\n                 raise ValueError(\n                     \"No available image conversion for uint16 array with 2 or 3 channels\")\n             return GrayscaleImage(x, frame)\n         elif dtype == np.float32 or dtype == np.float64:\n             if channels == 1:\n                 return DepthImage(x, frame)\n             elif channels == 2:\n                 return GdImage(x, frame)\n             elif channels == 3:\n                 logging.warning(\"Converting float array to uint8\")\n                 return ColorImage(x.astype(np.uint8), frame)\n             return RgbdImage(x, frame)\n         else:\n             raise ValueError(\n                 \"Conversion for dtype %s not supported!\" %\n                 (str(dtype)))", "query": "converting uint8 array to image"}
{"id": "https://github.com/Pajinek/vhm/blob/e323e99855fd5c40fd61fba87c2646a1165505ed/tools/vhmlib/manager.py#L103-L111", "method_name": "get_pid", "code": "def get_pid(self, id):\n         f = open(self.config[id][\"pidfile\"])\n         try:\n             pid = int(f.read().strip())\n         except ValueError:\n             print \"Wrong PID format (int %s)\" % self.config[id][\"pidfile\"]\n             sys.exit(1)\n         f.close()\n         return pid", "query": "get current process id"}
{"id": "https://github.com/nikcub/paths/blob/2200b85273d07d7a3c8b15ceb3b03cbb5c11439a/paths/__init__.py#L85-L107", "method_name": "find_executable", "code": "def find_executable(executable, path=None):\n   if path is None:\n     path = os.environ[\"PATH\"]\n   paths = path.split(os.pathsep)\n   base, ext = os.path.splitext(executable)\n   if (sys.platform == \"win32\" or os.name == \"os2\") and (ext != \".exe\"):\n     executable = executable + \".exe\"\n   if not os.path.isfile(executable):\n     for p in paths:\n       f = os.path.join(p, executable)\n       if os.path.isfile(f):\n         return f\n     return None\n   else:\n     return executable", "query": "get executable path"}
{"id": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/strip_url_params.py#L85-L95", "method_name": "strip_url_params3", "code": "def strip_url_params3(url, strip=None):\n     if not strip: strip = []\n     parse = urllib.parse.urlparse(url)\n     query = urllib.parse.parse_qs(parse.query)\n     query = {k: v[0] for k, v in query.items() if k not in strip}\n     query = urllib.parse.urlencode(query)\n     new = parse._replace(query=query)\n     return new.geturl()", "query": "parse query string in url"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/check_species.py#L14-L19", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Check species consistency between a VASP POSCAR file and a POTCAR file.\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\", nargs=\"?\", default=\"POSCAR\" )\n     parser.add_argument( \"potcar\", help=\"filename of the VASP POTCAR to be processed\", nargs=\"?\", default=\"POTCAR\" )\n     parser.add_argument( \"-p\", \"--ppset\", help=\"check whether the POTCAR pseudopotentials belong to a specific pseudopotential set\", choices=potcar_sets )\n     return parser.parse_args()", "query": "parse command line argument"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/smart_contract/native_contract/governance.py#L403-L414", "method_name": "to_json", "code": "def to_json(self):\n         map = dict()\n         map[\"peer_pubkey\"] = self.peer_pubkey\n         map[\"max_authorize\"] = self.max_authorize\n         map[\"old_peerCost\"] = self.old_peerCost\n         map[\"new_peer_cost\"] = self.new_peer_cost\n         map[\"set_cost_view\"] = self.set_cost_view\n         map[\"field1\"] = self.field1\n         map[\"field2\"] = self.field2\n         map[\"field3\"] = self.field3\n         map[\"field4\"] = self.field4\n         return map", "query": "map to json"}
{"id": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfobjects/pdftext.py#L165-L172", "method_name": "_underline", "code": "def _underline(self): \n         up = self.font.underline_position \n         ut = self.font.underline_thickness \n         w = self.font._string_width(self.text) \n         s = \"%.2f %.2f %.2f %.2f re f\" % (self.cursor.x, \n             self.cursor.y_prime - up, w, ut) \n         return s", "query": "underline text in label widget"}
{"id": "https://github.com/eliangcs/pystock-crawler/blob/8b803c8944f36af46daf04c6767a74132e37a101/pystock_crawler/spiders/yahoo.py#L12-L16", "method_name": "parse_date", "code": "def parse_date(date_str):\n     if date_str:\n         date = datetime.strptime(date_str, \"%Y%m%d\")\n         return date.year, date.month - 1, date.day\n     return \"\", \"\", \"\"", "query": "string to date"}
{"id": "https://github.com/mbedmicro/pyOCD/blob/41a174718a9739f3cbe785c2ba21cb7fd1310c6f/pyocd/debug/svd/parser.py#L82-L88", "method_name": "_parse_enumerated_value", "code": "def _parse_enumerated_value(self, enumerated_value_node):\n         return SVDEnumeratedValue(\n             name=_get_text(enumerated_value_node, \"name\"),\n             description=_get_text(enumerated_value_node, \"description\"),\n             value=_get_int(enumerated_value_node, \"value\"),\n             is_default=_get_int(enumerated_value_node, \"isDefault\")\n         )", "query": "get name of enumerated value"}
{"id": "https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_parsing.py#L22-L24", "method_name": "format_date_list", "code": "def format_date_list(dates):\n    format = dateformat.DateFormat(\"YYYY-MM-DD hh:mm:ss\")\n    return [format.format(date) for date in dates]", "query": "format date"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L59-L63", "method_name": "aes_ctr_encrypt", "code": "def aes_ctr_encrypt(plain_text: bytes, key: bytes):\n         cipher = AES.new(key=key, mode=AES.MODE_CTR)\n         cipher_text = cipher.encrypt(plain_text)\n         nonce = cipher.nonce\n         return nonce, cipher_text", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/hyde/fswrap/blob/41e4ad6f7e9ba73eabe61bd97847cd284e3edbd2/fswrap.py#L282-L289", "method_name": "read_all", "code": "def read_all(self, encoding=\"utf-8\"):\n         logger.info(\"Reading everything from %s\" % self)\n         with codecs.open(self.path, \"r\", encoding) as fin:\n             read_text = fin.read()\n         return read_text", "query": "buffered file reader read text"}
{"id": "https://github.com/DBuildService/dockerfile-parse/blob/3d7b514d8b8eded1b33529cf0f6a0770a573aee0/dockerfile_parse/parser.py#L708-L722", "method_name": "image_from", "code": "def image_from(from_value):\n     regex = re.compile(r)\n     match = re.match(regex, from_value)\n     return match.group(\"image\", \"name\") if match else (None, None)", "query": "regex case insensitive"}
{"id": "https://github.com/tjguk/winshell/blob/1509d211ab3403dd1cff6113e4e13462d6dec35b/winshell.py#L266-L294", "method_name": "copy_file", "code": "def copy_file(\n     source_path,\n     target_path,\n     allow_undo=True,\n     no_confirm=False,\n     rename_on_collision=True,\n     silent=False,\n     extra_flags=0,\n     hWnd=None\n ):\n     return _file_operation(\n         shellcon.FO_COPY,\n         source_path,\n         target_path,\n         allow_undo,\n         no_confirm,\n         rename_on_collision,\n         silent,\n         extra_flags,\n         hWnd\n     )", "query": "copying a file to a path"}
{"id": "https://github.com/dnanexus/dx-toolkit/blob/74befb53ad90fcf902d8983ae6d74580f402d619/doc/examples/dx-apps/report_example/src/report_example.py#L127-L137", "method_name": "generate_html", "code": "def generate_html():\n     html_file = open(html_filename, \"w\")\n     html_file.write(\"<html><body>\")\n     html_file.write(\"<h1>Here are some graphs for you!</h1>\")\n     for image in [lines_filename, bars_filename, histogram_filename]:\n         html_file.write(\"<div><h2>{0}</h2><img src=\"{0}\" /></div>\".format(image))\n     html_file.write(\"</body></html>\")\n     html_file.close()", "query": "output to html file"}
{"id": "https://github.com/szairis/sakmapper/blob/ac462fd2674e6aa1aa3b209222d8ac4e9268a790/sakmapper/network.py#L111-L170", "method_name": "optimal_clustering", "code": "def optimal_clustering(df, patch, method=\"kmeans\", statistic=\"gap\", max_K=5):\n     if len(patch) == 1:\n         return [patch]\n     if statistic == \"db\":\n         if method == \"kmeans\":\n             if len(patch) <= 5:\n                 K_max = 2\n             else:\n                 K_max = min(len(patch) / 2, max_K)\n             clustering = {}\n             db_index = []\n             X = df.ix[patch, :]\n             for k in range(2, K_max + 1):\n                 kmeans = cluster.KMeans(n_clusters=k).fit(X)\n                 clustering[k] = pd.DataFrame(kmeans.predict(X), index=patch)\n                 dist_mu = squareform(pdist(kmeans.cluster_centers_))\n                 sigma = []\n                 for i in range(k):\n                     points_in_cluster = clustering[k][clustering[k][0] == i].index\n                     sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n                 db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n             db_index = np.array(db_index)\n             k_optimal = np.argmin(db_index) + 2\n             return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n         elif method == \"agglomerative\":\n             if len(patch) <= 5:\n                 K_max = 2\n             else:\n                 K_max = min(len(patch) / 2, max_K)\n             clustering = {}\n             db_index = []\n             X = df.ix[patch, :]\n             for k in range(2, K_max + 1):\n                 agglomerative = cluster.AgglomerativeClustering(n_clusters=k, linkage=\"average\").fit(X)\n                 clustering[k] = pd.DataFrame(agglomerative.fit_predict(X), index=patch)\n                 tmp = [list(clustering[k][clustering[k][0] == i].index) for i in range(k)]\n                 centers = np.array([np.mean(X.ix[c, :], axis=0) for c in tmp])\n                 dist_mu = squareform(pdist(centers))\n                 sigma = []\n                 for i in range(k):\n                     points_in_cluster = clustering[k][clustering[k][0] == i].index\n                     sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n                 db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n             db_index = np.array(db_index)\n             k_optimal = np.argmin(db_index) + 2\n             return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n     elif statistic == \"gap\":\n         X = np.array(df.ix[patch, :])\n         if method == \"kmeans\":\n             f = cluster.KMeans\n         gaps = gap(X, ks=range(1, min(max_K, len(patch))), method=f)\n         k_optimal = list(gaps).index(max(gaps))+1\n         clustering = pd.DataFrame(f(n_clusters=k_optimal).fit_predict(X), index=patch)\n         return [list(clustering[clustering[0] == i].index) for i in range(k_optimal)]\n     else:\n         raise \"error: only db and gat statistics are supported\"", "query": "k means clustering"}
{"id": "https://github.com/davidfokkema/artist/blob/26ae7987522622710f2910980770c50012fda47d/demo/demo_histogram_fit.py#L8-L45", "method_name": "main", "code": "def main():\n     np.random.seed(1)\n     N = np.random.normal(size=2000)\n     edge = 5\n     bin_width = .1\n     bins = np.arange(-edge, edge + .5 * bin_width, bin_width)\n     n, bins = np.histogram(N, bins=bins)\n     x = (bins[:-1] + bins[1:]) / 2\n     y = n\n     f = lambda x, N, mu, sigma: N * scipy.stats.norm.pdf(x, mu, sigma)\n     popt, pcov = scipy.optimize.curve_fit(f, x, y)\n     print(\"Parameters from fit (N, mu, sigma):\", popt)\n     graph = Plot()\n     graph.histogram(n, bins)\n     x = np.linspace(-edge, edge, 100)\n     graph.plot(x, f(x, *popt), mark=None)\n     graph.set_xlabel(\"value\")\n     graph.set_ylabel(\"count\")\n     graph.set_label(\"Fit to data\")\n     graph.set_xlimits(-6, 6)\n     graph.save(\"histogram-fit\")", "query": "binomial distribution"}
{"id": "https://github.com/sunlightlabs/django-mediasync/blob/aa8ce4cfff757bbdb488463c64c0863cca6a1932/mediasync/__init__.py#L33-L38", "method_name": "compress", "code": "def compress(s):\n     zbuf = cStringIO.StringIO()\n     zfile = gzip.GzipFile(mode=\"wb\", compresslevel=6, fileobj=zbuf)\n     zfile.write(s)\n     zfile.close()\n     return zbuf.getvalue()", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/chaoss/grimoirelab-manuscripts/blob/94a3ad4f11bfbcd6c5190e01cb5d3e47a5187cd9/manuscripts/report.py#L812-L824", "method_name": "replace_text", "code": "def replace_text(filepath, to_replace, replacement):\n         with open(filepath) as file:\n             s = file.read()\n         s = s.replace(to_replace, replacement)\n         with open(filepath, \"w\") as file:\n             file.write(s)", "query": "replace in file"}
{"id": "https://github.com/flatironinstitute/kbucket/blob/867915ebb0ea153a399c3e392698f89bf43c7903/kbucket/kbucketclient.py#L639-L645", "method_name": "_read_json_file", "code": "def _read_json_file(path):\n   try:\n     with open(path) as f:\n       return json.load(f)\n   except:\n     print (\"Warning: Unable to read or parse json file: \"+path)\n     return None", "query": "parse json file"}
{"id": "https://github.com/firstprayer/monsql/blob/6285c15b574c8664046eae2edfeb548c7b173efd/monsql/queryset.py#L110-L117", "method_name": "distinct", "code": "def distinct(self):\n         new_query_set = self.clone()\n         new_query_set.query.distinct = True\n         return new_query_set", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/churchill-lab/emase/blob/ae3c6955bb175c1dec88dbf9fac1a7dcc16f4449/emase/Sparse3DMatrix.py#L130-L157", "method_name": "__mul__", "code": "def __mul__(self, other):\n         if self.finalized:\n             dmat = self.__class__()\n             dmat.shape = self.shape\n             if isinstance(other, Sparse3DMatrix):  \n                 if other.finalized:\n                     for hid in xrange(self.shape[1]):\n                         dmat.data.append(self.data[hid].multiply(other.data[hid]))\n                 else:\n                     raise RuntimeError(\"Both matrices must be finalized.\")\n             elif isinstance(other, (np.ndarray, csc_matrix, csr_matrix)):  \n                 for hid in xrange(self.shape[1]):\n                     dmat.data.append(self.data[hid] * other)\n                 dmat.shape = (other.shape[1], self.shape[1], self.shape[2])\n             elif isinstance(other, (coo_matrix, lil_matrix)):              \n                 other_csc = other.tocsc()\n                 for hid in xrange(self.shape[1]):\n                     dmat.data.append(self.data[hid] * other_csc)\n                 dmat.shape = (other_csc.shape[1], self.shape[1], self.shape[2])\n             elif isinstance(other, Number):  \n                 for hid in xrange(self.shape[1]):\n                     dmat.data.append(self.data[hid] * other)\n             else:\n                 raise TypeError(\"This operator is not supported between the given types.\")\n             dmat.finalized = True\n             return dmat\n         else:\n             raise RuntimeError(\"The original matrix must be finalized.\")", "query": "matrix multiply"}
{"id": "https://github.com/Neurita/boyle/blob/2dae7199849395a209c887d5f30506e1de8a9ad9/scripts/filetree.py#L20-L72", "method_name": "copy", "code": "def copy(configfile=\"\", destpath=\"\", overwrite=False, sub_node=\"\"):\n     log.info(\"Running {0} {1} {2}\".format(os.path.basename(__file__),\n                                           whoami(),\n                                           locals()))\n     assert(os.path.isfile(configfile))\n     if os.path.exists(destpath):\n         if os.listdir(destpath):\n             raise FolderAlreadyExists(\"Folder {0} already exists. Please clean \"\n                                       \"it or change destpath.\".format(destpath))\n     else:\n         log.info(\"Creating folder {0}\".format(destpath))\n         path(destpath).makedirs_p()\n     from boyle.files.file_tree_map import FileTreeMap\n     file_map = FileTreeMap()\n     try:\n         file_map.from_config_file(configfile)\n     except Exception as e:\n         raise FileTreeMapError(str(e))\n     if sub_node:\n         sub_map = file_map.get_node(sub_node)\n         if not sub_map:\n             raise FileTreeMapError(\"Could not find sub node \"\n                                    \"{0}\".format(sub_node))\n         file_map._filetree = {}\n         file_map._filetree[sub_node] = sub_map\n     try:\n         file_map.copy_to(destpath, overwrite=overwrite)\n     except Exception as e:\n         raise FileTreeMapError(str(e))", "query": "copying a file to a path"}
{"id": "https://github.com/SuminAndrew/lxml-asserts/blob/a5e8d25177357ba52e5eb0abacdc2f4bab59d584/lxml_asserts/__init__.py#L26-L59", "method_name": "_assert_tag_and_attributes_are_equal", "code": "def _assert_tag_and_attributes_are_equal(xml1, xml2, can_extend=False):\n     if xml1.tag != xml2.tag:\n         raise AssertionError(u\"Tags do not match: {tag1} != {tag2}\".format(\n             tag1=_describe_element(xml1), tag2=_describe_element(xml2)\n         ))\n     added_attributes = set(xml2.attrib).difference(xml1.attrib)\n     missing_attributes = set(xml1.attrib).difference(xml2.attrib)\n     if missing_attributes:\n         raise AssertionError(u\"Second xml misses attributes: {path}/({attributes})\".format(\n             path=_describe_element(xml2), attributes=\",\".join(missing_attributes)\n         ))\n     if not can_extend and added_attributes:\n         raise AssertionError(u\"Second xml has additional attributes: {path}/({attributes})\".format(\n             path=_describe_element(xml2), attributes=\",\".join(added_attributes)\n         ))\n     for attrib in xml1.attrib:\n         if not _xml_compare_text(xml1.attrib[attrib], xml2.attrib[attrib], False):\n             raise AssertionError(u\"Attribute values are not equal: {path}/{attribute}[\"{v1}\" != \"{v2}\"]\".format(\n                 path=_describe_element(xml1), attribute=attrib, v1=xml1.attrib[attrib], v2=xml2.attrib[attrib]\n             ))\n     if not _xml_compare_text(xml1.text, xml2.text, True):\n         raise AssertionError(u\"Tags text differs: {path}[\"{t1}\" != \"{t2}\"]\".format(\n             path=_describe_element(xml1), t1=xml1.text, t2=xml2.text\n         ))\n     if not _xml_compare_text(xml1.tail, xml2.tail, True):\n         raise AssertionError(u\"Tags tail differs: {path}[\"{t1}\" != \"{t2}\"]\".format(\n             path=_describe_element(xml1), t1=xml1.tail, t2=xml2.tail\n         ))", "query": "set file attrib hidden"}
{"id": "https://github.com/ajyoon/blur/blob/25fcf083af112bb003956a7a7e1c6ff7d8fef279/blur/rand.py#L252-L300", "method_name": "normal_distribution", "code": "def normal_distribution(mean, variance,\n                         minimum=None, maximum=None, weight_count=23):\n     standard_deviation = math.sqrt(variance)\n     min_x = (standard_deviation * -5) + mean\n     max_x = (standard_deviation * 5) + mean\n     step = (max_x - min_x) / weight_count\n     current_x = min_x\n     weights = []\n     while current_x < max_x:\n         weights.append(\n             (current_x, _normal_function(current_x, mean, variance))\n         )\n         current_x += step\n     if minimum is not None or maximum is not None:\n         return bound_weights(weights, minimum, maximum)\n     else:\n         return weights", "query": "normal distribution"}
{"id": "https://github.com/EventTeam/beliefs/blob/c07d22b61bebeede74a72800030dde770bf64208/src/beliefs/belief_utils.py#L143-L145", "method_name": "levenshtein_distance_metric", "code": "def levenshtein_distance_metric(a, b):\n    return (levenshtein_distance(a, b) / (2.0 * max(len(a), len(b), 1)))", "query": "string similarity levenshtein"}
{"id": "https://github.com/J535D165/recordlinkage/blob/87a5f4af904e0834047cd07ff1c70146b1e6d693/recordlinkage/algorithms/string.py#L52-L67", "method_name": "levenshtein_similarity", "code": "def levenshtein_similarity(s1, s2):\n     conc = pandas.Series(list(zip(s1, s2)))\n     def levenshtein_apply(x):\n         try:\n             return 1 - jellyfish.levenshtein_distance(x[0], x[1]) \\\n                 / np.max([len(x[0]), len(x[1])])\n         except Exception as err:\n             if pandas.isnull(x[0]) or pandas.isnull(x[1]):\n                 return np.nan\n             else:\n                 raise err\n     return conc.apply(levenshtein_apply)", "query": "string similarity levenshtein"}
{"id": "https://github.com/LIVVkit/LIVVkit/blob/680120cd437e408673e62e535fc0a246c7fc17db/livvkit/components/numerics.py#L82-L87", "method_name": "_print_summary", "code": "def _print_summary(module, case, summary):\n     try:\n         module.print_summary(case, summary)\n     except (NotImplementedError, AttributeError):\n         print(\"    Ran \" + case + \"!\")\n         print(\"\")", "query": "print model summary"}
{"id": "https://github.com/EntilZha/PyFunctional/blob/ac04e4a8552b0c464a7f492f7c9862424867b63e/functional/io.py#L115-L122", "method_name": "read", "code": "def read(self):\n         with gzip.GzipFile(self.path, compresslevel=self.compresslevel) as gz_file:\n             gz_file.read1 = gz_file.read\n             with io.TextIOWrapper(gz_file,\n                                   encoding=self.encoding,\n                                   errors=self.errors,\n                                   newline=self.newline) as file_content:\n                 return file_content.read()", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/marvin-ai/marvin-python-toolbox/blob/7c95cb2f9698b989150ab94c1285f3a9eaaba423/marvin_python_toolbox/common/utils.py#L286-L295", "method_name": "url_encode", "code": "def url_encode(url):\n     if isinstance(url, text_type):\n         url = url.encode(\"utf8\")\n     return quote(url, \":/%?&=\")", "query": "encode url"}
{"id": "https://github.com/shazow/unstdlib.py/blob/e0632fe165cfbfdb5a7e4bc7b412c9d6f2ebad83/unstdlib/standard/list_.py#L16-L36", "method_name": "groupby_count", "code": "def groupby_count(i, key=None, force_keys=None):\n     counter = defaultdict(lambda: 0)\n     if not key:\n         key = lambda o: o\n     for k in i:\n         counter[key(k)] += 1\n     if force_keys:\n         for k in force_keys:\n             counter[k] += 0\n     return counter.items()", "query": "group by count"}
{"id": "https://github.com/Microsoft/nni/blob/c7cc8db32da8d2ec77a382a55089f4e17247ce41/src/sdk/pynni/nni/curvefitting_assessor/model_factory.py#L204-L221", "method_name": "normal_distribution", "code": "def normal_distribution(self, pos, sample):\n         curr_sigma_sq = self.sigma_sq(sample)\n         delta = self.trial_history[pos - 1] - self.f_comb(pos, sample)\n         return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))", "query": "normal distribution"}
{"id": "https://github.com/rwl/pylon/blob/916514255db1ae1661406f0283df756baf960d14/contrib/pylontk.py#L377-L381", "method_name": "on_excel", "code": "def on_excel(self):\n         from pylon.io.excel import ExcelWriter\n         filename = asksaveasfilename(filetypes=[(\"Excel file\", \".xls\")])\n         if filename:\n             ExcelWriter(self.case).write(filename)", "query": "export to excel"}
{"id": "https://github.com/geometalab/pyGeoTile/blob/b1f44271698f5fc4d18c2add935797ed43254aa6/pygeotile/point.py#L12-L16", "method_name": "from_latitude_longitude", "code": "def from_latitude_longitude(cls, latitude=0.0, longitude=0.0):\n         assert -180.0 <= longitude <= 180.0, \"Longitude needs to be a value between -180.0 and 180.0.\"\n         assert -90.0 <= latitude <= 90.0, \"Latitude needs to be a value between -90.0 and 90.0.\"\n         return cls(latitude=latitude, longitude=longitude)", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/observable/decorator.py#L114-L143", "method_name": "_set_observable", "code": "def _set_observable(self, observable_name, new_value):\n         if self._has_value(observable_name):\n             old_value = self.new_value(observable_name)\n             must_set = np.any(new_value != old_value)\n             if must_set:\n                 if isinstance(old_value, Observable) and isinstance(new_value, Observable):\n                     old_value.copy_observers_to(new_value)\n                     old_value._notify_observers(observable_name=None, include_everything_observers=True)\n                     for observable_name in old_value._observers.keys():\n                         old_has_value = old_value._has_value(observable_name)\n                         new_has_value = new_value._has_value(observable_name)\n                         if old_has_value != new_has_value or (old_has_value and new_has_value and np.any(old_value._get_value(observable_name) != new_value._get_value(observable_name))):\n                             old_value._notify_observers(observable_name=observable_name, include_everything_observers=False)\n         else:\n             must_set = True\n         if must_set:\n             self._set_value(observable_name, new_value)\n             self._notify_observers(observable_name)", "query": "get current observable value"}
{"id": "https://github.com/decryptus/httpdis/blob/5d198cdc5558f416634602689b3df2c8aeb34984/httpdis/httpdis.py#L760-L769", "method_name": "set_cookie", "code": "def set_cookie(self, name, value = \"\", expires = 0, path = \"/\", domain = \"\", secure = False, http_only = False):\n         cook                    = Cookie.SimpleCookie()\n         cook[name]              = value\n         cook[name][\"expires\"]   = expires\n         cook[name][\"path\"]      = path\n         cook[name][\"domain\"]    = domain\n         cook[name][\"secure\"]    = secure\n         cook[name][\"httponly\"]  = http_only\n         self.send_header(\"Set-Cookie\", cook.output(header = \"\"))", "query": "create cookie"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568", "method_name": "assert_checked_checkbox", "code": "def assert_checked_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     assert check_box.is_selected(), \"Check box should be selected.\"", "query": "check if a checkbox is checked"}
{"id": "https://github.com/ModisWorks/modis/blob/1f1225c9841835ec1d1831fc196306527567db8b/modis/discord_modis/modules/music/api_music.py#L94-L234", "method_name": "parse_query", "code": "def parse_query(query, ilogger):\n     p = urlparse(query)\n     if p and p.scheme and p.netloc:\n         if \"youtube\" in p.netloc and p.query and ytdiscoveryapi is not None:\n             query_parts = p.query.split(\"&\")\n             yturl_parts = {}\n             for q in query_parts:\n                 s = q.split(\"=\")\n                 if len(s) < 2:\n                     continue\n                 q_name = s[0]\n                 q_val = \"=\".join(s[1:])\n                 if q_name not in yturl_parts:\n                     yturl_parts[q_name] = q_val\n             if \"list\" in yturl_parts:\n                 ilogger.info(\"Queued YouTube playlist from link\")\n                 return get_queue_from_playlist(yturl_parts[\"list\"])\n             elif \"v\" in yturl_parts:\n                 ilogger.info(\"Queued YouTube video from link\")\n                 return [[\"https://www.youtube.com/watch?v={}\".format(yturl_parts[\"v\"]), query]]\n         elif \"soundcloud\" in p.netloc:\n             if scclient is None:\n                 ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                 return [[query, query]]\n             try:\n                 result = scclient.get(\"/resolve\", url=query)\n                 track_list = []\n                 if isinstance(result, ResourceList):\n                     for r in result.data:\n                         tracks = get_sc_tracks(r)\n                         if tracks is not None:\n                             for t in tracks:\n                                 track_list.append(t)\n                 elif isinstance(result, Resource):\n                     tracks = get_sc_tracks(result)\n                     if tracks is not None:\n                         for t in tracks:\n                             track_list.append(t)\n                 if track_list is not None and len(track_list) > 0:\n                     ilogger.info(\"Queued SoundCloud songs from link\")\n                     return track_list\n                 else:\n                     ilogger.error(\"Could not queue from SoundCloud API\")\n                     return [[query, query]]\n             except Exception as e:\n                 logger.exception(e)\n                 ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                 return [[query, query]]\n         else:\n             ilogger.debug(\"Using url: {}\".format(query))\n             return [[query, query]]\n     args = query.split(\" \")\n     if len(args) == 0:\n         ilogger.error(\"No query given\")\n         return []\n     if args[0].lower() in [\"sp\", \"spotify\"] and spclient is not None:\n         if spclient is None:\n             ilogger.error(\"Host does not support Spotify\")\n             return []\n         try:\n             if len(args) > 2 and args[1] in [\"album\", \"artist\", \"song\", \"track\", \"playlist\"]:\n                 query_type = args[1].lower()\n                 query_search = \" \".join(args[2:])\n             else:\n                 query_type = \"track\"\n                 query_search = \" \".join(args[1:])\n             query_type = query_type.replace(\"song\", \"track\")\n             ilogger.info(\"Queueing Spotify {}: {}\".format(query_type, query_search))\n             spotify_tracks = search_sp_tracks(query_type, query_search)\n             if spotify_tracks is None or len(spotify_tracks) == 0:\n                 ilogger.error(\"Could not queue Spotify {}: {}\".format(query_type, query_search))\n                 return []\n             ilogger.info(\"Queued Spotify {}: {}\".format(query_type, query_search))\n             return spotify_tracks\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Error queueing from Spotify\")\n             return []\n     elif args[0].lower() in [\"sc\", \"soundcloud\"]:\n         if scclient is None:\n             ilogger.error(\"Host does not support SoundCloud\")\n             return []\n         try:\n             requests = [\"song\", \"songs\", \"track\", \"tracks\", \"user\", \"playlist\", \"tagged\", \"genre\"]\n             if len(args) > 2 and args[1] in requests:\n                 query_type = args[1].lower()\n                 query_search = \" \".join(args[2:])\n             else:\n                 query_type = \"track\"\n                 query_search = \" \".join(args[1:])\n             query_type = query_type.replace(\"song\", \"track\")\n             ilogger.info(\"Queueing SoundCloud {}: {}\".format(query_type, query_search))\n             soundcloud_tracks = search_sc_tracks(query_type, query_search)\n             ilogger.info(\"Queued SoundCloud {}: {}\".format(query_type, query_search))\n             return soundcloud_tracks\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Could not queue from SoundCloud\")\n             return []\n     elif args[0].lower() in [\"yt\", \"youtube\"] and ytdiscoveryapi is not None:\n         if ytdiscoveryapi is None:\n             ilogger.error(\"Host does not support YouTube\")\n             return []\n         try:\n             query_search = \" \".join(args[1:])\n             ilogger.info(\"Queued Youtube search: {}\".format(query_search))\n             return get_ytvideos(query_search, ilogger)\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Could not queue YouTube search\")\n             return []\n     if ytdiscoveryapi is not None:\n         ilogger.info(\"Queued YouTube search: {}\".format(query))\n         return get_ytvideos(query, ilogger)\n     else:\n         ilogger.error(\"Host does not support YouTube\".format(query))\n         return []", "query": "parse query string in url"}
{"id": "https://github.com/Rapptz/discord.py/blob/05d4f7f9620ef33635d6ac965b26528e09cdaf5b/discord/ext/commands/core.py#L94-L101", "method_name": "_convert_to_bool", "code": "def _convert_to_bool(argument):\n     lowered = argument.lower()\n     if lowered in (\"yes\", \"y\", \"true\", \"t\", \"1\", \"enable\", \"on\"):\n         return True\n     elif lowered in (\"no\", \"n\", \"false\", \"f\", \"0\", \"disable\", \"off\"):\n         return False\n     else:\n         raise BadArgument(lowered + \" is not a recognised boolean option\")", "query": "convert int to bool"}
{"id": "https://github.com/thomasjiangcy/django-rest-mock/blob/09e91de20d1a5efd5c47c6e3d7fe979443012e2c/rest_mock_server/core/parser.py#L43-L52", "method_name": "_parse_url", "code": "def _parse_url(url_string):\n         u = urlparse(url_string)\n         url = u.path\n         query = parse_qs(u.query)\n         return {\n             \"full_url\": url_string.strip(),\n             \"url\": url.strip(),\n             \"query\": query\n         }", "query": "parse query string in url"}
{"id": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/model/multinomial.py#L18-L26", "method_name": "confusion_matrix", "code": "def confusion_matrix(self, data):\n         assert_is_type(data, H2OFrame)\n         j = h2o.api(\"POST /3/Predictions/models/%s/frames/%s\" % (self._id, data.frame_id))\n         return j[\"model_metrics\"][0][\"cm\"][\"table\"]", "query": "confusion matrix"}
{"id": "https://github.com/erinxocon/requests-xml/blob/923571ceae4ddd4f2f57a2fc8780d89b50f3e7a1/requests_xml.py#L177-L203", "method_name": "json", "code": "def json(self, conversion: _Text = \"badgerfish\") -> Mapping:\n         if not self._json:\n             if conversion is \"badgerfish\":\n                 from xmljson import badgerfish as serializer\n             elif conversion is \"abdera\":\n                 from xmljson import abdera as serializer\n             elif conversion is \"cobra\":\n                 from xmljson import cobra as serializer\n             elif conversion is \"gdata\":\n                 from xmljson import gdata as serializer\n             elif conversion is \"parker\":\n                 from xmljson import parker as serializer\n             elif conversion is \"yahoo\":\n                 from xmljson import yahoo as serializer\n             self._json = json.dumps(serializer.data(etree.fromstring(self.xml)))\n         return self._json", "query": "json to xml conversion"}
{"id": "https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/stats.py#L114-L117", "method_name": "mad", "code": "def mad(v):\n     return np.median(np.abs(v - np.median(v)))", "query": "deducting the median from each column"}
{"id": "https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/filetypes.py#L264-L267", "method_name": "save", "code": "def save(self, filename, metadata={}, **data):\n         super(NumpyFile, self).save(filename, metadata, **data)\n         savefn = numpy.savez_compressed if self.compress else numpy.savez\n         savefn(self._savepath(filename), metadata=metadata, **data)", "query": "save list to file"}
{"id": "https://github.com/jbm950/pygame_toolbox/blob/3fe32145fc149e4dd0963c30a2b6a4dddd4fac0e/pygame_toolbox/graphics/widgets.py#L141-L162", "method_name": "__call__", "code": "def __call__(self, *arg):\n         if self.status:\n             self.status = 0\n             self.image.fill((255, 255, 255))\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)\n         else:\n             self.status = 1\n             if self.checktype == \"r\":\n                 self.draw_rect_check()\n             elif self.checktype == \"c\":\n                 self.draw_circle_check()\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)", "query": "make the checkbox checked"}
{"id": "https://github.com/Shoobx/xmldiff/blob/ec7835bce9ba69ff4ce03ab6c11397183b6f8411/xmldiff/_diff_match_patch_py3.py#L1110-L1134", "method_name": "diff_levenshtein", "code": "def diff_levenshtein(self, diffs):\n     levenshtein = 0\n     insertions = 0\n     deletions = 0\n     for (op, data) in diffs:\n       if op == self.DIFF_INSERT:\n         insertions += len(data)\n       elif op == self.DIFF_DELETE:\n         deletions += len(data)\n       elif op == self.DIFF_EQUAL:\n         levenshtein += max(insertions, deletions)\n         insertions = 0\n         deletions = 0\n     levenshtein += max(insertions, deletions)\n     return levenshtein", "query": "string similarity levenshtein"}
{"id": "https://github.com/churchill-lab/emase/blob/ae3c6955bb175c1dec88dbf9fac1a7dcc16f4449/emase/Sparse3DMatrix.py#L130-L157", "method_name": "__mul__", "code": "def __mul__(self, other):\n         if self.finalized:\n             dmat = self.__class__()\n             dmat.shape = self.shape\n             if isinstance(other, Sparse3DMatrix):  \n                 if other.finalized:\n                     for hid in xrange(self.shape[1]):\n                         dmat.data.append(self.data[hid].multiply(other.data[hid]))\n                 else:\n                     raise RuntimeError(\"Both matrices must be finalized.\")\n             elif isinstance(other, (np.ndarray, csc_matrix, csr_matrix)):  \n                 for hid in xrange(self.shape[1]):\n                     dmat.data.append(self.data[hid] * other)\n                 dmat.shape = (other.shape[1], self.shape[1], self.shape[2])\n             elif isinstance(other, (coo_matrix, lil_matrix)):              \n                 other_csc = other.tocsc()\n                 for hid in xrange(self.shape[1]):\n                     dmat.data.append(self.data[hid] * other_csc)\n                 dmat.shape = (other_csc.shape[1], self.shape[1], self.shape[2])\n             elif isinstance(other, Number):  \n                 for hid in xrange(self.shape[1]):\n                     dmat.data.append(self.data[hid] * other)\n             else:\n                 raise TypeError(\"This operator is not supported between the given types.\")\n             dmat.finalized = True\n             return dmat\n         else:\n             raise RuntimeError(\"The original matrix must be finalized.\")", "query": "matrix multiply"}
{"id": "https://github.com/boriel/zxbasic/blob/23b28db10e41117805bdb3c0f78543590853b132/arch/zx48k/backend/__init__.py#L2193-L2207", "method_name": "convertToBool", "code": "def convertToBool():\n     if not OPTIONS.strictBool.value:\n         return []\n     REQUIRES.add(\"strictbool.asm\")\n     result = []\n     result.append(\"pop af\")\n     result.append(\"call __NORMALIZE_BOOLEAN\")\n     result.append(\"push af\")\n     return result", "query": "convert int to bool"}
{"id": "https://github.com/tornadoweb/tornado/blob/b8b481770bcdb333a69afde5cce7eaa449128326/tornado/web.py#L2209-L2222", "method_name": "reverse_url", "code": "def reverse_url(self, name: str, *args: Any) -> str:\n         reversed_url = self.default_router.reverse_url(name, *args)\n         if reversed_url is not None:\n             return reversed_url\n         raise KeyError(\"%s not found in named urls\" % name)", "query": "reverse a string"}
{"id": "https://github.com/vmirly/pyclust/blob/bdb12be4649e70c6c90da2605bc5f4b314e2d07e/pyclust/_kmeans.py#L78-L101", "method_name": "_kmeans", "code": "def _kmeans(X, n_clusters, max_iter, n_trials, tol):\n     n_samples, n_features = X.shape[0], X.shape[1]\n     centers_best = np.empty(shape=(n_clusters,n_features), dtype=float)\n     labels_best  = np.empty(shape=n_samples, dtype=int)\n     for i in range(n_trials):\n         centers, labels, sse_tot, sse_arr, n_iter  = _kmeans_run(X, n_clusters, max_iter, tol)\n         if i==0:\n             sse_tot_best = sse_tot\n             sse_arr_best = sse_arr\n             n_iter_best = n_iter\n             centers_best = centers.copy()\n             labels_best  = labels.copy()\n         if sse_tot < sse_tot_best:\n             sse_tot_best = sse_tot\n             sse_arr_best = sse_arr\n             n_iter_best = n_iter\n             centers_best = centers.copy()\n             labels_best  = labels.copy()\n     return(centers_best, labels_best, sse_arr_best, n_iter_best)", "query": "k means clustering"}
{"id": "https://github.com/LuminosoInsight/langcodes/blob/0cedf9ca257ebf7250de5d3a63ec33a7d198db58/langcodes/registry_parser.py#L6-L25", "method_name": "parse_file", "code": "def parse_file(file):\n     lines = []\n     for line in file:\n         line = line.rstrip(\"\n\")\n         if line == \"%%\":\n             yield from parse_item(lines)\n             lines.clear()\n         elif line.startswith(\"  \"):\n             lines[-1] += line[1:]\n         else:\n             lines.append(line)\n     yield from parse_item(lines)", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/one.py#L133-L144", "method_name": "binomial_prefactor", "code": "def binomial_prefactor(s,ia,ib,xpa,xpb):\n     total= 0\n     for t in range(s+1):\n         if s-ia <= t <= ib:\n             total +=  binomial(ia,s-t)*binomial(ib,t)* \\\n                      pow(xpa,ia-s+t)*pow(xpb,ib-t)\n     return total", "query": "binomial distribution"}
{"id": "https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/data/stats.py#L8-L21", "method_name": "lreg", "code": "def lreg(self, xcol, ycol, name=\"Regression\"):\n         try:\n             x = self.df[xcol].values.reshape(-1, 1)\n             y = self.df[ycol]\n             lm = linear_model.LinearRegression()\n             lm.fit(x, y)\n             predictions = lm.predict(x)\n             self.df[name] = predictions\n         except Exception as e:\n             self.err(e, \"Can not calculate linear regression\")", "query": "linear regression"}
{"id": "https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/utils.py#L515-L539", "method_name": "get_random_int", "code": "def get_random_int(min_v=0, max_v=10, number=5, seed=None):\n     rnd = random.Random()\n     if seed:\n         rnd = random.Random(seed)\n     return [rnd.randint(min_v, max_v) for p in range(0, number)]", "query": "randomly pick a number"}
{"id": "https://github.com/radujica/baloo/blob/f6e05e35b73a75e8a300754c6bdc575e5f2d53b9/baloo/weld/weld_ops.py#L529-L569", "method_name": "weld_unique", "code": "def weld_unique(array, weld_type):\n     obj_id, weld_obj = create_weld_object(array)\n     weld_template = \n     weld_obj.weld_code = weld_template.format(array=obj_id,\n                                               type=weld_type)\n     return weld_obj", "query": "unique elements"}
{"id": "https://github.com/django-leonardo/django-leonardo/blob/4b933e1792221a13b4028753d5f1d3499b0816d4/leonardo/module/web/widgets/forms.py#L55-L144", "method_name": "__init__", "code": "def __init__(self, *args, **kwargs):\n         request = kwargs.pop(\"request\", None)\n         model = kwargs.pop(\"model\", None)\n         super(WidgetForm, self).__init__(*args, **kwargs)\n         if isinstance(model, Page):\n             self.fields[\"parent\"] = PageSelectField(\n                 label=_(\"Parent\"), help_text=_(\"Parent Page\"))\n         else:\n             self.fields[\"parent\"].widget = forms.widgets.HiddenInput()\n         initial = kwargs.get(\"initial\", None)\n         if initial and initial.get(\"id\", None):\n             widget = self._meta.model.objects.get(\n                 id=initial[\"id\"])\n             data = widget.dimensions\n             self.init_content_themes()\n         elif \"instance\" in kwargs:\n             widget = kwargs[\"instance\"]\n             data = widget.dimensions\n             self.init_content_themes()\n         else:\n             data = []\n             widget = None\n             self.init_themes()\n             del self.fields[\"id\"]\n         main_fields = self._meta.model.fields()\n         main_fields.update({\"label\": \"label\"})\n         main_fields.pop(\"parent\", None)\n         self.helper.layout = Layout(\n             TabHolder(\n                 Tab(self._meta.model._meta.verbose_name.capitalize(),\n                     *self.get_main_fields(main_fields),\n                     css_id=\"field-{}\".format(slugify(self._meta.model))\n                     ),\n                 Tab(_(\"Styles\"),\n                     \"base_theme\", \"content_theme\", \"color_scheme\",\n                     \"prerendered_content\",\n                     Fieldset(_(\"Positions\"), \"layout\", \"align\",\n                              \"vertical_align\", \"parent\"),\n                     *self.get_id_field(),\n                     css_id=\"theme-widget-settings\"\n                     ),\n                 Tab(_(\"Effects\"),\n                     \"enter_effect_style\", \"enter_effect_duration\",\n                     \"enter_effect_delay\", \"enter_effect_offset\",\n                     \"enter_effect_iteration\",\n                     css_id=\"theme-widget-effects\"\n                     ),\n             ),\n             HTML(render_to_string(\"widget/_update_preview.html\",\n                                   {\"class_name\": \".\".join([\n                                       self._meta.model._meta.app_label,\n                                       self._meta.model._meta.model_name])\n                                    }))\n         )\n         self.fields[\"label\"].widget = forms.TextInput(\n             attrs={\"placeholder\": self._meta.model._meta.verbose_name})\n         if request:\n             _request = copy.copy(request)\n             _request.POST = {}\n             _request.method = \"GET\"\n             from .tables import WidgetDimensionTable\n             dimensions = Tab(_(\"Dimensions\"),\n                              HTML(\n                 WidgetDimensionTable(_request,\n                                      widget=widget,\n                                      data=data).render()),\n                              )\n             self.helper.layout[0].append(dimensions)\n         if \"text\" in self.fields:\n             self.fields[\"text\"].label = \"\"\n         self.init_custom_tabs()", "query": "underline text in label widget"}
{"id": "https://github.com/Alignak-monitoring/alignak/blob/f3c145207e83159b799d3714e4241399c7740a64/alignak/daemon.py#L1243-L1259", "method_name": "change_to_workdir", "code": "def change_to_workdir(self):\n         logger.info(\"Changing working directory to: %s\", self.workdir)\n         self.check_dir(self.workdir)\n         try:\n             os.chdir(self.workdir)\n         except OSError as exp:\n             self.exit_on_error(\"Error changing to working directory: %s. Error: %s. \"\n                                \"Check the existence of %s and the %s/%s account \"\n                                \"permissions on this directory.\"\n                                % (self.workdir, str(exp), self.workdir, self.user, self.group),\n                                exit_code=3)\n         self.pre_log.append((\"INFO\", \"Using working directory: %s\" % os.path.abspath(self.workdir)))", "query": "set working directory"}
{"id": "https://github.com/fboender/ansible-cmdb/blob/ebd960ac10684e8c9ec2b12751bba2c4c9504ab7/lib/mako/filters.py#L159-L174", "method_name": "htmlentityreplace_errors", "code": "def htmlentityreplace_errors(ex):\n     if isinstance(ex, UnicodeEncodeError):\n         bad_text = ex.object[ex.start:ex.end]\n         text = _html_entities_escaper.escape(bad_text)\n         return (compat.text_type(text), ex.end)\n     raise ex", "query": "html entities replace"}
{"id": "https://github.com/majerteam/sylk_parser/blob/0f62f9d3ca1c273017d7ea728f9db809f2241389/sylk_parser/sylk_parser.py#L27-L40", "method_name": "to_csv", "code": "def to_csv(self, fbuf, quotechar=\"\"\", delimiter=\",\"):\n         csvwriter = csv.writer(\n             fbuf,\n             quotechar=quotechar,\n             delimiter=delimiter,\n             lineterminator=\"\n\",\n             quoting=csv.QUOTE_ALL\n         )\n         if self.headers:\n             csvwriter.writerow(self.headers)\n         for line in self.sylk_handler.stream_rows():\n             csvwriter.writerow(line)", "query": "write csv"}
{"id": "https://github.com/roclark/sportsreference/blob/ea0bae432be76450e137671d2998eb38f962dffd/sportsreference/mlb/schedule.py#L172-L179", "method_name": "datetime", "code": "def datetime(self):\n         date_string = \"%s %s\" % (self._date, self._year)\n         date_string = re.sub(r\" \\(\\d+\\)\", \"\", date_string)\n         return datetime.strptime(date_string, \"%A, %b %d %Y\")", "query": "string to date"}
{"id": "https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L84-L93", "method_name": "hex2dec", "code": "def hex2dec(s):\n     if not isinstance(s, str):\n         s = str(s)\n     return int(s.upper(), 16)", "query": "convert decimal to hex"}
{"id": "https://github.com/mapillary/mapillary_tools/blob/816785e90c589cae6e8e34a5530ce8417d29591c/mapillary_tools/gpmf.py#L72-L133", "method_name": "parse_bin", "code": "def parse_bin(path):\n     f = open(path, \"rb\")\n     s = {}  \n     output = []\n     methods = {\n         \"GPS5\": parse_gps,\n         \"GPSU\": parse_time,\n         \"GPSF\": parse_fix,\n         \"GPSP\": parse_precision,\n         \"ACCL\": parse_accl,\n         \"GYRO\": parse_gyro,\n     }\n     d = {\"gps\": []}  \n     while True:\n         label = f.read(4)\n         if not label:  \n             break\n         desc = f.read(4)\n         if \"00\" == binascii.hexlify(desc[0]):\n             continue\n         val_size = struct.unpack(\">b\", desc[1])[0]\n         num_values = struct.unpack(\">h\", desc[2:4])[0]\n         length = val_size * num_values\n         if label == \"DVID\":\n             if len(d[\"gps\"]):  \n                 output.append(d)\n             d = {\"gps\": []}  \n         for i in range(num_values):\n             data = f.read(val_size)\n             if label in methods:\n                 methods[label](data, d, s)\n             if label == \"SCAL\":\n                 if 2 == val_size:\n                     s[i] = struct.unpack(\">h\", data)[0]\n                 elif 4 == val_size:\n                     s[i] = struct.unpack(\">i\", data)[0]\n                 else:\n                     raise Exception(\"unknown scal size\")\n         mod = length % 4\n         if mod != 0:\n             seek = 4 - mod\n             f.read(seek)  \n     return output", "query": "parse binary file to custom class"}
{"id": "https://github.com/pndurette/gTTS/blob/b01ac4eb22d40c6241202e202d0418ccf4f98460/gtts/tts.py#L238-L250", "method_name": "save", "code": "def save(self, savefile):\n         with open(str(savefile), \"wb\") as f:\n             self.write_to_fp(f)\n             log.debug(\"Saved to %s\", savefile)", "query": "save list to file"}
{"id": "https://github.com/devassistant/devassistant/blob/2dbfeaa666a64127263664d18969c55d19ecc83e/devassistant/gui/gui_helper.py#L466-L473", "method_name": "create_clipboard", "code": "def create_clipboard(self, text, selection=Gdk.SELECTION_CLIPBOARD):\n         clipboard = Gtk.Clipboard.get(selection)\n         clipboard.set_text(\"\n\".join(text), -1)\n         clipboard.store()\n         return clipboard", "query": "copy to clipboard"}
{"id": "https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L471-L491", "method_name": "findAllSubstrings", "code": "def findAllSubstrings(string, substring):\n     start = 0\n     positions = []\n     while True:\n         start = string.find(substring, start)\n         if start == -1:\n             break\n         positions.append(start)\n         start += 1\n     return positions", "query": "positions of substrings in string"}
{"id": "https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/aaf2/core.py#L73-L113", "method_name": "read_properties", "code": "def read_properties(self):\n         stream = self.dir.get(\"properties\")\n         if stream is None:\n             return\n         s = stream.open()\n         f = BytesIO(s.read())\n         (byte_order, version, entry_count) = P_HEADER_STRUCT.unpack(f.read(4))\n         if byte_order != 0x4c:\n             raise NotImplementedError(\"be byteorder\")\n         props = array.array(str(\"H\"))\n         if hasattr(props, \"frombytes\"):\n             props.frombytes(f.read(6 * entry_count))\n         else:\n             props.fromstring(f.read(6 * entry_count))\n         if sys.byteorder == \"big\":\n             props.byteswap()\n         for i in range(entry_count):\n             index = i * 3\n             pid = props[index + 0]\n             format = props[index + 1]\n             byte_size = props[index + 2]\n             data = f.read(byte_size)\n             p = property_formats[format](self, pid, format, version)\n             p.data = data\n             self.property_entries[pid] = p\n         for p in self.property_entries.values():\n             p.decode()\n             if isinstance(p, (properties.StrongRefSetProperty,\n                               properties.StrongRefVectorProperty,\n                               properties.WeakRefArrayProperty)):\n                 p.read_index()", "query": "read properties file"}
{"id": "https://github.com/matthew-brett/delocate/blob/ed48de15fce31c3f52f1a9f32cae1b02fc55aa60/delocate/tools.py#L72-L89", "method_name": "unique_by_index", "code": "def unique_by_index(sequence):\n     uniques = []\n     for element in sequence:\n         if element not in uniques:\n             uniques.append(element)\n     return uniques", "query": "unique elements"}
{"id": "https://github.com/F483/apigen/blob/f05ce1509030764721cc3393410fa12b609e88f2/apigen/apigen.py#L234-L244", "method_name": "_deserialize", "code": "def _deserialize(kwargs):\n     def deserialize(item):\n         if isinstance(item[1], str):\n             try:\n                 data = json.loads(item[1])  \n             except:\n                 data = item[1].decode(\"utf-8\")  \n         else:\n             data = item[1]  \n         return (item[0], data)\n     return dict(map(deserialize, kwargs.items()))", "query": "deserialize json"}
{"id": "https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/cwl/workflow.py#L421-L437", "method_name": "_create_variable", "code": "def _create_variable(orig_v, step, variables):\n     try:\n         v = _get_variable(orig_v[\"id\"], variables)\n     except ValueError:\n         v = copy.deepcopy(orig_v)\n         if not isinstance(v[\"id\"], six.string_types):\n             v[\"id\"] = _get_string_vid(v[\"id\"])\n     for key, val in orig_v.items():\n         if key not in [\"id\", \"type\"]:\n             v[key] = val\n     if orig_v.get(\"type\") != \"null\":\n         v[\"type\"] = orig_v[\"type\"]\n     v[\"id\"] = \"%s/%s\" % (step.name, get_base_id(v[\"id\"]))\n     return v", "query": "get current process id"}
{"id": "https://github.com/geex-arts/django-jet/blob/64d5379f8a2278408694ce7913bf25f26035b855/jet/dashboard/dashboard_modules/google_analytics.py#L267-L279", "method_name": "format_grouped_date", "code": "def format_grouped_date(self, data, group):\n         date = self.get_grouped_date(data, group)\n         if group == \"week\":\n             date = u\"%s 鈥?%s\" % (\n                 (date - datetime.timedelta(days=6)).strftime(\"%d.%m\"),\n                 date.strftime(\"%d.%m\")\n             )\n         elif group == \"month\":\n             date = date.strftime(\"%b, %Y\")\n         else:\n             date = formats.date_format(date, \"DATE_FORMAT\")\n         return date", "query": "format date"}
{"id": "https://github.com/keybase/python-triplesec/blob/0a73e18cfe542d0cd5ee57bd823a67412b4b717e/triplesec/crypto.py#L86-L92", "method_name": "encrypt", "code": "def encrypt(cls, data, key, iv_data):\n         validate_key_size(key, cls.key_size, \"AES\")\n         iv, ctr = iv_data\n         ciphertext = Crypto_AES.new(key, Crypto_AES.MODE_CTR,\n                                     counter=ctr).encrypt(data)\n         return iv + ciphertext", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/circuit_algebra.py#L822-L834", "method_name": "series_with_permutation", "code": "def series_with_permutation(self, other):\n         combined_permutation = tuple([self.permutation[p]\n                                       for p in other.permutation])\n         return CPermutation.create(combined_permutation)", "query": "all permutations of a list"}
{"id": "https://github.com/nvbn/thefuck/blob/40ab4eb62db57627bff10cf029d29c94704086a2/thefuck/rules/dirty_unzip.py#L15-L25", "method_name": "_zip_file", "code": "def _zip_file(command):\n     for c in command.script_parts[1:]:\n         if not c.startswith(\"-\"):\n             if c.endswith(\".zip\"):\n                 return c\n             else:\n                 return u\"{}.zip\".format(c)", "query": "unzipping large files"}
{"id": "https://github.com/mathiasertl/django-ca/blob/976d7ea05276320f20daed2a6d59c8f5660fe976/ca/django_ca/utils.py#L211-L222", "method_name": "int_to_hex", "code": "def int_to_hex(i):\n     s = hex(i)[2:].upper()\n     if six.PY2 is True and isinstance(i, long):  \n         s = s[:-1]\n     return add_colons(s)", "query": "convert decimal to hex"}
{"id": "https://github.com/ChrisCummins/labm8/blob/dd10d67a757aefb180cb508f86696f99440c94f5/text.py#L137-L167", "method_name": "diff", "code": "def diff(s1, s2):\n     return levenshtein(s1, s2) / max(len(s1), len(s2))", "query": "string similarity levenshtein"}
{"id": "https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/ouidb/pyauto/ouidb/config.py#L43-L50", "method_name": "get_top_entries", "code": "def get_top_entries(self):\n         query = \"select count(vendor_name) count, vendor_name from mac_vendor group by vendor_name having count > 10 order by count;\"\n         try:\n             db = self.get_db()\n             results = db.execute(query)\n             return [i for i in results]\n         finally:\n             db.close()", "query": "group by count"}
{"id": "https://github.com/alexmojaki/littleutils/blob/1132d2d2782b05741a907d1281cd8c001f1d1d9d/littleutils/__init__.py#L716-L739", "method_name": "timer", "code": "def timer(description=\"Operation\", log=None):\n     start = time()\n     yield\n     elapsed = time() - start\n     message = \"%s took %s seconds\" % (description, elapsed)\n     (print if log is None else log.info)(message)", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/erikrose/more-itertools/blob/6a91b4e25c8e12fcf9fc2b53cf8ee0fba293e6f9/more_itertools/more.py#L528-L566", "method_name": "distinct_permutations", "code": "def distinct_permutations(iterable):\n     def make_new_permutations(permutations, e):\n         for permutation in permutations:\n             for j in range(len(permutation)):\n                 yield permutation[:j] + [e] + permutation[j:]\n                 if permutation[j] == e:\n                     break\n             else:\n                 yield permutation + [e]\n     permutations = [[]]\n     for e in iterable:\n         permutations = make_new_permutations(permutations, e)\n     return (tuple(t) for t in permutations)", "query": "all permutations of a list"}
{"id": "https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L70-L95", "method_name": "_trigger_event", "code": "def _trigger_event(\n             self, holder: T.Any, alt_name: str, action: str, *event_args: T.Any\n     ) -> None:\n         if isinstance(self.observable, Observable):\n             observable = self.observable\n         elif isinstance(self.observable, str):\n             observable = getattr(holder, self.observable)\n         elif isinstance(holder, Observable):\n             observable = holder\n         else:\n             raise TypeError(\n                 \"This ObservableProperty is no member of an Observable \"\n                 \"object. Specify where to find the Observable object for \"\n                 \"triggering events with the observable keyword argument \"\n                 \"when initializing the ObservableProperty.\"\n             )\n         name = alt_name if self.event is None else self.event\n         event = \"{}_{}\".format(action, name)\n         observable.trigger(event, *event_args)", "query": "get current observable value"}
{"id": "https://github.com/ccxt/ccxt/blob/23062efd7a5892c79b370c9d951c03cf8c0ddf23/python/ccxt/base/exchange.py#L692-L693", "method_name": "filterBy", "code": "def filterBy(self, array, key, value=None):\n        return Exchange.filter_by(array, key, value)", "query": "filter array"}
{"id": "https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L90-L113", "method_name": "csv_writer", "code": "def csv_writer(molecules, options, prefix):\n     outdir = os.getcwd()\n     filename = prefix + \".csv\"\n     outfile = os.path.join(outdir, filename)\n     f = open(outfile, \"w\")\n     csv_writer = csv.writer(f)\n     mol = molecules[0]\n     write_csv_header(mol, csv_writer)\n     for mol in molecules:\n         write_csv_line(mol, csv_writer, options)\n     f.close()", "query": "write csv"}
{"id": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_list.py#L2155-L2189", "method_name": "sample_lists", "code": "def sample_lists(items_list, num=1, seed=None):\n     r\n     if seed is not None:\n         rng = np.random.RandomState(seed)\n     else:\n         rng = np.random\n     def random_choice(items, num):\n         size = min(len(items), num)\n         return rng.choice(items, size, replace=False).tolist()\n     samples_list = [random_choice(items, num)\n                     if len(items) > 0 else []\n                     for items in items_list]\n     return samples_list", "query": "randomly extract x items from a list"}
{"id": "https://github.com/DeepHorizons/iarm/blob/b913c9fd577b793a6bbced78b78a5d8d7cd88de4/iarm/arm_instructions/_meta.py#L126-L132", "method_name": "convert_to_integer", "code": "def convert_to_integer(self, str):\n         if str.startswith(\"0x\") or str.startswith(\"0X\"):\n             return int(str, 16)\n         elif str.startswith(\"2_\"):\n             return int(str[2:], 2)\n         else:\n             return int(str)", "query": "convert int to string"}
{"id": "https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/transformations.py#L11-L75", "method_name": "toUIntArray", "code": "def toUIntArray(img, dtype=None, cutNegative=True, cutHigh=True, \n                 range=None, copy=True): \n     mn, mx = None, None \n     if range is not None: \n         mn, mx = range \n     if dtype is None: \n         if mx is None: \n             mx = np.nanmax(img) \n         dtype = np.uint16 if mx > 255 else np.uint8 \n     dtype = np.dtype(dtype) \n     if dtype == img.dtype: \n         return img \n     b = {\"uint8\": 255, \n          \"uint16\": 65535, \n          \"uint32\": 4294967295, \n          \"uint64\": 18446744073709551615}[dtype.name] \n     if copy: \n         img = img.copy() \n     if range is not None: \n         img = np.asfarray(img) \n         img -= mn \n         img *= b / (mx - mn) \n         img = np.clip(img, 0, b) \n     else: \n         if cutNegative: \n             img[img < 0] = 0 \n         else: \n             mn = np.min(img) \n             if mn < 0: \n                 img -= mn  \n         if cutHigh: \n             img[img > b] = b \n         else: \n             mx = np.nanmax(img) \n             img = np.asfarray(img) * (float(b) / mx) \n     img = img.astype(dtype) \n     return img", "query": "converting uint8 array to image"}
{"id": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/gzip_util.py#L43-L51", "method_name": "open_fileobj", "code": "def open_fileobj(fileobj, mode=\"rb\", compresslevel=9):\n     if hasattr(gzip.GzipFile, \"__enter__\"):\n         return gzip.GzipFile(\n             filename=\"\", mode=mode, fileobj=fileobj,\n             compresslevel=compresslevel\n         )\n     return GzipFile(\n         filename=\"\", mode=mode, fileobj=fileobj, compresslevel=compresslevel\n     )", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/__init__.py#L180-L205", "method_name": "random_elements", "code": "def random_elements(self, elements=(\"a\", \"b\", \"c\"), length=None, unique=False):\n         fn = choices_distribution_unique if unique else choices_distribution\n         if length is None:\n             length = self.generator.random.randint(1, len(elements))\n         if unique and length > len(elements):\n             raise ValueError(\n                 \"Sample length cannot be longer than the number of unique elements to pick from.\")\n         if isinstance(elements, dict):\n             choices = elements.keys()\n             probabilities = elements.values()\n         else:\n             if unique:\n                 return self.generator.random.sample(elements, length)\n             choices = elements\n             probabilities = [1.0 for _ in range(len(choices))]\n         return fn(\n             list(choices),\n             list(probabilities),\n             self.generator.random,\n             length=length,\n         )", "query": "unique elements"}
{"id": "https://github.com/python-xlib/python-xlib/blob/8901e831737e79fe5645f48089d70e1d1046d2f2/Xlib/ext/xinput.py#L382-L387", "method_name": "parse_binary", "code": "def parse_binary(self, data, display):\n         class_type, length = struct.unpack(\"=HH\", data[:4])\n         class_struct = INFO_CLASSES.get(class_type, AnyInfo)\n         class_data, _ = class_struct.parse_binary(data, display)\n         data = data[length * 4:]\n         return class_data, data", "query": "parse binary file to custom class"}
{"id": "https://github.com/Seeed-Studio/wio-cli/blob/ce83f4c2d30be7f72d1a128acd123dfc5effa563/wio/commands/cmd_setup.py#L217-L384", "method_name": "serial_send", "code": "def serial_send(msvr, msvr_ip, xsvr, xsvr_ip, node_sn, node_key, port):\n     thread = termui.waiting_echo(\"Getting device information...\")\n     thread.daemon = True\n     thread.start()\n     flag = False\n     try:\n         with serial.Serial(port, 115200, timeout=5) as ser:\n             cmd = \"Blank?\\r\n\"\n             ser.write(cmd.encode(\"utf-8\"))\n             if \"Node\" in ser.readline():\n                 flag = True\n     except serial.SerialException as e:\n         thread.stop(\"\")\n         thread.join()\n         click.secho(\">> \", fg=\"red\", nl=False)\n         click.echo(e)\n         if e.errno == 13:\n             click.echo(\"For more information, see https://github.com/Seeed-Studio/wio-cli\n         return None\n     thread.stop(\"\")\n     thread.join()\n     if flag:\n         click.secho(\"> \", fg=\"green\", nl=False)\n         click.secho(\"Found Wio.\", fg=\"green\", bold=True)\n         click.echo()\n     else:\n         click.secho(\"> \", fg=\"green\", nl=False)\n         click.secho(\"No nearby Wio detected.\", fg=\"white\", bold=True)\n         if click.confirm(click.style(\"? \", fg=\"green\") +\n                 click.style(\"Would you like to wait and monitor for Wio entering configure mode\", bold=True),\n                 default=True):\n             thread = termui.waiting_echo(\"Waiting for a wild Wio to appear... (press ctrl + C to exit)\")\n             thread.daemon = True\n             thread.start()\n             flag = False\n             while 1:\n                 with serial.Serial(port, 115200, timeout=5) as ser:\n                     cmd = \"Blank?\\r\n\"\n                     ser.write(cmd.encode(\"utf-8\"))\n                     if \"Node\" in ser.readline():\n                         flag = True\n                         break\n             thread.stop(\"\")\n             thread.join()\n             click.secho(\"> \", fg=\"green\", nl=False)\n             click.secho(\"Found Wio.\", fg=\"green\", bold=True)\n             click.echo()\n         else:\n             click.secho(\"> \", fg=\"green\", nl=False)\n             click.secho(\"\nQuit wio setup!\", bg=\"white\", bold=True)\n     while 1:\n         if not click.confirm(click.style(\"? \", fg=\"green\") +\n                     click.style(\"Would you like to manually enter your Wi-Fi network configuration?\", bold=True),\n                     default=False):\n             thread = termui.waiting_echo(\"Asking the Wio to scan for nearby Wi-Fi networks...\")\n             thread.daemon = True\n             thread.start()\n             flag = False\n             with serial.Serial(port, 115200, timeout=3) as ser:\n                 cmd = \"SCAN\\r\n\"\n                 ser.write(cmd.encode(\"utf-8\"))\n                 ssid_list = []\n                 while True:\n                     ssid = ser.readline()\n                     if ssid == \"\\r\n\":\n                         flag = True\n                         break\n                     ssid = ssid.strip(\"\\r\n\")\n                     ssid_list.append(ssid)\n             if flag:\n                 thread.stop(\"\")\n                 thread.join()\n             else:\n                 thread.stop(\"\\rsearch failure...\n\")\n                 return None\n             while 1:\n                 for x in range(len(ssid_list)):\n                     click.echo(\"%s.) %s\" %(x, ssid_list[x]))\n                 click.secho(\"? \", fg=\"green\", nl=False)\n                 value = click.prompt(\n                             click.style(\"Please select the network to which your Wio should connect\", bold=True),\n                             type=int)\n                 if value >= 0 and value < len(ssid_list):\n                     ssid = ssid_list[value]\n                     break\n                 else:\n                     click.echo(click.style(\">> \", fg=\"red\") + \"invalid input, range 0 to %s\" %(len(ssid_list)-1))\n             ap = ssid\n         else:\n             ap = click.prompt(click.style(\"> \", fg=\"green\") +\n                 click.style(\"Please enter the SSID of your Wi-Fi network\", bold=True), type=str)\n         ap_pwd = click.prompt(click.style(\"> \", fg=\"green\") +\n             click.style(\"Please enter your Wi-Fi network password (leave blank for none)\", bold=True),\n             default=\"\", show_default=False)\n         d_name = click.prompt(click.style(\"> \", fg=\"green\") +\n         click.style(\"Please enter the name of a device will be created\", bold=True), type=str)\n         click.echo(click.style(\"> \", fg=\"green\") + \"Here\"s what we\"re going to send to the Wio:\")\n         click.echo()\n         click.echo(click.style(\"> \", fg=\"green\") + \"Wi-Fi network: \" +\n             click.style(ap, fg=\"green\", bold=True))\n         ap_pwd_p = ap_pwd\n         if ap_pwd_p == \"\":\n             ap_pwd_p = \"None\"\n         click.echo(click.style(\"> \", fg=\"green\") + \"Password: \" +\n             click.style(ap_pwd_p, fg=\"green\", bold=True))\n         click.echo(click.style(\"> \", fg=\"green\") + \"Device name: \" +\n             click.style(d_name, fg=\"green\", bold=True))\n         click.echo()\n         if click.confirm(click.style(\"? \", fg=\"green\") +\n             \"Would you like to continue with the information shown above?\", default=True):\n             break\n     click.echo()\n     thread = termui.waiting_echo(\"Sending Wi-Fi information to device...\")\n     thread.daemon = True\n     thread.start()\n     version = 1.1\n     with serial.Serial(port, 115200, timeout=10) as ser:\n         cmd = \"VERSION\\r\n\"\n         ser.write(cmd.encode(\"utf-8\"))\n         res = ser.readline()\n         try:\n             version = float(re.match(r\"([0-9]+.[0-9]+)\", res).group(0))\n         except Exception as e:\n             version = 1.1\n     send_flag = False\n     while 1:\n         with serial.Serial(port, 115200, timeout=10) as ser:\n             if version <= 1.1:\n                 cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\n\" %(ap, ap_pwd, node_key, node_sn, xsvr_ip, msvr_ip)\n             elif version >= 1.2:\n                 cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\n\" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)\n             else:\n                 cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\n\" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)\n             ser.write(cmd.encode(\"utf-8\"))\n             if \"ok\" in ser.readline():\n                 click.echo(click.style(\"\\r> \", fg=\"green\") + \"Send Wi-Fi information to device success.\")\n                 thread.stop(\"\")\n                 thread.join()\n                 send_flag = True\n         if send_flag:\n             break\n     if send_flag:\n         return {\"name\": d_name}\n     else:\n         return None", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/thunlp/THULAC-Python/blob/3f1f126cd92c3d2aebdf4ab4850de3c9428a3b66/thulac/manage/TimeWord.py#L110-L117", "method_name": "isHttpWord", "code": "def isHttpWord(self, word):\n         if(len(word) < 5):\n             return False\n         else:\n             if(word[0] == ord(\"h\") and word[1] == ord(\"t\") and word[2] == ord(\"t\") and word[3] == ord(\"p\")):\n                 return True\n             else:\n                 return False", "query": "determine a string is a valid word"}
{"id": "https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/playhouse/sqlite_ext.py#L1136-L1169", "method_name": "rank", "code": "def rank(raw_match_info, *raw_weights):\n     match_info = _parse_match_info(raw_match_info)\n     score = 0.0\n     p, c = match_info[:2]\n     weights = get_weights(c, raw_weights)\n     for phrase_num in range(p):\n         phrase_info_idx = 2 + (phrase_num * c * 3)\n         for col_num in range(c):\n             weight = weights[col_num]\n             if not weight:\n                 continue\n             col_idx = phrase_info_idx + (col_num * 3)\n             row_hits = match_info[col_idx]\n             all_rows_hits = match_info[col_idx + 1]\n             if row_hits > 0:\n                 score += weight * (float(row_hits) / all_rows_hits)\n     return -score", "query": "fuzzy match ranking"}
{"id": "https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/experimental/streaming/batched_queue.py#L222-L227", "method_name": "read_next", "code": "def read_next(self):\n         if not self.read_buffer:\n             self._read_next_batch()\n             assert self.read_buffer\n         self.read_item_offset += 1\n         return self.read_buffer.pop(0)", "query": "buffered file reader read text"}
{"id": "https://github.com/pymacaron/pymacaron/blob/af244f203f8216108b39d374d46bf8e1813f13d5/pymacaron/utils.py#L48-L60", "method_name": "to_epoch", "code": "def to_epoch(t):\n     if isinstance(t, str):\n         if \"+\" not in t:\n             t = t + \"+00:00\"\n         t = parser.parse(t)\n     elif t.tzinfo is None or t.tzinfo.utcoffset(t) is None:\n         t = t.replace(tzinfo=pytz.timezone(\"utc\"))\n     t0 = datetime.datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone(\"utc\"))\n     delta = t - t0\n     return int(delta.total_seconds())", "query": "convert a utc time to epoch"}
{"id": "https://github.com/nickolas360/librecaptcha/blob/bd247082fd98118be220f326e5cd162e6d926655/librecaptcha/recaptcha.py#L354-L380", "method_name": "find_challenge_goal", "code": "def find_challenge_goal(self, id, raw=False):\n         start = 0\n         matching_strings = []\n         def try_find():\n             nonlocal start\n             index = self.js_strings.index(id, start)\n             for i in range(FIND_GOAL_SEARCH_DISTANCE):\n                 next_str = self.js_strings[index + i + 1]\n                 if re.search(r\"\\bselect all\\b\", next_str, re.I):\n                     matching_strings.append((i, index, next_str))\n             start = index + FIND_GOAL_SEARCH_DISTANCE + 1\n         try:\n             while True:\n                 try_find()\n         except (ValueError, IndexError):\n             pass\n         try:\n             goal = min(matching_strings)[2]\n         except ValueError:\n             return None, None\n         raw = goal\n         plain = raw.replace(\"<strong>\", \"\").replace(\"</strong>\", \"\")\n         return raw, plain", "query": "find int in string"}
{"id": "https://github.com/jbarlow83/OCRmyPDF/blob/79c84eefa353632a3d7ccddbd398c6678c1c1777/src/ocrmypdf/hocrtransform.py#L156-L231", "method_name": "to_pdf", "code": "def to_pdf(\n         self,\n         outFileName,\n         imageFileName=None,\n         showBoundingboxes=False,\n         fontname=\"Helvetica\",\n         invisibleText=False,\n         interwordSpaces=False,\n     ):\n         pdf = Canvas(outFileName, pagesize=(self.width, self.height), pageCompression=1)\n         pdf.setStrokeColorRGB(0, 1, 1)\n         pdf.setFillColorRGB(0, 1, 1)\n         pdf.setLineWidth(0)  \n         for elem in self.hocr.findall(\".//%sp[@class=\"%s\"]\" % (self.xmlns, \"ocr_par\")):\n             elemtxt = self._get_element_text(elem).rstrip()\n             if len(elemtxt) == 0:\n                 continue\n             pxl_coords = self.element_coordinates(elem)\n             pt = self.pt_from_pixel(pxl_coords)\n             if showBoundingboxes:\n                 pdf.rect(\n                     pt.x1, self.height - pt.y2, pt.x2 - pt.x1, pt.y2 - pt.y1, fill=1\n                 )\n         found_lines = False\n         for line in self.hocr.findall(\n             \".//%sspan[@class=\"%s\"]\" % (self.xmlns, \"ocr_line\")\n         ):\n             found_lines = True\n             self._do_line(\n                 pdf,\n                 line,\n                 \"ocrx_word\",\n                 fontname,\n                 invisibleText,\n                 interwordSpaces,\n                 showBoundingboxes,\n             )\n         if not found_lines:\n             root = self.hocr.find(\".//%sdiv[@class=\"%s\"]\" % (self.xmlns, \"ocr_page\"))\n             self._do_line(\n                 pdf,\n                 root,\n                 \"ocrx_word\",\n                 fontname,\n                 invisibleText,\n                 interwordSpaces,\n                 showBoundingboxes,\n             )\n         if imageFileName is not None:\n             pdf.drawImage(imageFileName, 0, 0, width=self.width, height=self.height)\n         pdf.showPage()\n         pdf.save()", "query": "convert html to pdf"}
{"id": "https://github.com/JamesPHoughton/pysd/blob/bf1b1d03954e9ba5acac9ba4f1ada7cd93352eda/pysd/py_backend/functions.py#L150-L154", "method_name": "initialize", "code": "def initialize(self):\n         self.state = self.init_func()\n         if isinstance(self.state, xr.DataArray):\n             self.shape_info = {\"dims\": self.state.dims,\n                                \"coords\": self.state.coords}", "query": "initializing array"}
{"id": "https://github.com/intuition-io/intuition/blob/cd517e6b3b315a743eb4d0d0dc294e264ab913ce/intuition/core/configuration.py#L28-L56", "method_name": "parse_commandline", "code": "def parse_commandline():\n     parser = argparse.ArgumentParser(\n         description=\"Intuition, the terrific trading system\")\n     parser.add_argument(\"-V\", \"--version\",\n                         action=\"version\",\n                         version=\"%(prog)s v{} Licence {}\".format(\n                             __version__, __licence__),\n                         help=\"Print program version\")\n     parser.add_argument(\"-v\", \"--showlog\",\n                         action=\"store_true\",\n                         help=\"Print logs on stdout\")\n     parser.add_argument(\"-b\", \"--bot\",\n                         action=\"store_true\",\n                         help=\"Allows the algorithm to process orders\")\n     parser.add_argument(\"-c\", \"--context\",\n                         action=\"store\", default=\"file::conf.yaml\",\n                         help=\"Provides the way to build context\")\n     parser.add_argument(\"-i\", \"--id\",\n                         action=\"store\", default=\"gekko\",\n                         help=\"Customize the session id\")\n     args = parser.parse_args()\n     return {\n         \"session\": args.id,\n         \"context\": args.context,\n         \"showlog\": args.showlog,\n         \"bot\": args.bot\n     }", "query": "parse command line argument"}
{"id": "https://github.com/tehmaze/ipcalc/blob/d436b95d2783347c3e0084d76ec3c52d1f5d2f0b/ipcalc.py#L544-L558", "method_name": "to_reverse", "code": "def to_reverse(self):\n         if self.v == 4:\n             return \".\".join(list(self.dq.split(\".\")[::-1]) + [\"in-addr\", \"arpa\"])\n         else:\n             return \".\".join(list(self.hex())[::-1] + [\"ip6\", \"arpa\"])", "query": "reverse a string"}
{"id": "https://github.com/thunlp/THULAC-Python/blob/3f1f126cd92c3d2aebdf4ab4850de3c9428a3b66/thulac/manage/TimeWord.py#L110-L117", "method_name": "isHttpWord", "code": "def isHttpWord(self, word):\n         if(len(word) < 5):\n             return False\n         else:\n             if(word[0] == ord(\"h\") and word[1] == ord(\"t\") and word[2] == ord(\"t\") and word[3] == ord(\"p\")):\n                 return True\n             else:\n                 return False", "query": "determine a string is a valid word"}
{"id": "https://github.com/newville/wxmplot/blob/8e0dc037453e5cdf18c968dc5a3d29efd761edee/wxmplot/plotframe.py#L45-L47", "method_name": "scatterplot", "code": "def scatterplot(self, x, y, **kw):\n        self.panel.scatterplot(x, y, **kw)", "query": "scatter plot"}
{"id": "https://github.com/aouyar/PyMunin/blob/4f58a64b6b37c85a84cc7e1e07aafaa0321b249d/pysysinfo/mysql.py#L64-L69", "method_name": "_connect", "code": "def _connect(self):\n         if self._connParams:\n             self._conn = MySQLdb.connect(**self._connParams)\n         else:\n             self._conn = MySQLdb.connect(\"\")", "query": "connect to sql"}
{"id": "https://github.com/linnarsson-lab/loompy/blob/62c8373a92b058753baa3a95331fb541f560f599/loompy/color.py#L288-L289", "method_name": "random_within", "code": "def random_within(self, r):\n  return self.random.randint(int(r[0]), int(r[1]))", "query": "randomly pick a number"}
{"id": "https://github.com/tyiannak/pyAudioAnalysis/blob/e3da991e7247492deba50648a4c7c0f41e684af4/pyAudioAnalysis/audioVisualization.py#L32-L52", "method_name": "levenshtein", "code": "def levenshtein(str1, s2):\n     N1 = len(str1)\n     N2 = len(s2)\n     stringRange = [range(N1 + 1)] * (N2 + 1)\n     for i in range(N2 + 1):\n         stringRange[i] = range(i,i + N1 + 1)\n     for i in range(0,N2):\n         for j in range(0,N1):\n             if str1[j] == s2[i]:\n                 stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                             stringRange[i][j+1] + 1,\n                                             stringRange[i][j])\n             else:\n                 stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                             stringRange[i][j+1] + 1,\n                                             stringRange[i][j] + 1)\n     return stringRange[N2][N1]", "query": "string similarity levenshtein"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/proc_poscar.py#L7-L21", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Manipulates VASP POSCAR files\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\" )\n     parser.add_argument( \"-l\", \"--label\", type=int, choices=[ 1, 4 ], help=\"label coordinates with atom name at position {1,4}\" )\n     parser.add_argument( \"-c\", \"--coordinates-only\", help=\"only output coordinates\", action=\"store_true\" )\n     parser.add_argument( \"-t\", \"--coordinate-type\", type=str, choices=[ \"c\", \"cartesian\", \"d\", \"direct\" ], default=\"direct\", help=\"specify coordinate type for output {(c)artesian (d)irect} [default = (d)irect]\" )\n     parser.add_argument( \"-g\", \"--group\", help=\"group atoms within supercell\", action=\"store_true\" )\n     parser.add_argument( \"-s\", \"--supercell\", type=int, nargs=3, metavar=( \"h\", \"k\", \"l\" ), help=\"construct supercell by replicating (h,k,l) times along [a b c]\" )\n     parser.add_argument( \"-b\", \"--bohr\", action=\"store_true\", help=\"assumes the input file is in Angstrom, and converts everything to bohr\")\n     parser.add_argument( \"-n\", \"--number-atoms\", action=\"store_true\", help=\"label coordinates with atom number\" )\n     parser.add_argument( \"--scale\", action=\"store_true\", help=\"scale the lattice parameters by the scaling factor\" )\n     parser.add_argument( \"--selective\", choices=[ \"T\", \"F\" ], help=\"generate Selective Dynamics POSCAR with all values set to T / F\" )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/census-instrumentation/opencensus-python/blob/992b223f7e34c5dcb65922b7d5c827e7a1351e7d/contrib/opencensus-ext-django/examples/app/views.py#L110-L130", "method_name": "sqlalchemy_mysql_trace", "code": "def sqlalchemy_mysql_trace(request):\n     try:\n         engine = sqlalchemy.create_engine(\n             \"mysql+mysqlconnector://{}:{}@{}\".format(\"root\", MYSQL_PASSWORD,\n                                                      DB_HOST))\n         conn = engine.connect()\n         query = \"SELECT 2*3\"\n         result_set = conn.execute(query)\n         result = []\n         for item in result_set:\n             result.append(item)\n         return HttpResponse(str(result))\n     except Exception:\n         msg = \"Query failed. Check your env vars for connection settings.\"\n         return HttpResponse(msg, status=500)", "query": "connect to sql"}
{"id": "https://github.com/VIVelev/PyDojoML/blob/773fdce6866aa6decd306a5a85f94129fed816eb/dojo/cluster/hierarchical.py#L43-L62", "method_name": "cluster", "code": "def cluster(self, X):\n         X = super().cluster(X)\n         self._distances = linkage(X, method=self.linkage)\n         if self.mode == \"n_clusters\":\n             return fcluster(\n                 self._distances,\n                 self.n_clusters,\n                 criterion=\"maxclust\"\n             )\n         elif self.mode == \"max_distance\":\n             return fcluster(\n                 self._distances,\n                 self.max_distance,\n                 criterion=\"distance\"\n             )\n         else:\n             raise ParameterError(f\"Unknown / unsupported clustering mode: \\\"{self.mode}\\\"\")", "query": "k means clustering"}
{"id": "https://github.com/F483/apigen/blob/f05ce1509030764721cc3393410fa12b609e88f2/apigen/apigen.py#L234-L244", "method_name": "_deserialize", "code": "def _deserialize(kwargs):\n     def deserialize(item):\n         if isinstance(item[1], str):\n             try:\n                 data = json.loads(item[1])  \n             except:\n                 data = item[1].decode(\"utf-8\")  \n         else:\n             data = item[1]  \n         return (item[0], data)\n     return dict(map(deserialize, kwargs.items()))", "query": "deserialize json"}
{"id": "https://github.com/riptano/ccm/blob/275699f79d102b5039b79cc17fa6305dccf18412/ccmlib/common.py#L208-L219", "method_name": "replaces_in_file", "code": "def replaces_in_file(file, replacement_list):\n     rs = [(re.compile(regexp), repl) for (regexp, repl) in replacement_list]\n     file_tmp = file + \".\" + str(os.getpid()) + \".tmp\"\n     with open(file, \"r\") as f:\n         with open(file_tmp, \"w\") as f_tmp:\n             for line in f:\n                 for r, replace in rs:\n                     match = r.search(line)\n                     if match:\n                         line = replace + \"\n\"\n                 f_tmp.write(line)\n     shutil.move(file_tmp, file)", "query": "replace in file"}
{"id": "https://github.com/fred49/linshare-api/blob/be646c25aa8ba3718abb6869c620b157d53d6e41/linshareapi/admin/jwt.py#L52-L64", "method_name": "_list", "code": "def _list(self, domain=None):\n         url = \"{base}\".format(\n             base=self.local_base_url\n         )\n         param = {}\n         if domain:\n             param[\"domainUuid\"] = domain\n         encode = urllib.urlencode(param)\n         if encode:\n             url += \"?\"\n             url += encode\n         return self.core.list(url)", "query": "encode url"}
{"id": "https://github.com/inveniosoftware-contrib/json-merger/blob/adc6d372da018427e1db7b92424d3471e01a4118/json_merger/contrib/inspirehep/match.py#L199-L207", "method_name": "_find", "code": "def _find(self, node):\n         root = node\n         while root in self.parents:\n             root = self.parents[root]\n         while node in self.parents:\n             prev_node = node\n             node = self.parents[node]\n             self.parents[prev_node] = root\n         return root", "query": "get all parents of xml node"}
{"id": "https://github.com/sassoo/goldman/blob/b72540c9ad06b5c68aadb1b4fa8cb0b716260bf2/goldman/deserializers/json_7159.py#L22-L47", "method_name": "deserialize", "code": "def deserialize(self):\n         super(Deserializer, self).deserialize()\n         try:\n             return json.loads(self.req.get_body())\n         except TypeError:\n             link = \"tools.ietf.org/html/rfc7159\"\n             self.fail(\"Typically, this error is due to a missing JSON \"\n                       \"payload in your request when one was required. \"\n                       \"Otherwise, it could be a bug in our API.\", link)\n         except UnicodeDecodeError:\n             link = \"tools.ietf.org/html/rfc7159\n             self.fail(\"We failed to process your JSON payload & it is \"\n                       \"most likely due to non UTF-8 encoded characters \"\n                       \"in your JSON.\", link)\n         except ValueError as exc:\n             link = \"tools.ietf.org/html/rfc7159\"\n             self.fail(\"The JSON payload appears to be malformed & we \"\n                       \"failed to process it. The error with line & column \"\n                       \"numbers is: %s\" % exc.message, link)", "query": "deserialize json"}
{"id": "https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/peewee.py#L3708-L3712", "method_name": "_connect", "code": "def _connect(self):\n         if mysql is None:\n             raise ImproperlyConfigured(\"MySQL driver not installed!\")\n         conn = mysql.connect(db=self.database, **self.connect_params)\n         return conn", "query": "connect to sql"}
{"id": "https://github.com/AirtestProject/Poco/blob/2c559a586adf3fd11ee81cabc446d4d3f6f2d119/poco/utils/simplerpc/transport/tcp/main.py#L34-L40", "method_name": "recv", "code": "def recv(self):\n         try:\n             msg_bytes = self.c.recv()\n         except socket.timeout:\n             msg_bytes = b\"\"\n         return self.prot.input(msg_bytes)", "query": "socket recv timeout"}
{"id": "https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/selectiontools.py#L659-L721", "method_name": "search_nodenames", "code": "def search_nodenames(self, *substrings: str, name: str = \"nodenames\") -> \\\n             \"Selection\":\n         try:\n             selection = Selection(name)\n             for node in self.nodes:\n                 for substring in substrings:\n                     if substring in node.name:\n                         selection.nodes += node\n                         break\n             return selection\n         except BaseException:\n             values = objecttools.enumeration(substrings)\n             objecttools.augment_excmessage(\n                 f\"While trying to determine the nodes of selection \"\n                 f\"`{self.name}` with names containing at least one \"\n                 f\"of the given substrings `{values}`\")", "query": "positions of substrings in string"}
{"id": "https://github.com/vecnet/vecnet.simulation/blob/3a4b3df7b12418c6fa8a7d9cd49656a1c031fc0e/vecnet/simulation/sim_status.py#L47-L54", "method_name": "get_description", "code": "def get_description(status_code):\n     description = _descriptions.get(status_code)\n     if description is None:\n         description = \"code = %s (no description)\" % str(status_code)\n     return description", "query": "get the description of a http status code"}
{"id": "https://github.com/SignalN/language/blob/5c50c78f65bcc2c999b44d530e7412185248352d/language/ngrams.py#L150-L151", "method_name": "word_similarity", "code": "def word_similarity(s1, s2, n=3):\n    return __similarity(s1, s2, word_ngrams, n=n)", "query": "string similarity levenshtein"}
{"id": "https://github.com/danidee10/Staticfy/blob/ebc555b00377394b0f714e4a173d37833fec90cb/staticfy/staticfy.py#L71-L89", "method_name": "get_elements", "code": "def get_elements(html_file, tags):\n     with open(html_file) as f:\n         document = BeautifulSoup(f, \"html.parser\")\n         def condition(tag, attr):\n             return lambda x: x.name == tag \\\n                 and not x.get(attr, \"http\").startswith((\"http\", \"//\"))\n         all_tags = [(attr, document.find_all(condition(tag, attr)))\n                     for tag, attr in tags]\n         return all_tags", "query": "reading element from html - <td>"}
{"id": "https://github.com/ladybug-tools/ladybug/blob/c08b7308077a48d5612f644943f92d5b5dade583/ladybug/euclid.py#L883-L955", "method_name": "__mul__", "code": "def __mul__(self, other):\n         if isinstance(other, Matrix4):\n             Aa = self.a\n             Ab = self.b\n             Ac = self.c\n             Ad = self.d\n             Ae = self.e\n             Af = self.f\n             Ag = self.g\n             Ah = self.h\n             Ai = self.i\n             Aj = self.j\n             Ak = self.k\n             Al = self.l\n             Am = self.m\n             An = self.n\n             Ao = self.o\n             Ap = self.p\n             Ba = other.a\n             Bb = other.b\n             Bc = other.c\n             Bd = other.d\n             Be = other.e\n             Bf = other.f\n             Bg = other.g\n             Bh = other.h\n             Bi = other.i\n             Bj = other.j\n             Bk = other.k\n             Bl = other.l\n             Bm = other.m\n             Bn = other.n\n             Bo = other.o\n             Bp = other.p\n             C = Matrix4()\n             C.a = Aa * Ba + Ab * Be + Ac * Bi + Ad * Bm\n             C.b = Aa * Bb + Ab * Bf + Ac * Bj + Ad * Bn\n             C.c = Aa * Bc + Ab * Bg + Ac * Bk + Ad * Bo\n             C.d = Aa * Bd + Ab * Bh + Ac * Bl + Ad * Bp\n             C.e = Ae * Ba + Af * Be + Ag * Bi + Ah * Bm\n             C.f = Ae * Bb + Af * Bf + Ag * Bj + Ah * Bn\n             C.g = Ae * Bc + Af * Bg + Ag * Bk + Ah * Bo\n             C.h = Ae * Bd + Af * Bh + Ag * Bl + Ah * Bp\n             C.i = Ai * Ba + Aj * Be + Ak * Bi + Al * Bm\n             C.j = Ai * Bb + Aj * Bf + Ak * Bj + Al * Bn\n             C.k = Ai * Bc + Aj * Bg + Ak * Bk + Al * Bo\n             C.l = Ai * Bd + Aj * Bh + Ak * Bl + Al * Bp\n             C.m = Am * Ba + An * Be + Ao * Bi + Ap * Bm\n             C.n = Am * Bb + An * Bf + Ao * Bj + Ap * Bn\n             C.o = Am * Bc + An * Bg + Ao * Bk + Ap * Bo\n             C.p = Am * Bd + An * Bh + Ao * Bl + Ap * Bp\n             return C\n         elif isinstance(other, Point3):\n             A = self\n             B = other\n             P = Point3(0, 0, 0)\n             P.x = A.a * B.x + A.b * B.y + A.c * B.z + A.d\n             P.y = A.e * B.x + A.f * B.y + A.g * B.z + A.h\n             P.z = A.i * B.x + A.j * B.y + A.k * B.z + A.l\n             return P\n         elif isinstance(other, Vector3):\n             A = self\n             B = other\n             V = Vector3(0, 0, 0)\n             V.x = A.a * B.x + A.b * B.y + A.c * B.z\n             V.y = A.e * B.x + A.f * B.y + A.g * B.z\n             V.z = A.i * B.x + A.j * B.y + A.k * B.z\n             return V\n         else:\n             other = other.copy()\n             other._apply_transform(self)\n             return other", "query": "matrix multiply"}
{"id": "https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/flows.py#L254-L266", "method_name": "set_workdir", "code": "def set_workdir(self, workdir, chroot=False):\n         if not chroot and hasattr(self, \"workdir\") and self.workdir != workdir:\n             raise ValueError(\"self.workdir != workdir: %s, %s\" % (self.workdir,  workdir))\n         self.workdir = os.path.abspath(workdir)\n         self.indir = Directory(os.path.join(self.workdir, \"indata\"))\n         self.outdir = Directory(os.path.join(self.workdir, \"outdata\"))\n         self.tmpdir = Directory(os.path.join(self.workdir, \"tmpdata\"))\n         self.wdir = Directory(self.workdir)", "query": "set working directory"}
{"id": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/trendmicroav.py#L144-L194", "method_name": "_ConvertToTimestamp", "code": "def _ConvertToTimestamp(self, date, time):\n     if len(date) != 8:\n       raise ValueError(\n           \"Unsupported length of date string: {0!s}\".format(repr(date)))\n     if len(time) < 3 or len(time) > 4:\n       raise ValueError(\n           \"Unsupported length of time string: {0!s}\".format(repr(time)))\n     try:\n       year = int(date[:4], 10)\n       month = int(date[4:6], 10)\n       day = int(date[6:8], 10)\n     except (TypeError, ValueError):\n       raise ValueError(\"Unable to parse date string: {0!s}\".format(repr(date)))\n     try:\n       hour = int(time[:-2], 10)\n       minutes = int(time[-2:], 10)\n     except (TypeError, ValueError):\n       raise ValueError(\"Unable to parse time string: {0!s}\".format(repr(date)))\n     time_elements_tuple = (year, month, day, hour, minutes, 0)\n     date_time = dfdatetime_time_elements.TimeElements(\n         time_elements_tuple=time_elements_tuple)\n     date_time.is_local_time = True\n     date_time._precision = dfdatetime_definitions.PRECISION_1_MINUTE  \n     return date_time", "query": "string to date"}
{"id": "https://github.com/Turbo87/utm/blob/efdd46ab0a341ce2aa45f8144d8b05a4fa0fd592/utm/conversion.py#L171-L246", "method_name": "from_latlon", "code": "def from_latlon(latitude, longitude, force_zone_number=None, force_zone_letter=None):\n     if not in_bounds(latitude, -80.0, 84.0):\n         raise OutOfRangeError(\"latitude out of range (must be between 80 deg S and 84 deg N)\")\n     if not in_bounds(longitude, -180.0, 180.0):\n         raise OutOfRangeError(\"longitude out of range (must be between 180 deg W and 180 deg E)\")\n     if force_zone_number is not None:\n         check_valid_zone(force_zone_number, force_zone_letter)\n     lat_rad = mathlib.radians(latitude)\n     lat_sin = mathlib.sin(lat_rad)\n     lat_cos = mathlib.cos(lat_rad)\n     lat_tan = lat_sin / lat_cos\n     lat_tan2 = lat_tan * lat_tan\n     lat_tan4 = lat_tan2 * lat_tan2\n     if force_zone_number is None:\n         zone_number = latlon_to_zone_number(latitude, longitude)\n     else:\n         zone_number = force_zone_number\n     if force_zone_letter is None:\n         zone_letter = latitude_to_zone_letter(latitude)\n     else:\n         zone_letter = force_zone_letter\n     lon_rad = mathlib.radians(longitude)\n     central_lon = zone_number_to_central_longitude(zone_number)\n     central_lon_rad = mathlib.radians(central_lon)\n     n = R / mathlib.sqrt(1 - E * lat_sin**2)\n     c = E_P2 * lat_cos**2\n     a = lat_cos * (lon_rad - central_lon_rad)\n     a2 = a * a\n     a3 = a2 * a\n     a4 = a3 * a\n     a5 = a4 * a\n     a6 = a5 * a\n     m = R * (M1 * lat_rad -\n              M2 * mathlib.sin(2 * lat_rad) +\n              M3 * mathlib.sin(4 * lat_rad) -\n              M4 * mathlib.sin(6 * lat_rad))\n     easting = K0 * n * (a +\n                         a3 / 6 * (1 - lat_tan2 + c) +\n                         a5 / 120 * (5 - 18 * lat_tan2 + lat_tan4 + 72 * c - 58 * E_P2)) + 500000\n     northing = K0 * (m + n * lat_tan * (a2 / 2 +\n                                         a4 / 24 * (5 - lat_tan2 + 9 * c + 4 * c**2) +\n                                         a6 / 720 * (61 - 58 * lat_tan2 + lat_tan4 + 600 * c - 330 * E_P2)))\n     if mixed_signs(latitude):\n         raise ValueError(\"latitudes must all have the same sign\")\n     elif negative(latitude):\n         northing += 10000000\n     return easting, northing, zone_number, zone_letter", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/lib/upload.py#L225-L246", "method_name": "_file_size", "code": "def _file_size(file_path, uncompressed=False):\n     _, ext = os.path.splitext(file_path)\n     if uncompressed:\n         if ext in {\".gz\", \".gzip\"}:\n             with gzip.GzipFile(file_path, mode=\"rb\") as fp:\n                 try:\n                     fp.seek(0, os.SEEK_END)\n                     return fp.tell()\n                 except ValueError:\n                     fp.seek(0)\n                     while len(fp.read(8192)) != 0:\n                         pass\n                     return fp.tell()\n         elif ext in {\".bz\", \".bz2\", \".bzip\", \".bzip2\"}:\n             with bz2.BZ2File(file_path, mode=\"rb\") as fp:\n                 fp.seek(0, os.SEEK_END)\n                 return fp.tell()\n     return os.path.getsize(file_path)", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/ml/algorithms/clustering.py#L307-L350", "method_name": "elbow_method", "code": "def elbow_method(data, k_min, k_max, distance=\"euclidean\"):\n         k_range = range(k_min, k_max)\n         k_means_var = [Clustering.kmeans(k).fit(data) for k in k_range]\n         centroids = [X.model.cluster_centers_ for X in k_means_var]\n         k_euclid = [cdist(data, cent, distance) for cent in centroids]\n         dist = [np.min(ke, axis=1) for ke in k_euclid]\n         wcss = [sum(d ** 2) for d in dist]\n         tss = sum(pdist(data) ** 2) / data.shape[0]\n         bss = tss - wcss\n         fig = plt.figure()\n         ax = fig.add_subplot(111)\n         ax.plot(k_range, bss / tss * 100, \"b*-\")\n         ax.set_ylim((0, 100))\n         plt.grid(True)\n         plt.xlabel(\"n_clusters\")\n         plt.ylabel(\"Percentage of variance explained\")\n         plt.title(\"Variance Explained vs. k\")\n         plt.show()", "query": "k means clustering"}
{"id": "https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/utils.py#L112-L121", "method_name": "pprint", "code": "def pprint(j, no_pretty):\n     if not no_pretty:\n         click.echo(\n             json.dumps(j, cls=PotionJSONEncoder, sort_keys=True, indent=4, separators=(\",\", \": \"))\n         )\n     else:\n         click.echo(j)", "query": "pretty print json"}
{"id": "https://github.com/MacHu-GWU/pymongo_mate-project/blob/be53170c2db54cb705b9e548d32ef26c773ff7f3/pymongo_mate/pkg/pandas_mate/sql_io.py#L55-L97", "method_name": "excel_to_sql", "code": "def excel_to_sql(excel_file_path, engine,\n                  read_excel_kwargs=None,\n                  to_generic_type_kwargs=None,\n                  to_sql_kwargs=None):\n     if read_excel_kwargs is None:\n         read_excel_kwargs = dict()\n     if to_sql_kwargs is None:\n         to_sql_kwargs = dict()\n     if to_generic_type_kwargs is None:\n         to_generic_type_kwargs = dict()\n     xl = pd.ExcelFile(excel_file_path)\n     for sheet_name in xl.sheet_names:\n         df = pd.read_excel(\n             excel_file_path, sheet_name,\n             **read_excel_kwargs.get(sheet_name, dict())\n         )\n         kwargs = to_generic_type_kwargs.get(sheet_name)\n         if kwargs:\n             data = to_dict_list_generic_type(df, **kwargs)\n             smart_insert(data, sheet_name, engine)\n         else:\n             df.to_sql(\n                 sheet_name, engine, index=False,\n                 **to_sql_kwargs.get(sheet_name, dict(if_exists=\"replace\"))\n             )", "query": "export to excel"}
{"id": "https://github.com/ladybug-tools/ladybug/blob/c08b7308077a48d5612f644943f92d5b5dade583/ladybug/euclid.py#L883-L955", "method_name": "__mul__", "code": "def __mul__(self, other):\n         if isinstance(other, Matrix4):\n             Aa = self.a\n             Ab = self.b\n             Ac = self.c\n             Ad = self.d\n             Ae = self.e\n             Af = self.f\n             Ag = self.g\n             Ah = self.h\n             Ai = self.i\n             Aj = self.j\n             Ak = self.k\n             Al = self.l\n             Am = self.m\n             An = self.n\n             Ao = self.o\n             Ap = self.p\n             Ba = other.a\n             Bb = other.b\n             Bc = other.c\n             Bd = other.d\n             Be = other.e\n             Bf = other.f\n             Bg = other.g\n             Bh = other.h\n             Bi = other.i\n             Bj = other.j\n             Bk = other.k\n             Bl = other.l\n             Bm = other.m\n             Bn = other.n\n             Bo = other.o\n             Bp = other.p\n             C = Matrix4()\n             C.a = Aa * Ba + Ab * Be + Ac * Bi + Ad * Bm\n             C.b = Aa * Bb + Ab * Bf + Ac * Bj + Ad * Bn\n             C.c = Aa * Bc + Ab * Bg + Ac * Bk + Ad * Bo\n             C.d = Aa * Bd + Ab * Bh + Ac * Bl + Ad * Bp\n             C.e = Ae * Ba + Af * Be + Ag * Bi + Ah * Bm\n             C.f = Ae * Bb + Af * Bf + Ag * Bj + Ah * Bn\n             C.g = Ae * Bc + Af * Bg + Ag * Bk + Ah * Bo\n             C.h = Ae * Bd + Af * Bh + Ag * Bl + Ah * Bp\n             C.i = Ai * Ba + Aj * Be + Ak * Bi + Al * Bm\n             C.j = Ai * Bb + Aj * Bf + Ak * Bj + Al * Bn\n             C.k = Ai * Bc + Aj * Bg + Ak * Bk + Al * Bo\n             C.l = Ai * Bd + Aj * Bh + Ak * Bl + Al * Bp\n             C.m = Am * Ba + An * Be + Ao * Bi + Ap * Bm\n             C.n = Am * Bb + An * Bf + Ao * Bj + Ap * Bn\n             C.o = Am * Bc + An * Bg + Ao * Bk + Ap * Bo\n             C.p = Am * Bd + An * Bh + Ao * Bl + Ap * Bp\n             return C\n         elif isinstance(other, Point3):\n             A = self\n             B = other\n             P = Point3(0, 0, 0)\n             P.x = A.a * B.x + A.b * B.y + A.c * B.z + A.d\n             P.y = A.e * B.x + A.f * B.y + A.g * B.z + A.h\n             P.z = A.i * B.x + A.j * B.y + A.k * B.z + A.l\n             return P\n         elif isinstance(other, Vector3):\n             A = self\n             B = other\n             V = Vector3(0, 0, 0)\n             V.x = A.a * B.x + A.b * B.y + A.c * B.z\n             V.y = A.e * B.x + A.f * B.y + A.g * B.z\n             V.z = A.i * B.x + A.j * B.y + A.k * B.z\n             return V\n         else:\n             other = other.copy()\n             other._apply_transform(self)\n             return other", "query": "matrix multiply"}
{"id": "https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/collection.py#L74-L79", "method_name": "extract_impression_from_file", "code": "def extract_impression_from_file(file_path):\n     with open(file_path, \"r\", encoding=\"utf8\") as f:\n         text = f.read()\n     return extract_impression_from_text(text)", "query": "extracting data from a text file"}
{"id": "https://github.com/Deathnerd/pyterp/blob/baf2957263685f03873f368226f5752da4e51f08/pyterp/__init__.py#L53-L65", "method_name": "_load_file", "code": "def _load_file(self, filename):\n         try:\n             read_file = os.path.realpath(os.path.join(os.curdir, filename))\n             with open(read_file, \"rb\") as file:\n                 temp_program = \"\"\n                 for line in file:\n                     temp_program += line.strip().replace(\" \", \"\").replace(\"\n\", \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\")\n         except IOError:  \n             print \"Cannot open file {}\".format(filename)\n             sys.exit(1)\n         return self._parse_program(temp_program)", "query": "replace in file"}
{"id": "https://github.com/APSL/transmanager/blob/79157085840008e146b264521681913090197ed1/transmanager/export.py#L58-L67", "method_name": "export_translations", "code": "def export_translations(tasks_ids):\n         qs = TransTask.objects.filter(pk__in=tasks_ids)\n         export = ExportQueryset(\n             qs,\n             TransTask,\n             (\"id\", \"object_name\", \"object_pk\", \"object_field_label\", \"object_field_value\", \"number_of_words\",\n              \"object_field_value_translation\", \"date_modification\", \"done\")\n         )\n         excel = export.get_excel()\n         return excel", "query": "export to excel"}
{"id": "https://github.com/ubc/github2gitlab/blob/795898f6d438621fa0c996a7156d70c382ff0493/github2gitlab/main.py#L434-L440", "method_name": "json_loads", "code": "def json_loads(payload):\n         \"Log the payload that cannot be parsed\"\n         try:\n             return json.loads(payload)\n         except ValueError as e:\n             log.error(\"unable to json.loads(\" + payload + \")\")\n             raise e", "query": "deserialize json"}
{"id": "https://github.com/gabfl/dbschema/blob/37722e6654e9f0374fac5518ebdca22f4c39f92f/src/schema_change.py#L81-L91", "method_name": "get_connection", "code": "def get_connection(engine, host, user, port, password, database, ssl={}):\n     if engine == \"mysql\":\n         return get_mysql_connection(host, user, port, password, database, ssl)\n     elif engine == \"postgresql\":\n         return get_pg_connection(host, user, port, password, database, ssl)\n     else:\n         raise RuntimeError(\"`%s` is not a valid engine.\" % engine)", "query": "postgresql connection"}
{"id": "https://github.com/nerdvegas/rez/blob/1d3b846d53b5b5404edfe8ddb9083f9ceec8c5e7/src/rezgui/windows/ContextSubWindow.py#L103-L108", "method_name": "copy_request_to_clipboard", "code": "def copy_request_to_clipboard(self):\n         txt = \" \".join(self.context_model.request)\n         clipboard = app.clipboard()\n         clipboard.setText(txt)\n         with app.status(\"Copied request to clipboard\"):\n             pass", "query": "copy to clipboard"}
{"id": "https://github.com/demosdemon/format-pipfile/blob/f95162c49d8fc13153080ddb11ac5a5dcd4d2e7c/ci/appveyor-download.py#L95-L102", "method_name": "unpack_zipfile", "code": "def unpack_zipfile(filename):\n     with open(filename, \"rb\") as fzip:\n         z = zipfile.ZipFile(fzip)\n         for name in z.namelist():\n             print((\"      extracting {}\".format(name)))\n             ensure_dirs(name)\n             z.extract(name)", "query": "extract zip file recursively"}
{"id": "https://github.com/bloomberg/bqplot/blob/8eb8b163abe9ee6306f6918067e2f36c1caef2ef/bqplot/pyplot.py#L816-L837", "method_name": "scatter", "code": "def scatter(x, y, **kwargs):\n     kwargs[\"x\"] = x\n     kwargs[\"y\"] = y\n     return _draw_mark(Scatter, **kwargs)", "query": "scatter plot"}
{"id": "https://github.com/fogleman/pg/blob/124ea3803c788b2c98c4f3a428e5d26842a67b58/pg/core.py#L93-L97", "method_name": "multiply", "code": "def multiply(self, matrix):\n         positions = [matrix * x for x in self.positions]\n         normals = list(self.normals)\n         uvs = list(self.uvs)\n         return Mesh(positions, normals, uvs)", "query": "matrix multiply"}
{"id": "https://github.com/TestInABox/stackInABox/blob/63ee457401e9a88d987f85f513eb512dcb12d984/stackinabox/util/requests_mock/core.py#L63-L75", "method_name": "get_reason_for_status", "code": "def get_reason_for_status(status_code):\n         if status_code in requests.status_codes.codes:\n             return requests.status_codes._codes[status_code][0].replace(\"_\",\n                                                                         \" \")\n         else:\n             return \"Unknown status code - {0}\".format(status_code)", "query": "get the description of a http status code"}
{"id": "https://github.com/vicalloy/lbutils/blob/66ae7e73bc939f073cdc1b91602a95e67caf4ba6/lbutils/xlsxutils.py#L20-L32", "method_name": "export_xlsx", "code": "def export_xlsx(wb, output, fn):\n     wb.close()\n     output.seek(0)\n     response = HttpResponse(output.read(), content_type=\"application/vnd.ms-excel\")\n     cd = codecs.encode(\"attachment;filename=%s\" % fn, \"utf-8\")\n     response[\"Content-Disposition\"] = cd\n     return response", "query": "export to excel"}
{"id": "https://github.com/rehive/rehive-python/blob/a7452a9cfecf76c5c8f0d443f122ed22167fb164/rehive/api/client.py#L50-L51", "method_name": "post", "code": "def post(self, path, data, json=True, **kwargs):\n        return self._request(\"post\", path, data, json=json, **kwargs)", "query": "httpclient post json"}
{"id": "https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/NDataHandler.py#L469-L481", "method_name": "read_properties", "code": "def read_properties(self):\n         with self.__lock:\n             absolute_file_path = self.__file_path\n             with open(absolute_file_path, \"rb\") as fp:\n                 local_files, dir_files, eocd = parse_zip(fp)\n                 properties = read_json(fp, local_files, dir_files, b\"metadata.json\")\n             return properties", "query": "read properties file"}
{"id": "https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hmtk/seismicity/smoothing/smoothed_seismicity.py#L491-L518", "method_name": "write_to_csv", "code": "def write_to_csv(self, filename):\n         fid = open(filename, \"wt\")\n         header_info = [\"Longitude\", \"Latitude\", \"Depth\", \"Observed Count\",\n                        \"Smoothed Rate\", \"b-value\"]\n         writer = csv.DictWriter(fid, fieldnames=header_info)\n         headers = dict((name0, name0) for name0 in header_info)\n         writer.writerow(headers)\n         for row in self.data:\n             if row[4] == 0:\n                 continue\n             row_dict = {\"Longitude\": \"%g\" % row[0],\n                         \"Latitude\": \"%g\" % row[1],\n                         \"Depth\": \"%g\" % row[2],\n                         \"Observed Count\": \"%d\" % row[3],\n                         \"Smoothed Rate\": \"%.6g\" % row[4],\n                         \"b-value\": \"%g\" % self.bval}\n             writer.writerow(row_dict)\n         fid.close()", "query": "write csv"}
{"id": "https://github.com/wdbm/shijian/blob/ad6aea877e1eb99fe148127ea185f39f1413ed4f/shijian.py#L990-L995", "method_name": "unique_list_elements", "code": "def unique_list_elements(x):\n     unique_elements = []\n     for element in x:\n         if element not in unique_elements:\n             unique_elements.append(element)\n     return unique_elements", "query": "unique elements"}
{"id": "https://github.com/fdb/aufmachen/blob/f2986a0cf087ac53969f82b84d872e3f1c6986f4/aufmachen/websites/immoweb.py#L115-L130", "method_name": "find_number", "code": "def find_number(regex, s):\n     result = find_string(regex, s)\n     if result is None:\n         return None\n     return int(result)", "query": "find int in string"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L578-L591", "method_name": "get_table", "code": "def get_table(self, table_name, db=\"default\"):\n         if db == \"default\" and \".\" in table_name:\n             db, table_name = table_name.split(\".\")[:2]\n         with self.metastore as client:\n             return client.get_table(dbname=db, tbl_name=table_name)", "query": "get database table name"}
{"id": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/functions.py#L1634-L1643", "method_name": "levenshtein", "code": "def levenshtein(left, right):\n     sc = SparkContext._active_spark_context\n     jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))\n     return Column(jc)", "query": "string similarity levenshtein"}
{"id": "https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/__init__.py#L207-L223", "method_name": "random_choices", "code": "def random_choices(self, elements=(\"a\", \"b\", \"c\"), length=None):\n         return self.random_elements(elements, length, unique=False)", "query": "unique elements"}
{"id": "https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/MainWindow.py#L123-L128", "method_name": "setLabel", "code": "def setLabel(self, label=None):\n         if label is None:\n             self.label.hide()\n         else:\n             self.label.setText(label)\n             self.label.show()", "query": "underline text in label widget"}
{"id": "https://github.com/JIC-CSB/jicbioimage.transform/blob/494c282d964c3a9b54c2a1b3730f5625ea2a494b/appveyor/install.py#L19-L23", "method_name": "unzip_file", "code": "def unzip_file(zip_fname):\n     print(\"Unzipping {}\".format(zip_fname))\n     with zipfile.ZipFile(zip_fname) as zf:\n         zf.extractall()", "query": "unzipping large files"}
{"id": "https://github.com/letuananh/chirptext/blob/ce60b47257b272a587c8703ea1f86cd1a45553a7/chirptext/leutile.py#L178-L184", "method_name": "group_by_count", "code": "def group_by_count(self):\n         d = OrderedDict()\n         for item, count in self.most_common():\n             if count not in d:\n                 d[count] = []\n             d[count].append(item)\n         return d.items()", "query": "group by count"}
{"id": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/file_utils.py#L138-L170", "method_name": "csv_to_json", "code": "def csv_to_json(csv_filepath, json_filepath, fieldnames, ignore_first_line=True):\n     import csv\n     import json\n     csvfile = open(csv_filepath, \"r\")\n     jsonfile = open(json_filepath, \"w\")\n     reader = csv.DictReader(csvfile, fieldnames)\n     rows = []\n     if ignore_first_line:\n         next(reader)\n     for row in reader:\n         rows.append(row)\n     json.dump(rows, jsonfile)\n     jsonfile.close()\n     csvfile.close()", "query": "convert json to csv"}
{"id": "https://github.com/alejandroesquiva/AutomaticApiRest-PythonConnector/blob/90e55d5cf953dc8d7186fc6df82a6c6a089a3bbe/build/lib/aarpy/AARConnector.py#L36-L38", "method_name": "printJson", "code": "def printJson(self):\n        jsonobject = self.getJson()\n        print(json.dumps(jsonobject, indent=1, sort_keys=True))", "query": "pretty print json"}
{"id": "https://github.com/tintinweb/pyetherchain/blob/cfeee3944b84fd12842ec3031d1d08ec7d63d33c/pyetherchain/pyetherchain.py#L185-L193", "method_name": "get_hardforks", "code": "def get_hardforks(self):\n         rows = self._parse_tbodies(self.session.get(\"/hardForks\").text)[0]  \n         result = []\n         for col in rows:\n             result.append({\"name\": self._extract_text_from_html( col[0]),\n                       \"on_roadmap\": True if \"yes\" in col[1].lower() else False,\n                       \"date\": self._extract_text_from_html(col[2]),\n                       \"block\": int(self._extract_text_from_html(col[3]))})\n         return result", "query": "extract data from html content"}
{"id": "https://github.com/ewdurbin/community/blob/f6b80e215a88508e3b07c2f4996ede6edea2c8e3/community/app.py#L36-L42", "method_name": "compute_status", "code": "def compute_status():\n     status_codes = []\n     for name, health_url in DEPENDENCIES.items():\n         status_codes.append(get_status(health_url)[0])\n     if max(status_codes) == 500:\n         return (\"UNHEALTHY\", max(status_codes))\n     return (\"HEALTHY\", max(status_codes))", "query": "get the description of a http status code"}
{"id": "https://github.com/internetarchive/warc/blob/8f05a000a23bbd6501217e37cfd862ffdf19da7f/warc/warc.py#L260-L263", "method_name": "reader", "code": "def reader(self):\n         if self._reader is None:\n             self._reader = WARCReader(self.fileobj)\n         return self._reader", "query": "buffered file reader read text"}
{"id": "https://github.com/rainwoodman/sharedmem/blob/b23e59c1ed0e28f7b6c96c17a04d55c700e06e3a/sharedmem/sharedmem.py#L785-L791", "method_name": "empty_like", "code": "def empty_like(array, dtype=None):\n     array = numpy.asarray(array)\n     if dtype is None: \n         dtype = array.dtype\n     return anonymousmemmap(array.shape, dtype)", "query": "empty array"}
{"id": "https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/traces/filter.py#L28-L34", "method_name": "apply_filter", "code": "def apply_filter(x, filter=None, axis=0):\n     x = _as_array(x)\n     if x.shape[axis] == 0:\n         return x\n     b, a = filter\n     return signal.filtfilt(b, a, x, axis=axis)", "query": "filter array"}
{"id": "https://github.com/lisael/fastidious/blob/2542db9de779ddabc3a64e9eb19a4e2de99741dc/fastidious/expressions.py#L287-L301", "method_name": "memoize", "code": "def memoize(self, code):\n         pk = hash(self.as_grammar())\n         return.format(\n             pk,\n             self._indent(code, 1),\n             self.id,\n             repr(self.rulename)\n         )", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L705-L720", "method_name": "multiply", "code": "def multiply(self, matrix):\n         if not isinstance(matrix, DenseMatrix):\n             raise ValueError(\"Only multiplication with DenseMatrix \"\n                              \"is supported.\")\n         return IndexedRowMatrix(self._java_matrix_wrapper.call(\"multiply\", matrix))", "query": "matrix multiply"}
{"id": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/helpers/utils.py#L43-L65", "method_name": "extract_zipdir", "code": "def extract_zipdir(zip_file):\n     if not os.path.exists(zip_file):\n         raise ValueError(\"{} does not exist\".format(zip_file))\n     directory = os.path.dirname(zip_file)\n     filename = os.path.basename(zip_file)\n     dirpath = os.path.join(directory, filename.replace(\".zip\", \"\"))\n     with zipfile.ZipFile(zip_file, \"r\", zipfile.ZIP_DEFLATED) as zipf:\n         zipf.extractall(dirpath)\n     return dirpath", "query": "extract zip file recursively"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/noaa_lpd.py#L559-L575", "method_name": "__camel_case", "code": "def __camel_case(word):\n         word = word.lower()\n         if \"_\" in word:\n             split_word = word.split(\"_\")\n         else:\n             split_word = word.split()\n         if len(split_word) > 0:\n             for i, word in enumerate(split_word):\n                 if i > 0:\n                     split_word[i] = word.title()\n         strings = \"\".join(split_word)\n         return strings", "query": "determine a string is a valid word"}
{"id": "https://github.com/LandRegistry/lr-utils/blob/811c9e5c11678a04ee203fa55a7c75080f4f9d89/lrutils/templatefilters/template_filters.py#L10-L12", "method_name": "dateformat", "code": "def dateformat(value, format=\"%-d %B %Y\"):\n    new_date = parser.parse(value, dayfirst=True)\n    return new_date.strftime(format)", "query": "format date"}
{"id": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L4548-L4555", "method_name": "_readonly", "code": "def _readonly(self, inplace=False):\n         df = self if inplace else self.copy()\n         for key, ar in self.columns.items():\n             if isinstance(ar, np.ndarray):\n                 df.columns[key] = ar = ar.view() \n                 ar.flags[\"WRITEABLE\"] = False\n         return df", "query": "readonly array"}
{"id": "https://github.com/zenwalker/python-xmltag/blob/5ba900753d939b0f3811c88b0f95ebbbdecd1727/xmltag/nodes.py#L45-L59", "method_name": "render", "code": "def render(self):\n         indent = self.doc.indent\n         inner = self.content or \"\"\n         if not self.safe:\n             inner = escape(inner, quote=False)\n         inner += \"\".join([n.render() for n in self.child_nodes])\n         html = self.doc.render_tag(self.tag_name, inner, self.attrs)\n         if indent:\n             pretty_html = \"\n\" + (indent * self.level) + html\n             if self._is_last():\n                 pretty_html += \"\n\" + indent * (self.level - 1)\n             html = pretty_html\n         return html", "query": "get inner html"}
{"id": "https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/controllers/android_device_lib/jsonrpc_shell_base.py#L44-L69", "method_name": "load_device", "code": "def load_device(self, serial=None):\n         serials = android_device.list_adb_devices()\n         if not serials:\n             raise Error(\"No adb device found!\")\n         if not serial:\n             env_serial = os.environ.get(\"ANDROID_SERIAL\", None)\n             if env_serial is not None:\n                 serial = env_serial\n             elif len(serials) == 1:\n                 serial = serials[0]\n             else:\n                 raise Error(\n                     \"Expected one phone, but %d found. Use the -s flag or \"\n                     \"specify ANDROID_SERIAL.\" % len(serials))\n         if serial not in serials:\n             raise Error(\"Device \"%s\" is not found by adb.\" % serial)\n         ads = android_device.get_instances([serial])\n         assert len(ads) == 1\n         self._ad = ads[0]", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/atztogo/phonopy/blob/869cc2ba9e7d495d5f4cf6942415ab3fc9e2a10f/phonopy/qha/eos.py#L111-L140", "method_name": "fit", "code": "def fit(self, initial_parameters):\n         import sys\n         import logging\n         import warnings\n         try:\n             from scipy.optimize import leastsq\n             import scipy\n         except ImportError:\n             print(\"You need to install python-scipy.\")\n             sys.exit(1)\n         warnings.filterwarnings(\"error\")\n         def residuals(p, eos, v, e):\n             return eos(v, *p) - e\n         try:\n             result = leastsq(residuals,\n                              initial_parameters,\n                              args=(self._eos, self._volume, self._energy),\n                              full_output=1)\n         except RuntimeError:\n             logging.exception(\"Fitting to EOS failed.\")\n             raise\n         except (RuntimeWarning, scipy.optimize.optimize.OptimizeWarning):\n             logging.exception(\"Difficulty in fitting to EOS.\")\n             raise\n         else:\n             self.parameters = result[0]", "query": "nelder mead optimize"}
{"id": "https://github.com/docker/docker-py/blob/613d6aad83acc9931ff2ecfd6a6c7bd8061dc125/docker/api/client.py#L275-L289", "method_name": "_post_json", "code": "def _post_json(self, url, data, **kwargs):\n         data2 = {}\n         if data is not None and isinstance(data, dict):\n             for k, v in six.iteritems(data):\n                 if v is not None:\n                     data2[k] = v\n         elif data is not None:\n             data2 = data\n         if \"headers\" not in kwargs:\n             kwargs[\"headers\"] = {}\n         kwargs[\"headers\"][\"Content-Type\"] = \"application/json\"\n         return self._post(url, data=json.dumps(data2), **kwargs)", "query": "httpclient post json"}
{"id": "https://github.com/thespacedoctor/sherlock/blob/2c80fb6fa31b04e7820e6928e3d437a21e692dd3/sherlock/imports/ned_d.py#L177-L274", "method_name": "_create_dictionary_of_ned_d", "code": "def _create_dictionary_of_ned_d(\n             self):\n         self.log.debug(\n             \"starting the ``_create_dictionary_of_ned_d`` method\")\n         count = 0\n         with open(self.pathToDataFile, \"rb\") as csvFile:\n             csvReader = csv.reader(\n                 csvFile, dialect=\"excel\", delimiter=\",\", quotechar=)\n             theseKeys = []\n             dictList = []\n             for row in csvReader:\n                 if len(theseKeys) == 0:\n                     totalRows -= 1\n                 if \"Exclusion Code\" in row and \"Hubble const.\" in row:\n                     for i in row:\n                         if i == \"redshift (z)\":\n                             theseKeys.append(\"redshift\")\n                         elif i == \"Hubble const.\":\n                             theseKeys.append(\"hubble_const\")\n                         elif i == \"G\":\n                             theseKeys.append(\"galaxy_index_id\")\n                         elif i == \"err\":\n                             theseKeys.append(\"dist_mod_err\")\n                         elif i == \"D (Mpc)\":\n                             theseKeys.append(\"dist_mpc\")\n                         elif i == \"Date (Yr. - 1980)\":\n                             theseKeys.append(\"ref_date\")\n                         elif i == \"REFCODE\":\n                             theseKeys.append(\"ref\")\n                         elif i == \"Exclusion Code\":\n                             theseKeys.append(\"dist_in_ned_flag\")\n                         elif i == \"Adopted LMC modulus\":\n                             theseKeys.append(\"lmc_mod\")\n                         elif i == \"m-M\":\n                             theseKeys.append(\"dist_mod\")\n                         elif i == \"Notes\":\n                             theseKeys.append(\"notes\")\n                         elif i == \"SN ID\":\n                             theseKeys.append(\"dist_derived_from_sn\")\n                         elif i == \"method\":\n                             theseKeys.append(\"dist_method\")\n                         elif i == \"Galaxy ID\":\n                             theseKeys.append(\"primary_ned_id\")\n                         elif i == \"D\":\n                             theseKeys.append(\"dist_index_id\")\n                         else:\n                             theseKeys.append(i)\n                     continue\n                 if len(theseKeys):\n                     count += 1\n                     if count > 1:\n                         sys.stdout.write(\"\\x1b[1A\\x1b[2K\")\n                     if count > totalCount:\n                         count = totalCount\n                     percent = (float(count) / float(totalCount)) * 100.\n                     print \"%(count)s / %(totalCount)s (%(percent)1.1f%%) rows added to memory\" % locals()\n                     rowDict = {}\n                     for t, r in zip(theseKeys, row):\n                         rowDict[t] = r\n                         if t == \"ref_date\":\n                             try:\n                                 rowDict[t] = int(r) + 1980\n                             except:\n                                 rowDict[t] = None\n                     if rowDict[\"dist_index_id\"] != \"999999\":\n                         dictList.append(rowDict)\n         csvFile.close()\n         self.log.debug(\n             \"completed the ``_create_dictionary_of_ned_d`` method\")\n         return dictList", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/GNS3/gns3-server/blob/a221678448fb5d24e977ef562f81d56aacc89ab1/gns3server/compute/dynamips/dynamips_hypervisor.py#L152-L162", "method_name": "set_working_dir", "code": "def set_working_dir(self, working_dir):\n         yield from self.send(\"hypervisor working_dir \"{}\"\".format(working_dir))\n         self._working_dir = working_dir\n         log.debug(\"Working directory set to {}\".format(self._working_dir))", "query": "set working directory"}
{"id": "https://github.com/alvations/lazyme/blob/961a8282198588ff72e15643f725ce895e51d06d/lazyme/string.py#L45-L53", "method_name": "deduplicate", "code": "def deduplicate(s, ch):\n     return ch.join([substring for substring in s.strip().split(ch) if substring])", "query": "positions of substrings in string"}
{"id": "https://github.com/amaas-fintech/amaas-core-sdk-python/blob/347b71f8e776b2dde582b015e31b4802d91e8040/amaascore/core/amaas_model.py#L30-L31", "method_name": "to_json_string", "code": "def to_json_string(dict_to_convert):\n    return json.dumps(dict_to_convert, ensure_ascii=False, default=json_handler, indent=4, separators=(\",\", \": \"))", "query": "json to xml conversion"}
{"id": "https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/jx_python/containers/doc_store.py#L144-L174", "method_name": "_sort", "code": "def _sort(self, short_list, sorts):\n         sort_values = self._index_columns(sorts)\n         output = []\n         def _sort_more(short_list, i, sorts):\n             if len(sorts) == 0:\n                 output.extend(short_list)\n             sort = sorts[0]\n             index = self._index[sort_values[i]]\n             if sort.sort == 1:\n                 sorted_keys = sorted(index.keys())\n             elif sort.sort == -1:\n                 sorted_keys = reversed(sorted(index.keys()))\n             else:\n                 sorted_keys = list(index.keys())\n             for k in sorted_keys:\n                 self._sort(index[k] & short_list, i + 1, sorts[1:])\n         _sort_more(short_list, 0, sorts)\n         return output", "query": "sort string list"}
{"id": "https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/widgets/image_cleaner.py#L209-L218", "method_name": "write_csv", "code": "def write_csv(self):\n         csv_path = self._path/\"cleaned.csv\"\n         with open(csv_path, \"w\") as f:\n             csv_writer = csv.writer(f)\n             csv_writer.writerow([\"name\",\"label\"])\n             for pair in self._csv_dict.items():\n                 pair = [os.path.relpath(pair[0], self._path), pair[1]]\n                 csv_writer.writerow(pair)\n         return csv_path", "query": "write csv"}
{"id": "https://github.com/remix/partridge/blob/0ba80fa30035e5e09fd8d7a7bdf1f28b93d53d03/partridge/gtfs.py#L89-L112", "method_name": "_read_csv", "code": "def _read_csv(self, filename: str) -> pd.DataFrame:\n         path = self._pathmap.get(filename)\n         columns = self._config.nodes.get(filename, {}).get(\"required_columns\", [])\n         if path is None or os.path.getsize(path) == 0:\n             return empty_df(columns)\n         with open(path, \"rb\") as f:\n             encoding = detect_encoding(f)\n         df = pd.read_csv(path, dtype=np.unicode, encoding=encoding, index_col=False)\n         df.rename(columns=lambda x: x.strip(), inplace=True)\n         if not df.empty:\n             for col in df.columns:\n                 df[col] = df[col].str.strip()\n         return df", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/dwillis/python-espncricinfo/blob/96469e39f309e28586fcec40cc6a20b2fddacff7/espncricinfo/player.py#L44-L50", "method_name": "get_html", "code": "def get_html(self):\n         r = requests.get(self.url)\n         if r.status_code == 404:\n             raise PlayerNotFoundError\n         else:\n             soup = BeautifulSoup(r.text, \"html.parser\")\n             return soup.find(\"div\", class_=\"pnl490M\")", "query": "get html of website"}
{"id": "https://github.com/shapiromatron/bmds/blob/395c6ce84ad82876fd9fa4a89a3497fb61616de0/bmds/models/base.py#L173-L177", "method_name": "get_exe_path", "code": "def get_exe_path(cls):\n         return os.path.abspath(os.path.join(ROOT, cls.bmds_version_dir, cls.exe + \".exe\"))", "query": "get executable path"}
{"id": "https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/seeyou/reader.py#L130-L143", "method_name": "decode_longitude", "code": "def decode_longitude(self, longitude):\n         match = RE_LONGITUDE.match(longitude)\n         if not match:\n             raise ParserError(\"Reading longitude failed\")\n         longitude = int(match.group(1)) + float(match.group(2)) / 60.\n         if not (0 <= longitude <= 180):\n             raise ParserError(\"Longitude out of bounds\")\n         if match.group(3).upper() == \"W\":\n             longitude = -longitude\n         return longitude", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/Riffstation/flask-philo/blob/76c9d562edb4a77010c8da6dfdb6489fa29cbc9e/flask_philo/db/postgresql/connection.py#L58-L102", "method_name": "initialize", "code": "def initialize(g, app):\n     if \"DATABASES\" in app.config and \"POSTGRESQL\" in app.config[\"DATABASES\"]:\n         for k, v in app.config[\"DATABASES\"][\"POSTGRESQL\"].items():\n             init_db_conn(k, v)\n         if \"test\" not in sys.argv:\n             @app.before_request\n             def before_request():\n                 from flask import _app_ctx_stack\n                 for k, v in app.config[\"DATABASES\"][\"POSTGRESQL\"].items():\n                     init_db_conn(k, v, scopefunc=_app_ctx_stack)\n                 g.postgresql_pool = pool\n             @app.teardown_request\n             def teardown_request(exception):\n                 pool = getattr(g, \"postgresql_pool\", None)\n                 if pool is not None:\n                     for k, v in pool.connections.items():\n                         v.session.remove()\n         else:\n             @app.before_request\n             def before_request():\n                 for k, v in app.config[\"DATABASES\"][\"POSTGRESQL\"].items():\n                     init_db_conn(k, v)\n                 g.postgresql_pool = pool", "query": "postgresql connection"}
{"id": "https://github.com/sods/paramz/blob/ae6fc6274b70fb723d91e48fc5026a9bc5a06508/paramz/optimization/scg.py#L44-L174", "method_name": "SCG", "code": "def SCG(f, gradf, x, optargs=(), maxiters=500, max_f_eval=np.inf, xtol=None, ftol=None, gtol=None):\n     if xtol is None:\n         xtol = 1e-6\n     if ftol is None:\n         ftol = 1e-6\n     if gtol is None:\n         gtol = 1e-5\n     sigma0 = 1.0e-7\n     fold = f(x, *optargs) \n     function_eval = 1\n     fnow = fold\n     gradnew = gradf(x, *optargs) \n     function_eval += 1\n     current_grad = np.dot(gradnew, gradnew)\n     gradold = gradnew.copy()\n     d = -gradnew \n     success = True \n     nsuccess = 0 \n     beta = 1.0 \n     betamin = 1.0e-15 \n     betamax = 1.0e15 \n     status = \"Not converged\"\n     flog = [fold]\n     iteration = 0\n     while iteration < maxiters:\n         if success:\n             mu = np.dot(d, gradnew)\n             if mu >= 0:  \n                 d = -gradnew\n                 mu = np.dot(d, gradnew)\n             kappa = np.dot(d, d)\n             sigma = sigma0 / np.sqrt(kappa)\n             xplus = x + sigma * d\n             gplus = gradf(xplus, *optargs)\n             function_eval += 1\n             theta = np.dot(d, (gplus - gradnew)) / sigma\n         delta = theta + beta * kappa\n         if delta <= 0: \n             delta = beta * kappa\n             beta = beta - theta / kappa\n         alpha = -mu / delta\n         xnew = x + alpha * d\n         fnew = f(xnew, *optargs)\n         function_eval += 1\n         Delta = 2.*(fnew - fold) / (alpha * mu)\n         if Delta >= 0.:\n             success = True\n             nsuccess += 1\n             x = xnew\n             fnow = fnew\n         else:\n             success = False\n             fnow = fold\n         flog.append(fnow) \n         iteration += 1\n         if success:\n             if (np.abs(fnew - fold) < ftol):\n                 status = \"converged - relative reduction in objective\"\n                 break\n             elif (np.max(np.abs(alpha * d)) < xtol):\n                 status = \"converged - relative stepsize\"\n                 break\n             else:\n                 gradold = gradnew\n                 gradnew = gradf(x, *optargs)\n                 function_eval += 1\n                 current_grad = np.dot(gradnew, gradnew)\n                 fold = fnew\n                 if current_grad <= gtol:\n                     status = \"converged - relative reduction in gradient\"\n                     break\n         if Delta < 0.25:\n             beta = min(4.0 * beta, betamax)\n         if Delta > 0.75:\n             beta = max(0.25 * beta, betamin)\n         if nsuccess == x.size:\n             d = -gradnew\n             beta = 1. \n             nsuccess = 0\n         elif success:\n             Gamma = np.dot(gradold - gradnew, gradnew) / (mu)\n             d = Gamma * d - gradnew\n     else:\n         status = \"maxiter exceeded\"\n     return x, flog, function_eval, status", "query": "nelder mead optimize"}
{"id": "https://github.com/andrewramsay/sk8-drivers/blob/67347a71762fb421f5ae65a595def5c7879e8b0c/pysk8/calibration/sk8_calibration_gui.py#L49-L57", "method_name": "update", "code": "def update(self):\n         elapsed = int(100 * ((time.time() - self.started_at) / self.GYRO_BIAS_TIME))\n         self.progressBar.setValue(elapsed)\n         self.samples.append(self.imu.gyro)\n         if time.time() - self.started_at > self.GYRO_BIAS_TIME:\n             self.accept()\n             QtWidgets.QMessageBox.information(self, \"Gyro calibration\", \"Calibration finished\")", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/StanfordBioinformatics/loom/blob/db2031a1a87124fee1aeb7414a668c03d774a698/worker/loomengine_worker/outputs.py#L71-L83", "method_name": "save", "code": "def save(self):\n         filename_list = self.output[\"source\"][\"filenames\"]\n         if not isinstance(filename_list, list):\n             filename_list = filename_list.split(\" \")\n         contents_list = []\n         for filename in filename_list:\n             file_path = os.path.join(\n                 self.working_dir, filename)\n             contents_list.append(self._read_file(file_path))\n         self.output.update({\"data\": {\"contents\": contents_list}})\n         self.connection.update_task_attempt_output(\n             self.output[\"uuid\"],\n             self.output)", "query": "save list to file"}
{"id": "https://github.com/shapiromatron/bmds/blob/395c6ce84ad82876fd9fa4a89a3497fb61616de0/bmds/models/base.py#L173-L177", "method_name": "get_exe_path", "code": "def get_exe_path(cls):\n         return os.path.abspath(os.path.join(ROOT, cls.bmds_version_dir, cls.exe + \".exe\"))", "query": "get executable path"}
{"id": "https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/osid/markers.py#L309-L318", "method_name": "is_effective", "code": "def is_effective(self):\n         now = DateTime.utcnow()\n         return self.get_start_date() <= now and self.get_end_date() >= now", "query": "get current date"}
{"id": "https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/models.py#L55-L57", "method_name": "get_tables", "code": "def get_tables(self, database_name):\n        database = self.get_database(database_name)\n        return [table for table_name, table in database.tables.items()]", "query": "get database table name"}
{"id": "https://github.com/hyde/fswrap/blob/41e4ad6f7e9ba73eabe61bd97847cd284e3edbd2/fswrap.py#L282-L289", "method_name": "read_all", "code": "def read_all(self, encoding=\"utf-8\"):\n         logger.info(\"Reading everything from %s\" % self)\n         with codecs.open(self.path, \"r\", encoding) as fin:\n             read_text = fin.read()\n         return read_text", "query": "buffered file reader read text"}
{"id": "https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/excel.py#L155-L157", "method_name": "save", "code": "def save(self):\n        pyexcel_export.save_data(self.excel_filename, data=self.excel_raw, retain_meta=True,\n                                 created=self.created, modified=datetime.now().isoformat())", "query": "export to excel"}
{"id": "https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xfilepathedit.py#L136-L142", "method_name": "copyFilepath", "code": "def copyFilepath( self ):\n         clipboard = QApplication.instance().clipboard()\n         clipboard.setText(self.filepath())\n         clipboard.setText(self.filepath(), clipboard.Selection)", "query": "copy to clipboard"}
{"id": "https://github.com/nerdvegas/rez/blob/1d3b846d53b5b5404edfe8ddb9083f9ceec8c5e7/src/rez/backport/zipfile.py#L808-L819", "method_name": "testzip", "code": "def testzip(self):\n         chunk_size = 2 ** 20\n         for zinfo in self.filelist:\n             try:\n                 f = self.open(zinfo.filename, \"r\")\n                 while f.read(chunk_size):     \n                     pass\n             except BadZipfile:\n                 return zinfo.filename", "query": "unzipping large files"}
{"id": "https://github.com/Netflix-Skunkworks/cloudaux/blob/c4b0870c3ac68b1c69e71d33cf78b6a8bdf437ea/cloudaux/orchestration/aws/s3.py#L160-L178", "method_name": "get_website", "code": "def get_website(bucket_name, **conn):\n     try:\n         result = get_bucket_website(Bucket=bucket_name, **conn)\n     except ClientError as e:\n         if \"NoSuchWebsiteConfiguration\" not in str(e):\n             raise e\n         return None\n     website = {}\n     if result.get(\"IndexDocument\"):\n         website[\"IndexDocument\"] = result[\"IndexDocument\"]\n     if result.get(\"RoutingRules\"):\n         website[\"RoutingRules\"] = result[\"RoutingRules\"]\n     if result.get(\"RedirectAllRequestsTo\"):\n         website[\"RedirectAllRequestsTo\"] = result[\"RedirectAllRequestsTo\"]\n     if result.get(\"ErrorDocument\"):\n         website[\"ErrorDocument\"] = result[\"ErrorDocument\"]\n     return website", "query": "get html of website"}
{"id": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/gzip_util.py#L74-L106", "method_name": "compress_file", "code": "def compress_file(fh_, compresslevel=9, chunk_size=1048576):\n     try:\n         bytes_read = int(chunk_size)\n         if bytes_read != chunk_size:\n             raise ValueError\n     except ValueError:\n         raise ValueError(\"chunk_size must be an integer\")\n     try:\n         while bytes_read == chunk_size:\n             buf = BytesIO()\n             with open_fileobj(buf, \"wb\", compresslevel) as ogz:\n                 try:\n                     bytes_read = ogz.write(fh_.read(chunk_size))\n                 except AttributeError:\n                     fh_ = salt.utils.files.fopen(fh_, \"rb\")\n                     bytes_read = ogz.write(fh_.read(chunk_size))\n             yield buf.getvalue()\n     finally:\n         try:\n             fh_.close()\n         except AttributeError:\n             pass", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/geometalab/pyGeoTile/blob/b1f44271698f5fc4d18c2add935797ed43254aa6/pygeotile/point.py#L12-L16", "method_name": "from_latitude_longitude", "code": "def from_latitude_longitude(cls, latitude=0.0, longitude=0.0):\n         assert -180.0 <= longitude <= 180.0, \"Longitude needs to be a value between -180.0 and 180.0.\"\n         assert -90.0 <= latitude <= 90.0, \"Latitude needs to be a value between -90.0 and 90.0.\"\n         return cls(latitude=latitude, longitude=longitude)", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/totalgood/nlpia/blob/efa01126275e9cd3c3a5151a644f1c798a9ec53f/src/nlpia/clean_alice.py#L59-L82", "method_name": "concatenate_aiml", "code": "def concatenate_aiml(path=\"aiml-en-us-foundation-alice.v1-9.zip\", outfile=\"aiml-en-us-foundation-alice.v1-9.aiml\"):\n     path = find_data_path(path) or path\n     zf = zipfile.ZipFile(path)\n     for name in zf.namelist():\n         if not name.lower().endswith(\".aiml\"):\n             continue\n         with zf.open(name) as fin:\n             happyending = \"\n             for i, line in enumerate(fin):\n                 try:\n                     line = line.decode(\"utf-8\").strip()\n                 except UnicodeDecodeError:\n                     line = line.decode(\"ISO-8859-1\").strip()\n                 if line.lower().startswith(\"</aiml>\") or line.lower().endswith(\"</aiml>\"):\n                     happyending = (i, line)\n                     break\n                 else:\n                     pass\n             if happyending != (i, line):\n                 print(\"Invalid AIML format: {}\nLast line (line number {}) was: {}\nexpected \"</aiml>\"\".format(\n                     name, i, line))", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/housecanary/hc-api-python/blob/2bb9e2208b34e8617575de45934357ee33b8531c/housecanary/excel/__init__.py#L18-L28", "method_name": "export_analytics_data_to_excel", "code": "def export_analytics_data_to_excel(data, output_file_name, result_info_key, identifier_keys):\n     workbook = create_excel_workbook(data, result_info_key, identifier_keys)\n     workbook.save(output_file_name)\n     print(\"Saved Excel file to {}\".format(output_file_name))", "query": "export to excel"}
{"id": "https://github.com/hit9/rux/blob/d7f60722658a3b83ac6d7bb3ca2790ac9c926b59/rux/parser.py#L33-L42", "method_name": "_code_no_lexer", "code": "def _code_no_lexer(self, text):\n         text = text.encode(charset).strip()\n         return(\n              % houdini.escape_html(text)\n         )", "query": "html encode string"}
{"id": "https://github.com/uw-it-aca/uw-restclients-iasystem/blob/f65f169d54b0d39e2d732cba529ccd8b6cb49f8a/uw_iasystem/evaluation.py#L192-L199", "method_name": "_datetime_from_string", "code": "def _datetime_from_string(date_string):\n     if date_string:\n         date_format = \"%Y-%m-%dT%H:%M:%S\"\n         date_string = date_string.replace(\"Z\", \"\")\n         date = datetime.strptime(date_string, date_format)\n         return pytz.utc.localize(date)\n     return \"\"", "query": "string to date"}
{"id": "https://github.com/twosigma/beakerx/blob/404de61ed627d9daaf6b77eb4859e7cb6f37413f/beakerx/beakerx/easyform/easyform.py#L112-L117", "method_name": "addCheckBox", "code": "def addCheckBox(self, *args, **kwargs):\n         checkbox = BeakerxCheckbox(description=self.getDescription(args, kwargs))\n         checkbox.value = getValue(kwargs, \"value\", False)\n         self.children += (checkbox,)\n         self.components[checkbox.description] = checkbox\n         return checkbox", "query": "make the checkbox checked"}
{"id": "https://github.com/ml4ai/delphi/blob/6d03d8aafeab99610387c51b89c99738ff2abbe3/delphi/translators/for2py/format.py#L99-L138", "method_name": "read_line", "code": "def read_line(self, line):\n         if not self._read_line_init:\n             self.init_read_line()\n         match = self._re.match(line)\n         assert match is not None, f\"Format mismatch (line = {line})\"\n         matched_values = []\n         for i in range(self._re.groups):\n             cvt_re = self._match_exps[i]\n             cvt_div = self._divisors[i]\n             cvt_fn = self._in_cvt_fns[i]\n             match_str = match.group(i + 1)\n             match0 = re.match(cvt_re, match_str)\n             if match0 is not None:\n                 if cvt_fn == \"float\":\n                     if \".\" in match_str:\n                         val = float(match_str)\n                     else:\n                         val = int(match_str) / cvt_div\n                 elif cvt_fn == \"int\":\n                     val = int(match_str)\n                 else:\n                     sys.stderr.write(\n                         f\"Unrecognized conversion function: {cvt_fn}\n\"\n                     )\n             else:\n                 sys.stderr.write(\n                     f\"Format conversion failed: {match_str}\n\"\n                 )\n             matched_values.append(val)\n         return tuple(matched_values)", "query": "read text file line by line"}
{"id": "https://github.com/newville/wxmplot/blob/8e0dc037453e5cdf18c968dc5a3d29efd761edee/wxmplot/plotframe.py#L45-L47", "method_name": "scatterplot", "code": "def scatterplot(self, x, y, **kw):\n        self.panel.scatterplot(x, y, **kw)", "query": "scatter plot"}
{"id": "https://github.com/msfrank/cifparser/blob/ecd899ba2e7b990e2cec62b115742d830e7e4384/cifparser/converters.py#L23-L29", "method_name": "str_to_bool", "code": "def str_to_bool(s):\n     s = s.lower()\n     if s in (\"true\", \"yes\", \"1\"):\n         return True\n     if s in (\"false\", \"no\", \"0\"):\n         return False\n     raise ConversionError(\"failed to convert {0} to bool\".format(s))", "query": "convert int to bool"}
{"id": "https://github.com/alexhayes/django-toolkit/blob/b64106392fad596defc915b8235fe6e1d0013b5b/django_toolkit/views.py#L343-L353", "method_name": "get_date", "code": "def get_date(self):\n         year = self.get_year()\n         month = self.get_month()\n         day = self.get_day()\n         return _date_from_string(year, self.get_year_format(),\n                                  month, self.get_month_format(),\n                                  day, self.get_day_format())", "query": "get current date"}
{"id": "https://github.com/appknox/vendor/blob/0b85d3c8328c5102cdcb1f619cbff215d7b5f228/ak_vendor/report.py#L31-L34", "method_name": "to_html_file", "code": "def to_html_file(self, path=\"\"):\n         with open(\"{}/output.html\".format(path), \"w\") as file:\n             tpl = self.to_html()\n             file.write(tpl)", "query": "output to html file"}
{"id": "https://github.com/clach04/x10_any/blob/5b90a543b127ab9e6112fd547929b5ef4b8f0cbc/x10_any/cm17a.py#L106-L118", "method_name": "_sendBinaryData", "code": "def _sendBinaryData(port, data):\n     _reset(port)\n     time.sleep(leadInOutDelay)\n     for digit in data:\n         _sendBit(port, digit)\n     time.sleep(leadInOutDelay)", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/pedroburon/tbk/blob/ecd6741e0bae06269eb4ac885c3ffcb7902ee40e/tbk/webpay/encryption.py#L36-L42", "method_name": "encrypt_message", "code": "def encrypt_message(self, signed_message, message, key, iv):\n         raw = signed_message + message\n         block_size = AES.block_size\n         pad = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size).encode(\"utf-8\")\n         message_to_encrypt = pad(raw)\n         cipher = AES.new(key, AES.MODE_CBC, iv)\n         return cipher.encrypt(message_to_encrypt)", "query": "aes encryption"}
{"id": "https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/cookie.py#L299-L342", "method_name": "create_cookie", "code": "def create_cookie(self, value, typ, cookie_name=None, ttl=-1, kill=False):\n         if kill:\n             ttl = -1\n         elif ttl < 0:\n             ttl = self.default_value[\"max_age\"]\n         if cookie_name is None:\n             cookie_name = self.default_value[\"name\"]\n         c_args = {}\n         srvdomain = self.default_value[\"domain\"]\n         if srvdomain and srvdomain not in [\"localhost\", \"127.0.0.1\",\n                                            \"0.0.0.0\"]:\n             c_args[\"domain\"] = srvdomain\n         srvpath = self.default_value[\"path\"]\n         if srvpath:\n             c_args[\"path\"] = srvpath\n         timestamp = str(int(time.time()))\n         try:\n             cookie_payload = \"::\".join([value, timestamp, typ])\n         except TypeError:\n             cookie_payload = \"::\".join([value[0], timestamp, typ])\n         cookie = make_cookie(\n             cookie_name, cookie_payload, self.sign_key,\n             timestamp=timestamp, enc_key=self.enc_key, max_age=ttl,\n             sign_alg=self.sign_alg, **c_args)\n         return cookie", "query": "create cookie"}
{"id": "https://github.com/tanghaibao/jcvi/blob/d2e31a77b6ade7f41f3b321febc2b4744d1cdeca/jcvi/formats/fpc.py#L96-L111", "method_name": "main", "code": "def main(fpcfile):\n     fw = sys.stdout\n     f = FpcReader(fpcfile)\n     header = \"\\t\".join((\"bac_name\", \"ctg_name\", \"map_left\", \"map_right\",\n         \"bands\", \"probes\", \"remark\"))\n     print(header, file=fw)\n     recs = list(f)\n     logging.debug(\"%d records parsed\" % len(recs))\n     recs.sort(key=lambda x: (x.ctg_name, x.map_left, x.map_right))\n     for rec in recs:\n         print(rec, file=fw)", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/cdeboever3/cdpybio/blob/38efdf0e11d01bc00a135921cb91a19c03db5d5c/cdpybio/variants.py#L83-L110", "method_name": "vcf_as_df", "code": "def vcf_as_df(fn):\n     header_lines = 0\n     with open(fn, \"r\") as f:\n         line = f.readline().strip()\n         header_lines += 1\n         while line[0] == \"\n             line = f.readline().strip()\n             header_lines += 1\n     header_lines -= 2\n     df = pd.read_table(fn, skiprows=header_lines, header=0)\n     df.columns = [\"CHROM\"] + list(df.columns[1:])\n     return df", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_parsing.py#L22-L24", "method_name": "format_date_list", "code": "def format_date_list(dates):\n    format = dateformat.DateFormat(\"YYYY-MM-DD hh:mm:ss\")\n    return [format.format(date) for date in dates]", "query": "format date"}
{"id": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/heatmaps.py#L392-L406", "method_name": "to_uint8", "code": "def to_uint8(self):\n         arr_0to255 = np.clip(np.round(self.arr_0to1 * 255), 0, 255)\n         arr_uint8 = arr_0to255.astype(np.uint8)\n         return arr_uint8", "query": "converting uint8 array to image"}
{"id": "https://github.com/peri-source/peri/blob/61beed5deaaf978ab31ed716e8470d86ba639867/peri/util.py#L1055-L1114", "method_name": "memoize", "code": "def memoize(cache_max_size=1e9):\n     def memoize_inner(obj):\n         cache_name = str(obj)\n         @functools.wraps(obj)\n         def wrapper(self, *args, **kwargs):\n             if not hasattr(self, \"_memoize_caches\"):\n                 def clear_cache(self):\n                     for k,v in iteritems(self._memoize_caches):\n                         self._memoize_caches[k] = newcache()\n                 self._memoize_caches = {}\n                 self._memoize_clear = types.MethodType(clear_cache, self)\n             cache = self._memoize_caches.get(cache_name)\n             if not cache:\n                 cache = newcache()\n                 self._memoize_caches[cache_name] = cache\n             size = 0\n             hashed = []\n             for arg in args:\n                 if isinstance(arg, np.ndarray):\n                     hashed.append(arg.tostring())\n                 else:\n                     hashed.append(arg)\n             for k,v in iteritems(kwargs):\n                 if isinstance(v, np.ndarray):\n                     hashed.append(v.tostring())\n                 else:\n                     hashed.append(v)\n             hashed = tuple(hashed)\n             if hashed not in cache:\n                 ans = obj(self, *args, **kwargs)\n                 if isinstance(ans, np.ndarray):\n                     size = ans.nbytes\n                 newsize = size + cache[\"size\"]\n                 if newsize < cache_max_size:\n                     cache[hashed] = ans\n                     cache[\"misses\"] += 1\n                     cache[\"size\"] = newsize\n                 return ans\n             cache[\"hits\"] += 1\n             return cache[hashed]\n         return wrapper\n     return memoize_inner", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/wecatch/app-turbo/blob/75faf97371a9a138c53f92168d0a486636cb8a9c/turbo/session.py#L185-L193", "method_name": "_set_cookie", "code": "def _set_cookie(self, name, value):\n         cookie_domain = self._config.cookie_domain\n         cookie_path = self._config.cookie_path\n         cookie_expires = self._config.cookie_expires\n         if self._config.secure:\n             return self.handler.set_secure_cookie(\n                 name, value, expires_days=cookie_expires / (3600 * 24), domain=cookie_domain, path=cookie_path)\n         else:\n             return self.handler.set_cookie(name, value, expires=cookie_expires, domain=cookie_domain, path=cookie_path)", "query": "create cookie"}
{"id": "https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xfilepathedit.py#L136-L142", "method_name": "copyFilepath", "code": "def copyFilepath( self ):\n         clipboard = QApplication.instance().clipboard()\n         clipboard.setText(self.filepath())\n         clipboard.setText(self.filepath(), clipboard.Selection)", "query": "copy to clipboard"}
{"id": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/math/cross.py#L96-L102", "method_name": "permutations", "code": "def permutations(x):\n   if len(x) > 1:\n     for permutation in permutations(x[1:]):\n       for i in xrange(len(permutation)+1):\n         yield permutation[:i] + x[0:1] + permutation[i:]\n   else: yield x", "query": "all permutations of a list"}
{"id": "https://github.com/tjcsl/ion/blob/5d722b0725d572039bb0929fd5715a4070c82c72/intranet/utils/html.py#L20-L21", "method_name": "safe_html", "code": "def safe_html(txt):\n    return bleach.linkify(bleach.clean(txt, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES, styles=ALLOWED_STYLES))", "query": "set file attrib hidden"}
{"id": "https://github.com/danhper/python-i18n/blob/bbba4b7ec091997ea8df2067acd7af316ee00b31/i18n/loaders/json_loader.py#L10-L14", "method_name": "parse_file", "code": "def parse_file(self, file_content):\n         try:\n             return json.loads(file_content)\n         except ValueError as e:\n             raise I18nFileLoadError(\"invalid JSON: {0}\".format(e.strerror))", "query": "parse json file"}
{"id": "https://github.com/google/transitfeed/blob/eb2991a3747ba541b2cb66502b305b6304a1f85f/merge.py#L1166-L1206", "method_name": "DisjoinCalendars", "code": "def DisjoinCalendars(self, cutoff):\n     def TruncatePeriod(service_period, start, end):\n       service_period.start_date = max(service_period.start_date, start)\n       service_period.end_date = min(service_period.end_date, end)\n       dates_to_delete = []\n       for k in service_period.date_exceptions:\n         if (k < start) or (k > end):\n           dates_to_delete.append(k)\n       for k in dates_to_delete:\n         del service_period.date_exceptions[k]\n     year = int(cutoff[:4])\n     month = int(cutoff[4:6])\n     day = int(cutoff[6:8])\n     cutoff_date = datetime.date(year, month, day)\n     one_day_delta = datetime.timedelta(days=1)\n     before = (cutoff_date - one_day_delta).strftime(\"%Y%m%d\")\n     for a in self.feed_merger.a_schedule.GetServicePeriodList():\n       TruncatePeriod(a, 0, before)\n     for b in self.feed_merger.b_schedule.GetServicePeriodList():\n       TruncatePeriod(b, cutoff, \"9\"*8)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/JohnVinyard/zounds/blob/337b3f98753d09eaab1c72dcd37bb852a3fa5ac6/zounds/ui/baseapp.py#L82-L95", "method_name": "_get_html", "code": "def _get_html(self, html_filename):\n         path, fn = os.path.split(__file__)\n         with open(os.path.join(path, html_filename)) as f:\n             html = f.read()\n         to_replace = [(m.groupdict()[\"filename\"], html[m.start(): m.end()])\n                       for m in self.SCRIPT_TAG.finditer(html)]\n         for filename, tag in to_replace:\n             with open(os.path.join(path, filename)) as scriptfile:\n                 html = html.replace(\n                     tag, \"<script>{}</script>\".format(scriptfile.read()))\n         return html", "query": "get inner html"}
{"id": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/normal.py#L241-L261", "method_name": "_kl_normal_normal", "code": "def _kl_normal_normal(n_a, n_b, name=None):\n   with tf.name_scope(name or \"kl_normal_normal\"):\n     one = tf.constant(1, dtype=n_a.dtype)\n     two = tf.constant(2, dtype=n_a.dtype)\n     half = tf.constant(0.5, dtype=n_a.dtype)\n     s_a_squared = tf.square(n_a.scale)\n     s_b_squared = tf.square(n_b.scale)\n     ratio = s_a_squared / s_b_squared\n     return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *\n             (ratio - one - tf.math.log(ratio)))", "query": "normal distribution"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/smart_contract/native_contract/governance.py#L403-L414", "method_name": "to_json", "code": "def to_json(self):\n         map = dict()\n         map[\"peer_pubkey\"] = self.peer_pubkey\n         map[\"max_authorize\"] = self.max_authorize\n         map[\"old_peerCost\"] = self.old_peerCost\n         map[\"new_peer_cost\"] = self.new_peer_cost\n         map[\"set_cost_view\"] = self.set_cost_view\n         map[\"field1\"] = self.field1\n         map[\"field2\"] = self.field2\n         map[\"field3\"] = self.field3\n         map[\"field4\"] = self.field4\n         return map", "query": "map to json"}
{"id": "https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L24-L47", "method_name": "pick", "code": "def pick(self):\n         v = random.uniform(0, self.ub)\n         d = self.dist\n         c = self.vc - 1\n         s = self.vc\n         while True:\n             s = s / 2\n             if s == 0:\n                 break\n             if v <= d[c][1]:\n                 c -= s\n             else:\n                 c += s\n                 while len(d) <= c:\n                     s = s / 2\n                     c -= s\n                     if s == 0:\n                         break\n         if c == len(d) or v <= d[c][1]:\n             c -= 1\n         return d[c][0]", "query": "randomly pick a number"}
{"id": "https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/html_metadata_extractor.py#L40-L90", "method_name": "extract", "code": "def extract(self, html_text: str,\n                 extract_title: bool = False,\n                 extract_meta: bool = False,\n                 extract_microdata: bool = False,\n                 microdata_base_url: str = \"\",\n                 extract_json_ld: bool = False,\n                 extract_rdfa: bool = False,\n                 rdfa_base_url: str = \"\") \\\n             -> List[Extraction]:\n         res = list()\n         soup = BeautifulSoup(html_text, \"html.parser\")\n         if soup.title and extract_title:\n             title = self._wrap_data(\"title\", soup.title.string.encode(\"utf-8\").decode(\"utf-8\"))\n             res.append(title)\n         if soup.title and extract_meta:\n             meta_content = self._wrap_meta_content(soup.find_all(\"meta\"))\n             meta_data = self._wrap_data(\"meta\", meta_content)\n             res.append(meta_data)\n         if extract_microdata:\n             mde = MicrodataExtractor()\n             mde_data = self._wrap_data(\"microdata\", mde.extract(html_text, microdata_base_url))\n             res.append(mde_data)\n         if extract_json_ld:\n             jslde = JsonLdExtractor()\n             jslde_data = self._wrap_data(\"json-ld\", jslde.extract(html_text))\n             res.append(jslde_data)\n         if extract_rdfa:\n             rdfae = RDFaExtractor()\n             rdfae_data = self._wrap_data(\"rdfa\", rdfae.extract(html_text, rdfa_base_url))\n             res.append(rdfae_data)\n         return res", "query": "extract data from html content"}
{"id": "https://github.com/ubc/github2gitlab/blob/795898f6d438621fa0c996a7156d70c382ff0493/github2gitlab/main.py#L434-L440", "method_name": "json_loads", "code": "def json_loads(payload):\n         \"Log the payload that cannot be parsed\"\n         try:\n             return json.loads(payload)\n         except ValueError as e:\n             log.error(\"unable to json.loads(\" + payload + \")\")\n             raise e", "query": "deserialize json"}
{"id": "https://github.com/sony/nnabla/blob/aaf3d33b7cbb38f2a03aa754178ba8f7c8481320/python/benchmark/function/function_benchmark.py#L224-L234", "method_name": "_calc_benchmark_stat", "code": "def _calc_benchmark_stat(self, f):\n         timer = Timer()\n         i = 0\n         while True:\n             f()\n             i += 1\n             if i >= self.min_run:\n                 _, elapsed = timer.lap()\n                 if elapsed > self.min_time:\n                     break\n         return BenchmarkStat(elapsed / i, i)", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/devassistant/devassistant/blob/2dbfeaa666a64127263664d18969c55d19ecc83e/devassistant/gui/gui_helper.py#L466-L473", "method_name": "create_clipboard", "code": "def create_clipboard(self, text, selection=Gdk.SELECTION_CLIPBOARD):\n         clipboard = Gtk.Clipboard.get(selection)\n         clipboard.set_text(\"\n\".join(text), -1)\n         clipboard.store()\n         return clipboard", "query": "copy to clipboard"}
{"id": "https://github.com/steenzout/python-serialization-json/blob/583568e14cc02ba0bf711f56b8a0a3ad142c696d/steenzout/serialization/json/__init__.py#L50-L71", "method_name": "deserialize", "code": "def deserialize(json, cls=None):\n     LOGGER.debug(\"deserialize(%s)\", json)\n     out = simplejson.loads(json)\n     if isinstance(out, dict) and cls is not None:\n         return cls(**out)\n     return out", "query": "deserialize json"}
{"id": "https://github.com/explosion/spaCy/blob/8ee4100f8ffb336886208a1ea827bf4c745e2709/spacy/cli/profile.py#L55-L69", "method_name": "_read_inputs", "code": "def _read_inputs(loc, msg):\n     if loc == \"-\":\n         msg.info(\"Reading input from sys.stdin\")\n         file_ = sys.stdin\n         file_ = (line.encode(\"utf8\") for line in file_)\n     else:\n         input_path = Path(loc)\n         if not input_path.exists() or not input_path.is_file():\n             msg.fail(\"Not a valid input data file\", loc, exits=1)\n         msg.info(\"Using data from {}\".format(input_path.parts[-1]))\n         file_ = input_path.open()\n     for line in file_:\n         data = srsly.json_loads(line)\n         text = data[\"text\"]\n         yield text", "query": "extracting data from a text file"}
{"id": "https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/contrib/isotp.py#L612-L621", "method_name": "recv_with_timeout", "code": "def recv_with_timeout(self, timeout=1):\n         msg = self.ins.recv(timeout)\n         t = time.time()\n         if msg is None:\n             raise Scapy_Exception(\"Timeout\")\n         return self.basecls, msg, t", "query": "socket recv timeout"}
{"id": "https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/viz.py#L12-L44", "method_name": "plot_confusion_matrix", "code": "def plot_confusion_matrix(cm, classes,\n                           normalize=False,\n                           title=\"Confusion matrix\",\n                           cmap=plt.cm.Blues):\n     if normalize:\n         cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n         print(\"Normalized confusion matrix\")\n     else:\n         print(\"Confusion matrix, without normalization\")\n     print(cm)\n     plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n     plt.title(title)\n     plt.colorbar()\n     tick_marks = np.arange(len(classes))\n     plt.xticks(tick_marks, classes, rotation=45)\n     plt.yticks(tick_marks, classes)\n     fmt = \".2f\" if normalize else \"d\"\n     thresh = cm.max() / 2.\n     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n         plt.text(j, i, format(cm[i, j], fmt),\n                  horizontalalignment=\"center\",\n                  color=\"white\" if cm[i, j] > thresh else \"black\")\n     plt.tight_layout()\n     plt.ylabel(\"True label\")\n     plt.xlabel(\"Predicted label\")", "query": "confusion matrix"}
{"id": "https://github.com/mbj4668/pyang/blob/f2a5cc3142162e5b9ee4e18d154568d939ff63dd/pyang/plugins/check_update.py#L584-L598", "method_name": "chk_unique", "code": "def chk_unique(old, new, ctx):\n     if not hasattr(old, \"i_unique\") or not hasattr(new, \"i_unique\"):\n         return\n     oldunique = []\n     for (u, l) in old.i_unique:\n         oldunique.append((u, [s.arg for s in l]))\n     for (u, l) in new.i_unique:\n         o = util.keysearch([s.arg for s in l], 1, oldunique)\n         if o is not None:\n             oldunique.remove(o)\n         else:\n             err_def_added(u, ctx)", "query": "unique elements"}
{"id": "https://github.com/Unidata/MetPy/blob/16f68a94919b9a82dcf9cada2169cf039129e67b/metpy/calc/tools.py#L819-L870", "method_name": "lat_lon_grid_deltas", "code": "def lat_lon_grid_deltas(longitude, latitude, **kwargs):\n     r\n     from pyproj import Geod\n     if latitude.ndim != longitude.ndim:\n         raise ValueError(\"Latitude and longitude must have the same number of dimensions.\")\n     if latitude.ndim < 2:\n         longitude, latitude = np.meshgrid(longitude, latitude)\n     geod_args = {\"ellps\": \"sphere\"}\n     if kwargs:\n         geod_args = kwargs\n     g = Geod(**geod_args)\n     forward_az, _, dy = g.inv(longitude[..., :-1, :], latitude[..., :-1, :],\n                               longitude[..., 1:, :], latitude[..., 1:, :])\n     dy[(forward_az < -90.)   (forward_az > 90.)] *= -1\n     forward_az, _, dx = g.inv(longitude[..., :, :-1], latitude[..., :, :-1],\n                               longitude[..., :, 1:], latitude[..., :, 1:])\n     dx[(forward_az < 0.)   (forward_az > 180.)] *= -1\n     return dx * units.meter, dy * units.meter", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/Facade.py#L612-L615", "method_name": "create_check_box_widget", "code": "def create_check_box_widget(self, text=None):\n         check_box_widget = CheckBoxWidget(self.__ui)\n         check_box_widget.text = text\n         return check_box_widget", "query": "make the checkbox checked"}
{"id": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/audiofile.py#L330-L376", "method_name": "read_properties", "code": "def read_properties(self):\n         self.log(u\"Reading properties...\")\n         if not gf.file_can_be_read(self.file_path):\n             self.log_exc(u\"File \"%s\" cannot be read\" % (self.file_path), None, True, OSError)\n         self.log([u\"Getting file size for \"%s\"\", self.file_path])\n         self.file_size = gf.file_size(self.file_path)\n         self.log([u\"File size for \"%s\" is \"%d\"\", self.file_path, self.file_size])\n         try:\n             self.log(u\"Reading properties with FFPROBEWrapper...\")\n             properties = FFPROBEWrapper(\n                 rconf=self.rconf,\n                 logger=self.logger\n             ).read_properties(self.file_path)\n             self.log(u\"Reading properties with FFPROBEWrapper... done\")\n         except FFPROBEPathError:\n             self.log_exc(u\"Unable to call ffprobe executable\", None, True, AudioFileProbeError)\n         except (FFPROBEUnsupportedFormatError, FFPROBEParsingError):\n             self.log_exc(u\"Audio file format not supported by ffprobe\", None, True, AudioFileUnsupportedFormatError)\n         self.audio_length = TimeValue(properties[FFPROBEWrapper.STDOUT_DURATION])\n         self.audio_format = properties[FFPROBEWrapper.STDOUT_CODEC_NAME]\n         self.audio_sample_rate = gf.safe_int(properties[FFPROBEWrapper.STDOUT_SAMPLE_RATE])\n         self.audio_channels = gf.safe_int(properties[FFPROBEWrapper.STDOUT_CHANNELS])\n         self.log([u\"Stored audio_length: \"%s\"\", self.audio_length])\n         self.log([u\"Stored audio_format: \"%s\"\", self.audio_format])\n         self.log([u\"Stored audio_sample_rate: \"%s\"\", self.audio_sample_rate])\n         self.log([u\"Stored audio_channels: \"%s\"\", self.audio_channels])\n         self.log(u\"Reading properties... done\")", "query": "read properties file"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/preferences/configdialog.py#L515-L534", "method_name": "create_textedit", "code": "def create_textedit(self, text, option, default=NoDefault, \n                         tip=None, restart=False): \n         label = QLabel(text) \n         label.setWordWrap(True) \n         edit = QPlainTextEdit() \n         edit.setWordWrapMode(QTextOption.WordWrap) \n         layout = QVBoxLayout() \n         layout.addWidget(label) \n         layout.addWidget(edit) \n         layout.setContentsMargins(0, 0, 0, 0) \n         if tip: \n             edit.setToolTip(tip) \n         self.textedits[edit] = (option, default) \n         widget = QWidget(self) \n         widget.label = label \n         widget.textbox = edit \n         widget.setLayout(layout) \n         edit.restart_required = restart \n         edit.label_text = text \n         return widget", "query": "underline text in label widget"}
{"id": "https://github.com/joelfrederico/SciSalt/blob/7bf57c49c7dde0a8b0aa337fbd2fbd527ce7a67f/scisalt/facettools/print2elog.py#L13-L23", "method_name": "_copy_file", "code": "def _copy_file(filepath, fulltime):\n     if filepath is None:\n         filepath_out = \"\"\n     else:\n         filename  = _os.path.basename(filepath)\n         root, ext = _os.path.splitext(filename)\n         filepath_out = fulltime + ext\n         copypath = _os.path.join(basedir, filepath_out)\n         _shutil.copyfile(filepath, copypath)\n     return filepath_out", "query": "copying a file to a path"}
{"id": "https://github.com/thewca/wca-regulations-compiler/blob/3ebbd8fe8fec7c9167296f59b2677696fe61a954/wrc/wrc.py#L85-L113", "method_name": "html_to_pdf", "code": "def html_to_pdf(tmp_filenames, output_directory, lang_options):\n     input_html = output_directory + \"/\" + tmp_filenames[0]\n     wkthml_cmd = [\"wkhtmltopdf\"]\n     wkthml_cmd.extend([\"--margin-left\", \"18\"])\n     wkthml_cmd.extend([\"--margin-right\", \"18\"])\n     wkthml_cmd.extend([\"--page-size\", \"Letter\"])\n     header_file = pkg_resources.resource_filename(\"wrc\", \"data/header.html\")\n     footer_file = pkg_resources.resource_filename(\"wrc\", \"data/footer.html\")\n     wkthml_cmd.extend([\"--header-html\", header_file])\n     wkthml_cmd.extend([\"--footer-html\", footer_file])\n     wkthml_cmd.extend([\"--header-spacing\", \"8\"])\n     wkthml_cmd.extend([\"--footer-spacing\", \"8\"])\n     wkthml_cmd.append(input_html)\n     wkthml_cmd.append(output_directory + \"/\" + lang_options[\"pdf\"] + \".pdf\")\n     try:\n         check_call(wkthml_cmd)\n         print \"Successfully generated pdf file!\"\n         print \"Cleaning temporary file (%s)...\" % input_html\n         os.remove(input_html)\n     except CalledProcessError as err:\n         print \"Error while generating pdf:\"\n         print err\n         sys.exit(1)\n     except OSError as err:\n         print \"Error when running command \\\"\" + \" \".join(wkthml_cmd) + \"\\\"\"\n         print err\n         sys.exit(1)", "query": "convert html to pdf"}
{"id": "https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_to_csv.py#L28-L47", "method_name": "json_to_csv", "code": "def json_to_csv(json_input):\n     try:\n         json_input = json.loads(json_input)\n     except:\n         pass \n     headers = set()\n     for json_row in json_input:\n         headers.update(json_row.keys())\n     csv_io = StringIO.StringIO()\n     csv_out = csv.DictWriter(csv_io,headers)\n     csv_out.writeheader()\n     for json_row in json_input:\n         csv_out.writerow(json_row)\n     csv_io.seek(0)\n     return csv_io.read()", "query": "convert json to csv"}
{"id": "https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/algorithms.py#L115-L202", "method_name": "dominator_tree", "code": "def dominator_tree(graph):\n     todo = Queue()\n     done = set()\n     dominator = Tree(None)\n     nodes = list(graph.nodes)\n     while True:\n         if todo:\n             node = todo.pop()\n         elif nodes:\n             node = nodes.pop()\n             if len(nodes) % 1000 == 0:\n                 Log.note(\"{{num}} nodes remaining\", num=len(nodes))\n         else:\n             break\n         if node in done:\n             continue\n         parents = graph.get_parents(node) - {node}\n         if not parents:\n             done.add(node)\n             dominator.add_edge(Edge(ROOTS, node))\n             continue\n         not_done = parents - done\n         if not_done:\n             more_todo = not_done - todo\n             if not more_todo:\n                 done.add(node)\n                 dominator.add_edge(Edge(LOOPS, node))\n             else:\n                 todo.push(node)\n                 for p in more_todo:\n                     todo.push(p)\n             continue\n         if len(parents) == 1:\n             dominator.add_edge(Edge(list(parents)[0], node))\n             done.add(node)\n             continue\n         paths_from_roots = [\n             list(reversed(dominator.get_path_to_root(p)))\n             for p in parents\n         ]\n         if any(p[0] is ROOTS for p in paths_from_roots):\n             paths_from_roots = [p for p in paths_from_roots if p[0] is ROOTS]\n             if len(paths_from_roots) == 1:\n                 dom = paths_from_roots[0][-1]\n                 dominator.add_edge(Edge(dom, node))\n                 done.add(node)\n                 continue\n         num_paths = len(paths_from_roots)\n         for i, x in enumerate(zip_longest(*paths_from_roots)):\n             if x.count(x[0]) != num_paths:\n                 dom = paths_from_roots[0][i-1]\n                 if dom is LOOPS:\n                     dom = paths_from_roots[0][-1]\n                 break\n         else:\n             dom = paths_from_roots[0][-1]\n         dominator.add_edge(Edge(dom, node))\n         done.add(node)\n     return dominator", "query": "get all parents of xml node"}
{"id": "https://github.com/mjirik/io3d/blob/ccaf3e378dcc967f2565d477fc27583fd0f61fcc/io3d/datasets.py#L821-L833", "method_name": "unzip_recursive", "code": "def unzip_recursive(zip_file_name):\n     logger.debug(\"unzipping \" + zip_file_name)\n     fnlist = unzip_one(zip_file_name)\n     for fn in fnlist:\n         if zipfile.is_zipfile(fn):\n             local_fnlist = unzip_recursive(fn)\n             fnlist.extend(local_fnlist)\n     return fnlist", "query": "unzipping large files"}
{"id": "https://github.com/AustralianSynchrotron/lightflow/blob/dc53dbc1d961e20fb144273baca258060705c03e/lightflow/models/parameters.py#L62-L93", "method_name": "convert", "code": "def convert(self, value):\n         if self._type is str:\n             return str(value)\n         elif self._type is int:\n             try:\n                 return int(value)\n             except (UnicodeError, ValueError):\n                 raise WorkflowArgumentError(\"Cannot convert {} to int\".format(value))\n         elif self._type is float:\n             try:\n                 return float(value)\n             except (UnicodeError, ValueError):\n                 raise WorkflowArgumentError(\"Cannot convert {} to float\".format(value))\n         elif self._type is bool:\n             if isinstance(value, bool):\n                 return bool(value)\n             value = value.lower()\n             if value in (\"true\", \"1\", \"yes\", \"y\"):\n                 return True\n             elif value in (\"false\", \"0\", \"no\", \"n\"):\n                 return False\n             raise WorkflowArgumentError(\"Cannot convert {} to bool\".format(value))\n         else:\n             return value", "query": "convert int to bool"}
{"id": "https://github.com/infobip/infobip-api-python-client/blob/2da11962f877294c53bd545715e62d6ce5c139f0/infobip/util/http.py#L14-L16", "method_name": "deserialize", "code": "def deserialize(self, s, cls):\n        vals = json.JSONDecoder().decode(s)\n        return self.deserialize_map(vals, cls)", "query": "deserialize json"}
{"id": "https://github.com/linnarsson-lab/loompy/blob/62c8373a92b058753baa3a95331fb541f560f599/loompy/color.py#L288-L289", "method_name": "random_within", "code": "def random_within(self, r):\n  return self.random.randint(int(r[0]), int(r[1]))", "query": "randomly pick a number"}
{"id": "https://github.com/toomore/goristock/blob/e61f57f11a626cfbc4afbf66337fd9d1c51e3e71/grs/mobileapi.py#L38-L72", "method_name": "output", "code": "def output(self):\n     <table>\n             <tr><td>%(name)s</td><td>%(c)s</td><td>%(range)+.2f(%(pp)+.2f%%)</td></tr>\n             <tr><td>%(stock_no)s</td><td>%(value)s</td><td>%(time)s</td></tr></table>\n     if covstr(self.g[\"range\"]) > 0:\n       css = \"red\"\n     elif covstr(self.g[\"range\"]) < 0:\n       css = \"green\"\n     else:\n       css = \"gray\"\n     re = {\n       \"name\": self.g[\"name\"],\n       \"stock_no\": self.g[\"no\"],\n       \"time\": self.g[\"time\"],\n       \"open\": self.g[\"open\"],\n       \"h\": self.g[\"h\"],\n       \"l\": self.g[\"l\"],\n       \"c\": self.g[\"c\"],\n       \"max\": self.g[\"max\"],\n       \"min\": self.g[\"min\"],\n       \"range\": covstr(self.g[\"range\"]),\n       \"ranges\": self.g[\"ranges\"],\n       \"value\": self.g[\"value\"],\n       \"pvalue\": self.g[\"pvalue\"],\n       \"pp\": covstr(self.g[\"pp\"]),\n       \"top5buy\": self.g[\"top5buy\"],\n       \"top5sell\": self.g[\"top5sell\"],\n       \"crosspic\": self.g[\"crosspic\"],\n       \"css\": css\n     }\n     return re", "query": "reading element from html - <td>"}
{"id": "https://github.com/refnode/liquid/blob/8b2b5efc635b0dbfe610db9036fdb4ae3e3d5439/src/liquid/strscan.py#L516-L535", "method_name": "get_regex", "code": "def get_regex(regex):\n     if isinstance(regex, basestring):\n         return re.compile(regex)\n     elif not isinstance(regex, re._pattern_type):\n         raise TypeError(\"Invalid regex type: %r\" % (regex,))\n     return regex", "query": "regex case insensitive"}
{"id": "https://github.com/rcook/upload-haddocks/blob/a33826be1873da68ba073a42ec828c8ec150d576/setup.py#L16-L26", "method_name": "_read_properties", "code": "def _read_properties():\n     init_path = os.path.abspath(os.path.join(\"uploadhaddocks\", \"__init__.py\"))\n     regex = re.compile(\"^\\\\s*__(?P<key>.*)__\\\\s*=\\\\s*\\\"(?P<value>.*)\\\"\\\\s*$\")\n     with open(init_path, \"rt\") as f:\n         props = {}\n         for line in f.readlines():\n             m = regex.match(line)\n             if m is not None:\n                 props[m.group(\"key\")] = m.group(\"value\")\n     return props", "query": "read properties file"}
{"id": "https://github.com/ManiacalLabs/BiblioPixelAnimations/blob/fba81f6b94f5265272a53f462ef013df1ccdb426/BiblioPixelAnimations/strip/WhiteTwinkle.py#L68-L77", "method_name": "pick_led", "code": "def pick_led(self, inc):\n         idx = random.randrange(0, self.layout.numLEDs)\n         this_led = self.layout.get(idx)\n         r = this_led[0]\n         if random.randrange(0, self._maxLed) < self.density:\n             if r == 0:\n                 r += inc\n                 self.layout.set(idx, (2, 2, 2))", "query": "randomly pick a number"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/pdf.py#L232-L402", "method_name": "make_pdf_from_html", "code": "def make_pdf_from_html(\n         on_disk: bool,\n         html: str,\n         output_path: str = None,\n         header_html: str = None,\n         footer_html: str = None,\n         wkhtmltopdf_filename: str = _WKHTMLTOPDF_FILENAME,\n         wkhtmltopdf_options: Dict[str, Any] = None,\n         file_encoding: str = \"utf-8\",\n         debug_options: bool = False,\n         debug_content: bool = False,\n         debug_wkhtmltopdf_args: bool = True,\n         fix_pdfkit_encoding_bug: bool = None,\n         processor: str = _DEFAULT_PROCESSOR) -> Union[bytes, bool]:\n     wkhtmltopdf_options = wkhtmltopdf_options or {}  \n     assert_processor_available(processor)\n     if debug_content:\n         log.debug(\"html: {}\", html)\n         log.debug(\"header_html: {}\", header_html)\n         log.debug(\"footer_html: {}\", footer_html)\n     if fix_pdfkit_encoding_bug is None:\n         fix_pdfkit_encoding_bug = get_default_fix_pdfkit_encoding_bug()\n     if processor == Processors.XHTML2PDF:\n         if on_disk:\n             with open(output_path, mode=\"wb\") as outfile:\n                 xhtml2pdf.document.pisaDocument(html, outfile)\n             return True\n         else:\n             memfile = io.BytesIO()\n             xhtml2pdf.document.pisaDocument(html, memfile)\n             memfile.seek(0)\n             return memfile.read()\n     elif processor == Processors.WEASYPRINT:\n         if on_disk:\n             return weasyprint.HTML(string=html).write_pdf(output_path)\n         else:\n             return weasyprint.HTML(string=html).write_pdf()\n     elif processor == Processors.PDFKIT:\n         if not wkhtmltopdf_filename:\n             config = None\n         else:\n             if fix_pdfkit_encoding_bug:  \n                 log.debug(\"Attempting to fix bug in pdfkit (e.g. version 0.5.0)\"\n                           \" by encoding wkhtmltopdf_filename to UTF-8\")\n                 config = pdfkit.configuration(\n                     wkhtmltopdf=wkhtmltopdf_filename.encode(\"utf-8\"))\n             else:\n                 config = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_filename)\n         h_filename = None\n         f_filename = None\n         try:\n             if header_html:\n                 h_fd, h_filename = tempfile.mkstemp(suffix=\".html\")\n                 os.write(h_fd, header_html.encode(file_encoding))\n                 os.close(h_fd)\n                 wkhtmltopdf_options[\"header-html\"] = h_filename\n             if footer_html:\n                 f_fd, f_filename = tempfile.mkstemp(suffix=\".html\")\n                 os.write(f_fd, footer_html.encode(file_encoding))\n                 os.close(f_fd)\n                 wkhtmltopdf_options[\"footer-html\"] = f_filename\n             if debug_options:\n                 log.debug(\"wkhtmltopdf config: {!r}\", config)\n                 log.debug(\"wkhtmltopdf_options: {}\",\n                           pformat(wkhtmltopdf_options))\n             kit = pdfkit.pdfkit.PDFKit(html, \"string\", configuration=config,\n                                        options=wkhtmltopdf_options)\n             if on_disk:\n                 path = output_path\n             else:\n                 path = None\n             if debug_wkhtmltopdf_args:\n                 log.debug(\"Probable current user: {!r}\", getpass.getuser())\n                 log.debug(\"wkhtmltopdf arguments will be: {!r}\",\n                           kit.command(path=path))\n             return kit.to_pdf(path=path)\n         finally:\n             if h_filename:\n                 os.remove(h_filename)\n             if f_filename:\n                 os.remove(f_filename)\n     else:\n         raise AssertionError(\"Unknown PDF engine\")", "query": "convert html to pdf"}
{"id": "https://github.com/neuropsychology/NeuroKit.py/blob/c9589348fbbde0fa7e986048c48f38e6b488adfe/examples/UnderDev/eeg/eeg_microstates.py#L201-L253", "method_name": "eeg_microstates_clustering", "code": "def eeg_microstates_clustering(data, n_microstates=4, clustering_method=\"kmeans\", n_jobs=1, n_init=25, occurence_rejection_treshold=0.05, max_refitting=5, verbose=True):\n     training_set = data.copy()\n     if verbose is True:\n         print(\"- Initializing the clustering algorithm...\")\n     if clustering_method == \"kmeans\":\n         algorithm = sklearn.cluster.KMeans(init=\"k-means++\", n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n     elif clustering_method == \"spectral\":\n         algorithm = sklearn.cluster.SpectralClustering(n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n     elif clustering_method == \"agglom\":\n         algorithm = sklearn.cluster.AgglomerativeClustering(n_clusters=n_microstates, linkage=\"complete\")\n     elif clustering_method == \"dbscan\":\n         algorithm = sklearn.cluster.DBSCAN(min_samples=100)\n     elif clustering_method == \"affinity\":\n         algorithm = sklearn.cluster.AffinityPropagation(damping=0.5)\n     else:\n         print(\"NeuroKit Error: eeg_microstates(): clustering_method must be \"kmeans\", \"spectral\", \"dbscan\", \"affinity\" or \"agglom\"\")\n     refitting = 0  \n     good_fit_achieved = False\n     while good_fit_achieved is False:\n         good_fit_achieved = True\n         if verbose is True:\n             print(\"- Fitting the classifier...\")\n         algorithm.fit(training_set)\n         if verbose is True:\n             print(\"- Clustering back the initial data...\")\n         predicted = algorithm.fit_predict(training_set)\n         if verbose is True:\n             print(\"- Check for abnormalities...\")\n         occurences = dict(collections.Counter(predicted))\n         masks = [np.array([True]*len(training_set))]\n         for microstate in occurences:\n             if occurences[microstate] < len(data)*occurence_rejection_treshold:\n                 good_fit_achieved = False\n                 refitting += 1  \n                 print(\"NeuroKit Warning: eeg_microstates(): detected some outliers: refitting the classifier (n=\" + str(refitting) + \").\")\n                 masks.append(predicted!=microstate)\n         mask = np.all(masks, axis=0)\n         training_set = training_set[mask]\n     return(algorithm)", "query": "k means clustering"}
{"id": "https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/examples/llcp-dta-iut.py#L156-L180", "method_name": "recv", "code": "def recv(self, recv_socket, llc):\n         time.sleep(0.1) \n         echo_buffer = queue.Queue(self.options.co_echo_buffer)\n         send_socket = nfc.llcp.Socket(llc, nfc.llcp.DATA_LINK_CONNECTION)\n         if self.options.pattern_number == 0x1200:\n             send_socket.connect(self.options.sap_lt_co_out_dest)\n         elif self.options.pattern_number == 0x1240:\n             send_socket.connect(\"urn:nfc:sn:dta-co-echo-out\")\n         elif self.options.pattern_number == 0x1280:\n             send_socket.connect(llc.resolve(\"urn:nfc:sn:dta-co-echo-out\"))\n         send_thread = Thread(target=self.send, args=(send_socket, echo_buffer))\n         send_thread.start()\n         log.info(\"receiving from sap %d\", recv_socket.getpeername())\n         while recv_socket.poll(\"recv\"):\n             data = recv_socket.recv()\n             if data == None: break\n             log.info(\"rcvd %d byte\", len(data))\n             recv_socket.setsockopt(nfc.llcp.SO_RCVBSY, echo_buffer.full())\n             echo_buffer.put(data)\n         log.info(\"remote side closed connection\")\n         try: echo_buffer.put_nowait(int(0))\n         except queue.Full: pass\n         send_thread.join()\n         recv_socket.close()\n         log.info(\"recv thread terminated\")", "query": "socket recv timeout"}
{"id": "https://github.com/Azure/azure-cli-extensions/blob/3d4854205b0f0d882f688cfa12383d14506c2e35/src/db-up/azext_db_up/custom.py#L381-L394", "method_name": "_run_postgresql_commands", "code": "def _run_postgresql_commands(host, user, password, database):\n     connection = psycopg2.connect(user=user, host=host, password=password, database=database)\n     connection.set_session(autocommit=True)\n     logger.warning(\"Successfully Connected to PostgreSQL.\")\n     cursor = connection.cursor()\n     try:\n         db_password = _create_db_password(database)\n         cursor.execute(\"CREATE USER root WITH ENCRYPTED PASSWORD \"{}\"\".format(db_password))\n         logger.warning(\"Ran Database Query: `CREATE USER root WITH ENCRYPTED PASSWORD \"%s\"`\", db_password)\n     except psycopg2.ProgrammingError:\n         pass\n     cursor.execute(\"GRANT ALL PRIVILEGES ON DATABASE {} TO root\".format(database))\n     logger.warning(\"Ran Database Query: `GRANT ALL PRIVILEGES ON DATABASE %s TO root`\", database)", "query": "postgresql connection"}
{"id": "https://github.com/saschpe/rapport/blob/ccceb8f84bd7e8add88ab5e137cdab6424aa4683/rapport/util.py#L55-L69", "method_name": "datetime_from_iso8601", "code": "def datetime_from_iso8601(date):\n     format = ISO8610_FORMAT\n     if date.endswith(\"Z\"):\n         date = date[:-1]  \n     if re.match(\".*\\.\\d+\", date):\n         format = ISO8610_FORMAT_MICROSECONDS\n     return datetime.datetime.strptime(date, format)", "query": "format date"}
{"id": "https://github.com/mapillary/mapillary_tools/blob/816785e90c589cae6e8e34a5530ce8417d29591c/mapillary_tools/process_csv.py#L175-L184", "method_name": "read_csv", "code": "def read_csv(csv_path, delimiter=\",\", header=False):\n     csv_data = []\n     with open(csv_path, \"r\") as csvfile:\n         csvreader = csv.reader(csvfile, delimiter=delimiter)\n         if header:\n             next(csvreader, None)\n         csv_data = zip(*csvreader)\n     return csv_data", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/optimization/optimizer.py#L36-L61", "method_name": "optimize", "code": "def optimize(self, x0, f=None, df=None, f_df=None):\n         import scipy.optimize\n         if f_df is None and df is not None: f_df = lambda x: float(f(x)), df(x)\n         if f_df is not None:\n             def _f_df(x):\n                 return f(x), f_df(x)[1][0]\n         if f_df is None and df is None:\n             res = scipy.optimize.fmin_l_bfgs_b(f, x0=x0, bounds=self.bounds,approx_grad=True, maxiter=self.maxiter)\n         else:\n             res = scipy.optimize.fmin_l_bfgs_b(_f_df, x0=x0, bounds=self.bounds, maxiter=self.maxiter)\n         if res[2][\"task\"] == b\"ABNORMAL_TERMINATION_IN_LNSRCH\":\n             result_x  = np.atleast_2d(x0)\n             result_fx =  np.atleast_2d(f(x0))\n         else:\n             result_x = np.atleast_2d(res[0])\n             result_fx = np.atleast_2d(res[1])\n         return result_x, result_fx", "query": "nelder mead optimize"}
{"id": "https://github.com/Alignak-monitoring/alignak/blob/f3c145207e83159b799d3714e4241399c7740a64/alignak/daemon.py#L1243-L1259", "method_name": "change_to_workdir", "code": "def change_to_workdir(self):\n         logger.info(\"Changing working directory to: %s\", self.workdir)\n         self.check_dir(self.workdir)\n         try:\n             os.chdir(self.workdir)\n         except OSError as exp:\n             self.exit_on_error(\"Error changing to working directory: %s. Error: %s. \"\n                                \"Check the existence of %s and the %s/%s account \"\n                                \"permissions on this directory.\"\n                                % (self.workdir, str(exp), self.workdir, self.user, self.group),\n                                exit_code=3)\n         self.pre_log.append((\"INFO\", \"Using working directory: %s\" % os.path.abspath(self.workdir)))", "query": "set working directory"}
{"id": "https://github.com/AustralianSynchrotron/lightflow/blob/dc53dbc1d961e20fb144273baca258060705c03e/lightflow/models/parameters.py#L62-L93", "method_name": "convert", "code": "def convert(self, value):\n         if self._type is str:\n             return str(value)\n         elif self._type is int:\n             try:\n                 return int(value)\n             except (UnicodeError, ValueError):\n                 raise WorkflowArgumentError(\"Cannot convert {} to int\".format(value))\n         elif self._type is float:\n             try:\n                 return float(value)\n             except (UnicodeError, ValueError):\n                 raise WorkflowArgumentError(\"Cannot convert {} to float\".format(value))\n         elif self._type is bool:\n             if isinstance(value, bool):\n                 return bool(value)\n             value = value.lower()\n             if value in (\"true\", \"1\", \"yes\", \"y\"):\n                 return True\n             elif value in (\"false\", \"0\", \"no\", \"n\"):\n                 return False\n             raise WorkflowArgumentError(\"Cannot convert {} to bool\".format(value))\n         else:\n             return value", "query": "convert int to bool"}
{"id": "https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/Facade.py#L594-L597", "method_name": "create_label_widget", "code": "def create_label_widget(self, text=None):\n         label_widget = LabelWidget(self.__ui)\n         label_widget.text = text\n         return label_widget", "query": "underline text in label widget"}
{"id": "https://github.com/numenta/htmresearch/blob/70c096b09a577ea0432c3f3bfff4442d4871b7aa/htmresearch/frameworks/specific_timing/timing_adtm.py#L276-L306", "method_name": "tempoAdjust2", "code": "def tempoAdjust2(self, tempoFactor):\n     late_votes = (len(self.adtm.getNextBasalPredictedCells()) - len(self.apicalIntersect)) * -1\n     early_votes = len(self.apicalIntersect)\n     votes = late_votes + early_votes\n     print(\"vote tally\", votes)\n     if votes > 0:\n       tempoFactor = tempoFactor * 0.5\n       print \"speed up\"\n     elif votes < 0:\n       tempoFactor = tempoFactor * 2\n       print \"slow down\"\n     elif votes == 0:\n       print \"pick randomly\"\n       if random.random() > 0.5:\n         tempoFactor = tempoFactor * 0.5\n         print \"random pick: speed up\"\n       else:\n         tempoFactor = tempoFactor * 2\n         print \"random pick: slow down\"\n     return tempoFactor", "query": "randomly pick a number"}
{"id": "https://github.com/yandex/yandex-tank/blob/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b/yandextank/config_converter/converter.py#L100-L104", "method_name": "to_bool", "code": "def to_bool(value):\n     try:\n         return bool(int(value))\n     except ValueError:\n         return True if \"true\" == value.lower() else False", "query": "convert int to bool"}
{"id": "https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/cfgpars.py#L752-L790", "method_name": "saveParList", "code": "def saveParList(self, *args, **kw):\n         if \"filename\" in kw:\n             filename = kw[\"filename\"]\n         if not filename:\n             filename = self.getFilename()\n         if not filename:\n             raise ValueError(\"No filename specified to save parameters\")\n         if hasattr(filename,\"write\"):\n             fh = filename\n             absFileName = os.path.abspath(fh.name)\n         else:\n             absFileName = os.path.expanduser(filename)\n             absDir = os.path.dirname(absFileName)\n             if len(absDir) and not os.path.isdir(absDir): os.makedirs(absDir)\n             fh = open(absFileName,\"w\")\n         numpars = len(self.__paramList)\n         if self._forUseWithEpar: numpars -= 1\n         if not self.final_comment: self.final_comment = [\"\"] \n at EOF\n         while len(self.defaults):\n             self.defaults.pop(-1) \n         for key in self._neverWrite:\n             self.defaults.append(key)\n         self.write(fh)\n         fh.close()\n         retval = str(numpars) + \" parameters written to \" + absFileName\n         self.filename = absFileName \n         self.debug(\"Keys not written: \"+str(self.defaults))\n         return retval", "query": "save list to file"}
{"id": "https://github.com/hearsaycorp/normalize/blob/8b36522ddca6d41b434580bd848f3bdaa7a999c8/normalize/diff.py#L547-L588", "method_name": "_fuzzy_match", "code": "def _fuzzy_match(set_a, set_b):\n     seen = dict()\n     scores = list()\n     for a_pk_seq, b_pk_seq in product(set_a, set_b):\n         a_pk, a_seq = a_pk_seq\n         b_pk, b_seq = b_pk_seq\n         if (a_pk, b_pk) in seen:\n             if seen[a_pk, b_pk][0]:\n                 score = list(seen[a_pk, b_pk])\n                 scores.append(score + [a_pk_seq, b_pk_seq])\n         else:\n             match = 0\n             common = min((len(a_pk), len(b_pk)))\n             no_match = max((len(a_pk), len(b_pk))) - common\n             for i in range(0, common):\n                 if a_pk[i] == b_pk[i]:\n                     if not _nested_falsy(a_pk[i]):\n                         match += 1\n                 else:\n                     no_match += 1\n             seen[a_pk, b_pk] = (match, no_match)\n             if match:\n                 scores.append([match, no_match, a_pk_seq, b_pk_seq])\n     remaining_a = set(set_a)\n     remaining_b = set(set_b)\n     for match, no_match, a_pk_seq, b_pk_seq in sorted(\n         scores,\n         key=lambda x: x[0] - x[1],\n         reverse=True,\n     ):\n         if a_pk_seq in remaining_a and b_pk_seq in remaining_b:\n             remaining_a.remove(a_pk_seq)\n             remaining_b.remove(b_pk_seq)\n             yield a_pk_seq, b_pk_seq\n         if not remaining_a or not remaining_b:\n             break", "query": "fuzzy match ranking"}
{"id": "https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/cookies.py#L27-L45", "method_name": "from_file", "code": "def from_file(filename):\n     entries = []\n     with open(filename) as fd:\n         lines = []\n         for line in fd.readlines():\n             line = line.rstrip()\n             if not line:\n                 if lines:\n                     entries.append(from_headers(\"\\r\n\".join(lines)))\n                 lines = []\n             else:\n                 lines.append(line)\n         if lines:\n             entries.append(from_headers(\"\\r\n\".join(lines)))\n         return entries", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/tensorforce/tensorforce/blob/520a8d992230e382f08e315ede5fc477f5e26bfb/tensorforce/core/distributions/gaussian.py#L83-L93", "method_name": "tf_sample", "code": "def tf_sample(self, distr_params, deterministic):\n         mean, stddev, _ = distr_params\n         definite = mean\n         normal_distribution = tf.random_normal(shape=tf.shape(input=mean))\n         sampled = mean + stddev * normal_distribution\n         return tf.where(condition=deterministic, x=definite, y=sampled)", "query": "normal distribution"}
{"id": "https://github.com/neuropsychology/NeuroKit.py/blob/c9589348fbbde0fa7e986048c48f38e6b488adfe/examples/UnderDev/eeg/eeg_microstates.py#L201-L253", "method_name": "eeg_microstates_clustering", "code": "def eeg_microstates_clustering(data, n_microstates=4, clustering_method=\"kmeans\", n_jobs=1, n_init=25, occurence_rejection_treshold=0.05, max_refitting=5, verbose=True):\n     training_set = data.copy()\n     if verbose is True:\n         print(\"- Initializing the clustering algorithm...\")\n     if clustering_method == \"kmeans\":\n         algorithm = sklearn.cluster.KMeans(init=\"k-means++\", n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n     elif clustering_method == \"spectral\":\n         algorithm = sklearn.cluster.SpectralClustering(n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n     elif clustering_method == \"agglom\":\n         algorithm = sklearn.cluster.AgglomerativeClustering(n_clusters=n_microstates, linkage=\"complete\")\n     elif clustering_method == \"dbscan\":\n         algorithm = sklearn.cluster.DBSCAN(min_samples=100)\n     elif clustering_method == \"affinity\":\n         algorithm = sklearn.cluster.AffinityPropagation(damping=0.5)\n     else:\n         print(\"NeuroKit Error: eeg_microstates(): clustering_method must be \"kmeans\", \"spectral\", \"dbscan\", \"affinity\" or \"agglom\"\")\n     refitting = 0  \n     good_fit_achieved = False\n     while good_fit_achieved is False:\n         good_fit_achieved = True\n         if verbose is True:\n             print(\"- Fitting the classifier...\")\n         algorithm.fit(training_set)\n         if verbose is True:\n             print(\"- Clustering back the initial data...\")\n         predicted = algorithm.fit_predict(training_set)\n         if verbose is True:\n             print(\"- Check for abnormalities...\")\n         occurences = dict(collections.Counter(predicted))\n         masks = [np.array([True]*len(training_set))]\n         for microstate in occurences:\n             if occurences[microstate] < len(data)*occurence_rejection_treshold:\n                 good_fit_achieved = False\n                 refitting += 1  \n                 print(\"NeuroKit Warning: eeg_microstates(): detected some outliers: refitting the classifier (n=\" + str(refitting) + \").\")\n                 masks.append(predicted!=microstate)\n         mask = np.all(masks, axis=0)\n         training_set = training_set[mask]\n     return(algorithm)", "query": "k means clustering"}
{"id": "https://github.com/deshima-dev/decode/blob/e789e174cd316e7ec8bc55be7009ad35baced3c0/decode/core/array/functions.py#L109-L121", "method_name": "empty", "code": "def empty(shape, dtype=None, **kwargs):\n     data = np.empty(shape, dtype)\n     return dc.array(data, **kwargs)", "query": "empty array"}
{"id": "https://github.com/thomasdelaet/python-velbus/blob/af2f8af43f1a24bf854eff9f3126fd7b5c41b3dd/velbus/messages/write_module_address_and_serial_number.py#L47-L54", "method_name": "data_to_binary", "code": "def data_to_binary(self):\n         return chr(COMMAND_CODE) + chr(self.module_type) + \\\n             struct.pack(\">L\", self.current_serial)[2:] + \\\n             chr(self.module_address) + \\\n             struct.pack(\">L\", self.new_serial)[2:]", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/majerteam/sylk_parser/blob/0f62f9d3ca1c273017d7ea728f9db809f2241389/sylk_parser/sylk_parser.py#L27-L40", "method_name": "to_csv", "code": "def to_csv(self, fbuf, quotechar=\"\"\", delimiter=\",\"):\n         csvwriter = csv.writer(\n             fbuf,\n             quotechar=quotechar,\n             delimiter=delimiter,\n             lineterminator=\"\n\",\n             quoting=csv.QUOTE_ALL\n         )\n         if self.headers:\n             csvwriter.writerow(self.headers)\n         for line in self.sylk_handler.stream_rows():\n             csvwriter.writerow(line)", "query": "write csv"}
{"id": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/data.py#L22-L42", "method_name": "uniq_stable", "code": "def uniq_stable(elems):\n     unique = []\n     unique_dict = {}\n     for nn in elems:\n         if nn not in unique_dict:\n             unique.append(nn)\n             unique_dict[nn] = None\n     return unique", "query": "unique elements"}
{"id": "https://github.com/scikit-hep/uproot/blob/fc406827e36ed87cfb1062806e118f53fd3a3b0a/uproot/interp/numerical.py#L340-L341", "method_name": "empty", "code": "def empty(self):\n        return self.awkward.numpy.empty((0, self.numbytes), dtype=self.todtype)", "query": "empty array"}
{"id": "https://github.com/teepark/greenhouse/blob/8fd1be4f5443ba090346b5ec82fdbeb0a060d956/greenhouse/io/sockets.py#L346-L377", "method_name": "recvfrom", "code": "def recvfrom(self, bufsize, flags=0):\n         with self._registered(\"re\"):\n             while 1:\n                 if self._closed:\n                     raise socket.error(errno.EBADF, \"Bad file descriptor\")\n                 try:\n                     return self._sock.recvfrom(bufsize, flags)\n                 except socket.error, exc:\n                     if not self._blocking or exc[0] not in _BLOCKING_OP:\n                         raise\n                     sys.exc_clear()\n                     if self._readable.wait(self.gettimeout()):\n                         raise socket.timeout(\"timed out\")\n                     if scheduler.state.interrupted:\n                         raise IOError(errno.EINTR, \"interrupted system call\")", "query": "socket recv timeout"}
{"id": "https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/monitors/web.py#L91-L101", "method_name": "_sma_observable", "code": "def _sma_observable(observable_name, expected, function=None, param=None, value=None):\n     param = param or os.environ.get(observable_name)\n     if not param:\n         return\n     groups = parse_param(param) if not isinstance(param, list) else param\n     function = function or globals()[\"test_\" + observable_name]\n     value = value or function(*groups)\n     name = function.__name__.replace(\"test_\", \"\").replace(\"_\", \" \").capitalize()\n     print(\"{}.name = \"{} {}\"\".format(observable_name, name, groups[1]))\n     print(\"{}.value = {}\".format(observable_name, value))\n     print(\"{}.expected = {}\".format(observable_name, expected))", "query": "get current observable value"}
{"id": "https://github.com/pyecharts/pyecharts/blob/02050acb0e94bb9453b88a25028de7a0ce23f125/pyecharts/commons/utils.py#L39-L41", "method_name": "write_utf8_html_file", "code": "def write_utf8_html_file(file_name, html_content):\n    with open(file_name, \"w+\", encoding=\"utf-8\") as html_file:\n        html_file.write(html_content)", "query": "output to html file"}
{"id": "https://github.com/s1s1ty/py-jsonq/blob/9625597a2578bddcbed4e540174d5253b1fc3b75/pyjsonq/query.py#L46-L60", "method_name": "__parse_json_file", "code": "def __parse_json_file(self, file_path):\n         if file_path == \"\" or os.path.splitext(file_path)[1] != \".json\":\n             raise IOError(\"Invalid Json file\")\n         with open(file_path) as json_file:\n             self._raw_data = json.load(json_file)\n         self._json_data = copy.deepcopy(self._raw_data)", "query": "parse json file"}
{"id": "https://github.com/AndrewAnnex/SpiceyPy/blob/fc20a9b9de68b58eed5b332f0c051fb343a6e335/spiceypy/spiceypy.py#L8663-L8689", "method_name": "mxmg", "code": "def mxmg(m1, m2, nrow1, ncol1, ncol2):\n     m1 = stypes.toDoubleMatrix(m1)\n     m2 = stypes.toDoubleMatrix(m2)\n     mout = stypes.emptyDoubleMatrix(x=ncol2, y=nrow1)\n     nrow1 = ctypes.c_int(nrow1)\n     ncol1 = ctypes.c_int(ncol1)\n     ncol2 = ctypes.c_int(ncol2)\n     libspice.mxmg_c(m1, m2, nrow1, ncol1, ncol2, mout)\n     return stypes.cMatrixToNumpy(mout)", "query": "matrix multiply"}
{"id": "https://github.com/MacHu-GWU/angora-project/blob/689a60da51cd88680ddbe26e28dbe81e6b01d275/angora/dtypes/dicttree.py#L378-L382", "method_name": "prettyprint", "code": "def prettyprint(d):\n         print(json.dumps(d, sort_keys=True, \n                          indent=4, separators=(\",\" , \": \")))", "query": "pretty print json"}
{"id": "https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149", "method_name": "_dt_to_epoch", "code": "def _dt_to_epoch(self, dt):\n         if PY2:\n             time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n             return int(time_delta.total_seconds())\n         else:\n             return int(dt.timestamp())", "query": "convert a utc time to epoch"}
{"id": "https://github.com/klahnakoski/mo-logs/blob/0971277ac9caf28a755b766b70621916957d4fea/mo_logs/strings.py#L547-L554", "method_name": "find_first", "code": "def find_first(value, find_arr, start=0):\n     i = len(value)\n     for f in find_arr:\n         temp = value.find(f, start)\n         if temp == -1: continue\n         i = min(i, temp)\n     if i == len(value): return -1\n     return i", "query": "find int in string"}
{"id": "https://github.com/snower/TorMySQL/blob/01ff87f03a850a2a6e466700d020878eecbefa5d/tormysql/pool.py#L102-L111", "method_name": "connect", "code": "def connect(self):\n         future = super(RecordQueryConnection, self).connect()\n         origin_query = self._connection.query\n         def query(sql, unbuffered=False):\n             self._last_query_sql = sql\n             return origin_query(sql, unbuffered)\n         self._connection.query = query\n         return future", "query": "connect to sql"}
{"id": "https://github.com/frawau/aioblescan/blob/02d12e90db3ee6df7be6513fec171f20dc533de3/aioblescan/plugins/eddystone.py#L105-L151", "method_name": "url_encoder", "code": "def url_encoder(self):\n         encodedurl = []\n         encodedurl.append(aios.IntByte(\"Tx Power\",self.power))\n         asisurl=\"\"\n         myurl = urlparse(self.type_payload)\n         myhostname = myurl.hostname\n         mypath = myurl.path\n         if (myurl.scheme,myhostname.startswith(\"www.\")) in url_schemes:\n             encodedurl.append(aios.IntByte(\"URL Scheme\",\n                                            url_schemes.index((myurl.scheme,myhostname.startswith(\"www.\")))))\n             if myhostname.startswith(\"www.\"):\n                 myhostname = myhostname[4:]\n         extval=None\n         if myhostname.split(\".\")[-1] in url_domain:\n             extval = url_domain.index(myhostname.split(\".\")[-1])\n             myhostname = \".\".join(myhostname.split(\".\")[:-1])\n         if extval is not None and not mypath.startswith(\"/\"):\n             extval+=7\n         else:\n             if myurl.port is None:\n                 if extval is not None:\n                     mypath = mypath[1:]\n             else:\n                 extval += 7\n         encodedurl.append(aios.String(\"URL string\"))\n         encodedurl[-1].val = myhostname\n         if extval is not None:\n             encodedurl.append(aios.IntByte(\"URL Extention\",extval))\n         if myurl.port:\n             asisurl += \":\"+str(myurl.port)+mypath\n         asisurl += mypath\n         if myurl.params:\n             asisurl += \";\"+myurl.params\n         if myurl.query:\n             asisurl += \"?\"+myurl.query\n         if myurl.fragment:\n             asisurl += \"\n         encodedurl.append(aios.String(\"Rest of URL\"))\n         encodedurl[-1].val = asisurl\n         tlength=0\n         for x in encodedurl: \n             tlength += len(x)\n         if tlength > 19: \n             raise Exception(\"Encoded url too long (max 18 bytes)\")\n         self.service_data_length.val += tlength \n         return encodedurl", "query": "encode url"}
{"id": "https://github.com/ctuning/ck/blob/7e009814e975f8742790d3106340088a46223714/ck/kernel.py#L4351-L4377", "method_name": "get_current_date_time", "code": "def get_current_date_time(i):\n     import datetime\n     a={}\n     now1=datetime.datetime.now()\n     now=now1.timetuple()\n     a[\"date_year\"]=now[0]\n     a[\"date_month\"]=now[1]\n     a[\"date_day\"]=now[2]\n     a[\"time_hour\"]=now[3]\n     a[\"time_minute\"]=now[4]\n     a[\"time_second\"]=now[5]\n     return {\"return\":0, \"array\":a, \"iso_datetime\":now1.isoformat()}", "query": "get current date"}
{"id": "https://github.com/demosdemon/format-pipfile/blob/f95162c49d8fc13153080ddb11ac5a5dcd4d2e7c/ci/appveyor-download.py#L95-L102", "method_name": "unpack_zipfile", "code": "def unpack_zipfile(filename):\n     with open(filename, \"rb\") as fzip:\n         z = zipfile.ZipFile(fzip)\n         for name in z.namelist():\n             print((\"      extracting {}\".format(name)))\n             ensure_dirs(name)\n             z.extract(name)", "query": "extract zip file recursively"}
{"id": "https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/utils/dialog.py#L233-L234", "method_name": "get_checkbox_state_by_name", "code": "def get_checkbox_state_by_name(self, checkbox_text):\n        return [checkbox.get_active() for checkbox in self.checkboxes if checkbox.get_label() == checkbox_text]", "query": "make the checkbox checked"}
{"id": "https://github.com/intuition-io/intuition/blob/cd517e6b3b315a743eb4d0d0dc294e264ab913ce/intuition/core/configuration.py#L28-L56", "method_name": "parse_commandline", "code": "def parse_commandline():\n     parser = argparse.ArgumentParser(\n         description=\"Intuition, the terrific trading system\")\n     parser.add_argument(\"-V\", \"--version\",\n                         action=\"version\",\n                         version=\"%(prog)s v{} Licence {}\".format(\n                             __version__, __licence__),\n                         help=\"Print program version\")\n     parser.add_argument(\"-v\", \"--showlog\",\n                         action=\"store_true\",\n                         help=\"Print logs on stdout\")\n     parser.add_argument(\"-b\", \"--bot\",\n                         action=\"store_true\",\n                         help=\"Allows the algorithm to process orders\")\n     parser.add_argument(\"-c\", \"--context\",\n                         action=\"store\", default=\"file::conf.yaml\",\n                         help=\"Provides the way to build context\")\n     parser.add_argument(\"-i\", \"--id\",\n                         action=\"store\", default=\"gekko\",\n                         help=\"Customize the session id\")\n     args = parser.parse_args()\n     return {\n         \"session\": args.id,\n         \"context\": args.context,\n         \"showlog\": args.showlog,\n         \"bot\": args.bot\n     }", "query": "parse command line argument"}
{"id": "https://github.com/FelixSchwarz/pymta/blob/1884accc3311e6c2e89259784f9592314f6d34fc/pymta/session.py#L175-L180", "method_name": "_send_custom_response", "code": "def _send_custom_response(self, reply):\n         code, custom_response = reply\n         if self._is_multiline_reply(custom_response):\n             self.multiline_reply(code, custom_response)\n         else:\n             self.reply(code, custom_response)", "query": "custom http error response"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/editor/plugin.py#L1537-L1542", "method_name": "__set_workdir", "code": "def __set_workdir(self): \n         fname = self.get_current_filename() \n         if fname is not None: \n             directory = osp.dirname(osp.abspath(fname)) \n             self.open_dir.emit(directory)", "query": "set working directory"}
{"id": "https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/utils.py#L147-L165", "method_name": "epoch_to_human_time", "code": "def epoch_to_human_time(epoch_time):\n     if isinstance(epoch_time, int):\n         try:\n             d = datetime.datetime.fromtimestamp(epoch_time / 1000)\n             return d.strftime(\"%m-%d-%Y %H:%M:%S \")\n         except ValueError:\n             return None", "query": "convert a utc time to epoch"}
{"id": "https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/requests/cookies.py#L441-L474", "method_name": "create_cookie", "code": "def create_cookie(name, value, **kwargs):\n     result = {\n         \"version\": 0,\n         \"name\": name,\n         \"value\": value,\n         \"port\": None,\n         \"domain\": \"\",\n         \"path\": \"/\",\n         \"secure\": False,\n         \"expires\": None,\n         \"discard\": True,\n         \"comment\": None,\n         \"comment_url\": None,\n         \"rest\": {\"HttpOnly\": None},\n         \"rfc2109\": False,\n     }\n     badargs = set(kwargs) - set(result)\n     if badargs:\n         err = \"create_cookie() got unexpected keyword arguments: %s\"\n         raise TypeError(err % list(badargs))\n     result.update(kwargs)\n     result[\"port_specified\"] = bool(result[\"port\"])\n     result[\"domain_specified\"] = bool(result[\"domain\"])\n     result[\"domain_initial_dot\"] = result[\"domain\"].startswith(\".\")\n     result[\"path_specified\"] = bool(result[\"path\"])\n     return cookielib.Cookie(**result)", "query": "create cookie"}
{"id": "https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/processors.py#L91-L95", "method_name": "__call__", "code": "def __call__(self, value):\n         for substring in self.substrings:\n             if value.startswith(substring):\n                 return value[len(substring):]\n         return value", "query": "positions of substrings in string"}
{"id": "https://github.com/tutorcruncher/pydf/blob/53dd030f02f112593ed6e2655160a40b892a23c0/docker-entrypoint.py#L36-L53", "method_name": "generate", "code": "async def generate(request):\n     start = time()\n     config = {}\n     for k, v in request.headers.items():\n         if k.startswith(\"Pdf-\") or k.startswith(\"Pdf_\"):\n             config[k[4:].lower()] = v.lower()\n     data = await request.read()\n     if not data:\n         logger.info(\"Request with no body data\")\n         raise web.HTTPBadRequest(text=\"400: no HTML data to convert to PDF in request body\n\")\n     try:\n         pdf_content = await app[\"apydf\"].generate_pdf(data.decode(), **config)\n     except RuntimeError as e:\n         logger.info(\"Error generating PDF, time %0.2fs, config: %s\", time() - start, config)\n         return web.Response(text=str(e) + \"\n\", status=418)\n     else:\n         logger.info(\"PDF generated in %0.2fs, html-len %d, pdf-len %d\", time() - start, len(data), len(pdf_content))\n         return web.Response(body=pdf_content, content_type=\"application/pdf\")", "query": "convert html to pdf"}
{"id": "https://github.com/bloomberg/bqplot/blob/8eb8b163abe9ee6306f6918067e2f36c1caef2ef/bqplot/pyplot.py#L816-L837", "method_name": "scatter", "code": "def scatter(x, y, **kwargs):\n     kwargs[\"x\"] = x\n     kwargs[\"y\"] = y\n     return _draw_mark(Scatter, **kwargs)", "query": "scatter plot"}
{"id": "https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/responses.py#L36-L41", "method_name": "get_table", "code": "def get_table(self):\n         database_name = self.parameters.get(\"DatabaseName\")\n         table_name = self.parameters.get(\"Name\")\n         table = self.glue_backend.get_table(database_name, table_name)\n         return json.dumps({\"Table\": table.as_dict()})", "query": "get database table name"}
{"id": "https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/contrib/isotp.py#L612-L621", "method_name": "recv_with_timeout", "code": "def recv_with_timeout(self, timeout=1):\n         msg = self.ins.recv(timeout)\n         t = time.time()\n         if msg is None:\n             raise Scapy_Exception(\"Timeout\")\n         return self.basecls, msg, t", "query": "socket recv timeout"}
{"id": "https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L297-L304", "method_name": "read_csv", "code": "def read_csv(filename, has_header=True):\n     with open(filename) as fh:\n         csv_reader = csv.reader(fh)\n         header = None\n         if has_header:\n             header = csv_reader.next()\n         rows = [row for row in csv_reader]\n     return header, rows", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/gtaylor/django-athumb/blob/69261ace0dff81e33156a54440874456a7b38dfb/athumb/backends/s3boto.py#L143-L150", "method_name": "_compress_content", "code": "def _compress_content(self, content):\n         zbuf = StringIO()\n         zfile = GzipFile(mode=\"wb\", compresslevel=6, fileobj=zbuf)\n         zfile.write(content.read())\n         zfile.close()\n         content.file = zbuf\n         return content", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/thebjorn/pydeps/blob/1e6715b7bea47a40e8042821b57937deaaa0fdc3/pydeps/arguments.py#L20-L31", "method_name": "boolval", "code": "def boolval(v): \n     if isinstance(v, bool): \n         return v \n     if isinstance(v, int): \n         return bool(v) \n     if is_string(v): \n         v = v.lower() \n         if v in {\"j\", \"y\", \"ja\", \"yes\", \"1\", \"true\"}: \n             return True \n         if v in {\"n\", \"nei\", \"no\", \"0\", \"false\"}: \n             return False \n     raise ValueError(\"Don\"t know how to convert %r to bool\" % v)", "query": "convert int to bool"}
{"id": "https://github.com/openstates/billy/blob/5fc795347f12a949e410a8cfad0c911ea6bced67/billy/web/api/handlers.py#L27-L48", "method_name": "_build_mongo_filter", "code": "def _build_mongo_filter(request, keys, icase=True):\n     _filter = {}\n     keys = set(keys) - set([\"fields\"])\n     for key in keys:\n         value = request.GET.get(key)\n         if value:\n             if key in _lower_fields:\n                 _filter[key] = value.lower()\n             elif key.endswith(\"__in\"):\n                 values = value.split(\" \")\n                 _filter[key[:-4]] = values\n             elif key == \"bill_id\":\n                 _filter[key] = fix_bill_id(value.upper())\n             else:\n                 _filter[key] = re.compile(\"^%s$\" % value, re.IGNORECASE)\n     return _filter", "query": "regex case insensitive"}
{"id": "https://github.com/ricobl/django-importer/blob/6967adfa7a286be7aaf59d3f33c6637270bd9df6/sample_project/tasks/importers.py#L64-L88", "method_name": "parse_date", "code": "def parse_date(self, item, field_name, source_name):\n         now = datetime.now().date()\n         val = self.get_value(item, source_name)\n         week_day, day = val.split()\n         day = int(day)\n         if now.day < day:\n             if now.month == 1:\n                 now = now.replace(month=12, year=now.year-1)\n             else:\n                 now = now.replace(month=now.month-1)\n         now = now.replace(day=day)\n         return now", "query": "get current date"}
{"id": "https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/nfs_share.py#L38-L50", "method_name": "get_xml_node", "code": "def get_xml_node(self):\n         xb = xmlapi.XmlBuilder()\n         ret = []\n         if self.access_hosts is not None:\n             ret.append(xb.list_elements(\"AccessHosts\", self.access_hosts))\n         if self.rw_hosts is not None:\n             ret.append(xb.list_elements(\"RwHosts\", self.rw_hosts))\n         if self.ro_hosts is not None:\n             ret.append(xb.list_elements(\"RoHosts\", self.ro_hosts))\n         if self.root_hosts is not None:\n             ret.append(xb.list_elements(\"RootHosts\", self.root_hosts))\n         return ret", "query": "get all parents of xml node"}
{"id": "https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/models.py#L48-L53", "method_name": "get_table", "code": "def get_table(self, database_name, table_name):\n         database = self.get_database(database_name)\n         try:\n             return database.tables[table_name]\n         except KeyError:\n             raise TableNotFoundException(table_name)", "query": "get database table name"}
{"id": "https://github.com/skorch-dev/skorch/blob/5b9b8b7b7712cb6e5aaa759d9608ea6269d5bcd3/skorch/net.py#L541-L554", "method_name": "initialize", "code": "def initialize(self):\n         self.initialize_virtual_params()\n         self.initialize_callbacks()\n         self.initialize_criterion()\n         self.initialize_module()\n         self.initialize_optimizer()\n         self.initialize_history()\n         self.initialized_ = True\n         return self", "query": "initializing array"}
{"id": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/event/pqueue.py#L1032-L1040", "method_name": "setPriority", "code": "def setPriority(self, queue, priority):\n         q = self.queueindex[queue]\n         self.queues[q[0]].removeSubQueue(q[1])\n         newPriority = self.queues.setdefault(priority, CBQueue.MultiQueue(self, priority))\n         q[0] = priority\n         newPriority.addSubQueue(q[1])", "query": "priority queue"}
{"id": "https://github.com/nigma/django-easy-pdf/blob/327605b91a445b453d8969b341ef74b12ab00a83/easy_pdf/rendering.py#L51-L78", "method_name": "html_to_pdf", "code": "def html_to_pdf(content, encoding=\"utf-8\",\n                 link_callback=fetch_resources, **kwargs):\n     src = BytesIO(content.encode(encoding))\n     dest = BytesIO()\n     pdf = pisa.pisaDocument(src, dest, encoding=encoding,\n                             link_callback=link_callback, **kwargs)\n     if pdf.err:\n         logger.error(\"Error rendering PDF document\")\n         for entry in pdf.log:\n             if entry[0] == xhtml2pdf.default.PML_ERROR:\n                 logger_x2p.error(\"line %s, msg: %s, fragment: %s\", entry[1], entry[2], entry[3])\n         raise PDFRenderingError(\"Errors rendering PDF\", content=content, log=pdf.log)\n     if pdf.warn:\n         for entry in pdf.log:\n             if entry[0] == xhtml2pdf.default.PML_WARNING:\n                 logger_x2p.warning(\"line %s, msg: %s, fragment: %s\", entry[1], entry[2], entry[3])\n     return dest.getvalue()", "query": "convert html to pdf"}
{"id": "https://github.com/KelSolaar/Umbra/blob/66f45f08d9d723787f1191989f8b0dda84b412ce/umbra/reporter.py#L468-L477", "method_name": "__set_html", "code": "def __set_html(self, html=None):\n         self.__html = self.__get_html(html)\n         self.__view.setHtml(self.__html)", "query": "get inner html"}
{"id": "https://github.com/wdbm/shijian/blob/ad6aea877e1eb99fe148127ea185f39f1413ed4f/shijian.py#L990-L995", "method_name": "unique_list_elements", "code": "def unique_list_elements(x):\n     unique_elements = []\n     for element in x:\n         if element not in unique_elements:\n             unique_elements.append(element)\n     return unique_elements", "query": "unique elements"}
{"id": "https://github.com/halcy/Mastodon.py/blob/35c43562dd3d34d6ebf7a0f757c09e8fcccc957c/mastodon/Mastodon.py#L2443-L2458", "method_name": "__datetime_to_epoch", "code": "def __datetime_to_epoch(self, date_time):\n         date_time_utc = None\n         if date_time.tzinfo is None:\n             date_time_utc = date_time.replace(tzinfo=pytz.utc)\n         else:\n             date_time_utc = date_time.astimezone(pytz.utc)\n         epoch_utc = datetime.datetime.utcfromtimestamp(0).replace(tzinfo=pytz.utc)\n         return (date_time_utc - epoch_utc).total_seconds()", "query": "convert a utc time to epoch"}
{"id": "https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/expr/api.py#L1943-L1962", "method_name": "_string_substr", "code": "def _string_substr(self, start, length=None):\n     op = ops.Substring(self, start, length)\n     return op.to_expr()", "query": "positions of substrings in string"}
{"id": "https://github.com/honzajavorek/redis-collections/blob/07ca8efe88fb128f7dc7319dfa6a26cd39b3776b/redis_collections/lists.py#L482-L498", "method_name": "reverse", "code": "def reverse(self):\n         def reverse_trans(pipe):\n             if self.writeback:\n                 self._sync_helper(pipe)\n             n = self.__len__(pipe)\n             for i in range(n // 2):\n                 left = pipe.lindex(self.key, i)\n                 right = pipe.lindex(self.key, n - i - 1)\n                 pipe.lset(self.key, i, right)\n                 pipe.lset(self.key, n - i - 1, left)\n         self._transaction(reverse_trans)", "query": "reverse a string"}
{"id": "https://github.com/cloudendpoints/endpoints-python/blob/00dd7c7a52a9ee39d5923191c2604b8eafdb3f24/endpoints/errors.py#L239-L253", "method_name": "_get_status_code", "code": "def _get_status_code(self, http_status):\n     try:\n       return int(http_status.split(\" \", 1)[0])\n     except TypeError:\n       _logger.warning(\"Unable to find status code in HTTP status %r.\",\n                       http_status)\n     return 500", "query": "get the description of a http status code"}
{"id": "https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/ndarray/sparse.py#L1507-L1543", "method_name": "zeros", "code": "def zeros(stype, shape, ctx=None, dtype=None, **kwargs):\n     if stype == \"default\":\n         return _zeros_ndarray(shape, ctx=ctx, dtype=dtype, **kwargs)\n     if ctx is None:\n         ctx = current_context()\n     dtype = mx_real_t if dtype is None else dtype\n     if stype in (\"row_sparse\", \"csr\"):\n         aux_types = _STORAGE_AUX_TYPES[stype]\n     else:\n         raise ValueError(\"unknown storage type\" + stype)\n     out = _ndarray_cls(_new_alloc_handle(stype, shape, ctx, True, dtype, aux_types))\n     return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, out=out, **kwargs)", "query": "initializing array"}
{"id": "https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/util/epmgp.py#L122-L208", "method_name": "min_faktor", "code": "def min_faktor(Mu, Sigma, k, gamma=1):\n     D = Mu.shape[0]\n     logS = np.zeros((D - 1,))\n     MP = np.zeros((D - 1,))\n     P = np.zeros((D - 1,))\n     M = np.copy(Mu)\n     V = np.copy(Sigma)\n     b = False\n     d = np.NaN\n     for count in range(50):\n         diff = 0\n         for i in range(D - 1):\n             l = i if  i < k else i + 1\n             try:\n                 M, V, P[i], MP[i], logS[i], d = lt_factor(k, l, M, V,\n                                                         MP[i], P[i], gamma)\n             except Exception as e:\n                 raise\n             if np.isnan(d):\n                 break\n             diff += np.abs(d)\n         if np.isnan(d):\n             break\n         if np.abs(diff) < 0.001:\n             b = True\n             break\n     if np.isnan(d):\n         logZ = -np.Infinity\n         yield logZ\n         dlogZdMu = np.zeros((D, 1))\n         yield dlogZdMu\n         dlogZdMudMu = np.zeros((D, D))\n         yield dlogZdMudMu\n         dlogZdSigma = np.zeros((int(0.5 * (D * (D + 1))), 1))\n         yield dlogZdSigma\n         mvmin = [Mu[k], Sigma[k, k]]\n         yield mvmin\n     else:\n         C = np.eye(D) / sq2\n         C[k, :] = -1 / sq2\n         C = np.delete(C, k, 1)\n         R = np.sqrt(P.T) * C\n         r = np.sum(MP.T * C, 1)\n         mp_not_zero = np.where(MP != 0)\n         mpm = MP[mp_not_zero] * MP[mp_not_zero] / P[mp_not_zero]\n         mpm = sum(mpm)\n         s = sum(logS)\n         IRSR = (np.eye(D - 1) + np.dot(np.dot(R.T, Sigma), R))\n         rSr = np.dot(np.dot(r.T, Sigma), r)\n         A = np.dot(R, np.linalg.solve(IRSR, R.T))\n         A = 0.5 * (A.T + A)  \n         b = (Mu + np.dot(Sigma, r))\n         Ab = np.dot(A, b)\n         try:\n             cIRSR = np.linalg.cholesky(IRSR)\n         except np.linalg.LinAlgError:\n             try:\n                 cIRSR = np.linalg.cholesky(IRSR + 1e-10 * np.eye(IRSR.shape[0]))\n             except np.linalg.LinAlgError:\n                 cIRSR = np.linalg.cholesky(IRSR + 1e-6 * np.eye(IRSR.shape[0]))\n         dts = 2 * np.sum(np.log(np.diagonal(cIRSR)))\n         logZ = 0.5 * (rSr - np.dot(b.T, Ab) - dts) + np.dot(Mu.T, r) + s - 0.5 * mpm\n         yield logZ\n         btA = np.dot(b.T, A)\n         dlogZdMu = r - Ab\n         yield dlogZdMu\n         dlogZdMudMu = -A\n         yield dlogZdMudMu\n         dlogZdSigma = -A - 2 * np.outer(r, Ab.T) + np.outer(r, r.T)\\\n                     + np.outer(btA.T, Ab.T)\n         dlogZdSigma2 = np.zeros_like(dlogZdSigma)\n         np.fill_diagonal(dlogZdSigma2, np.diagonal(dlogZdSigma))\n         dlogZdSigma = 0.5 * (dlogZdSigma + dlogZdSigma.T - dlogZdSigma2)\n         dlogZdSigma = np.rot90(dlogZdSigma, k=2)[np.triu_indices(D)][::-1]\n         yield dlogZdSigma", "query": "nelder mead optimize"}
{"id": "https://github.com/python-rope/rope/blob/1c9f9cd5964b099a99a9111e998f0dc728860688/rope/base/change.py#L94-L109", "method_name": "__str__", "code": "def __str__(self):\n         if self.time is not None:\n             date = datetime.datetime.fromtimestamp(self.time)\n             if date.date() == datetime.date.today():\n                 string_date = \"today\"\n             elif date.date() == (datetime.date.today() -\n                                  datetime.timedelta(1)):\n                 string_date = \"yesterday\"\n             elif date.year == datetime.date.today().year:\n                 string_date = date.strftime(\"%b %d\")\n             else:\n                 string_date = date.strftime(\"%d %b, %Y\")\n             string_time = date.strftime(\"%H:%M:%S\")\n             string_time = \"%s %s \" % (string_date, string_time)\n             return self.description + \" - \" + string_time\n         return self.description", "query": "string to date"}
{"id": "https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L62-L67", "method_name": "pick", "code": "def pick(self):\n         while True:\n             idx = random.randint(0, len(self.values) - 1)\n             v, p = self.values[idx]\n             if p >= random.uniform(0, 1):\n                 return v", "query": "randomly pick a number"}
{"id": "https://github.com/Jaymon/endpoints/blob/2f1c4ae2c69a168e69447d3d8395ada7becaa5fb/endpoints/interface/uwsgi/client.py#L265-L307", "method_name": "recv_raw", "code": "def recv_raw(self, timeout, opcodes, **kwargs):\n         orig_timeout = self.get_timeout(timeout)\n         timeout = orig_timeout\n         while timeout > 0.0:\n             start = time.time()\n             if not self.connected: self.connect(timeout=timeout, **kwargs)\n             with self.wstimeout(timeout, **kwargs) as timeout:\n                 logger.debug(\"{} waiting to receive for {} seconds\".format(self.client_id, timeout))\n                 try:\n                     opcode, data = self.ws.recv_data()\n                     if opcode in opcodes:\n                         timeout = 0.0\n                         break\n                     else:\n                         if opcode == websocket.ABNF.OPCODE_CLOSE:\n                             raise websocket.WebSocketConnectionClosedException()\n                 except websocket.WebSocketTimeoutException:\n                     pass\n                 except websocket.WebSocketConnectionClosedException:\n                     try:\n                         self.ws.shutdown()\n                     except AttributeError:\n                         pass\n             if timeout:\n                 stop = time.time()\n                 timeout -= (stop - start)\n             else:\n                 break\n         if timeout < 0.0:\n             raise IOError(\"recv timed out in {} seconds\".format(orig_timeout))\n         return opcode, data", "query": "socket recv timeout"}
{"id": "https://github.com/why2pac/dp-tornado/blob/a5948f5693f6ee2d9bab31f611fedc074e1caa96/dp_tornado/helper/datetime/time.py#L38-L53", "method_name": "convert", "code": "def convert(self,\n                 auto=None,\n                 datetime=None,\n                 timezone=None,\n                 timestamp=None,\n                 yyyymmdd=None,\n                 yyyymmddhhiiss=None,\n                 ms=False):\n         return self.helper.datetime.convert(\n             auto=auto,\n             datetime=datetime,\n             timezone=timezone,\n             timestamp=timestamp,\n             yyyymmdd=yyyymmdd,\n             yyyymmddhhiiss=yyyymmddhhiiss,\n             ms=ms)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/clach04/x10_any/blob/5b90a543b127ab9e6112fd547929b5ef4b8f0cbc/x10_any/cm17a.py#L106-L118", "method_name": "_sendBinaryData", "code": "def _sendBinaryData(port, data):\n     _reset(port)\n     time.sleep(leadInOutDelay)\n     for digit in data:\n         _sendBit(port, digit)\n     time.sleep(leadInOutDelay)", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/googleapis/google-auth-library-python/blob/2c6ad78917e936f38f87c946209c8031166dc96e/google/auth/_helpers.py#L130-L173", "method_name": "update_query", "code": "def update_query(url, params, remove=None):\n     if remove is None:\n         remove = []\n     parts = urllib.parse.urlparse(url)\n     query_params = urllib.parse.parse_qs(parts.query)\n     query_params.update(params)\n     query_params = {\n         key: value for key, value\n         in six.iteritems(query_params)\n         if key not in remove}\n     new_query = urllib.parse.urlencode(query_params, doseq=True)\n     new_parts = parts._replace(query=new_query)\n     return urllib.parse.urlunparse(new_parts)", "query": "parse query string in url"}
{"id": "https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/web/html.py#L109-L123", "method_name": "prepare", "code": "def prepare(self, html):\n         html = html.replace(\"<br/>\", \"<br />\")\n         html = html.replace(\"<hr/>\", \"<hr />\")\n         html = re.sub(r\"<li.*?>\", \"\n<li>* \", html)\n\", \"li>\")\n         html = html.replace(\"><\", \"> <\")\n         if not self.linebreaks:\n         \thtml = html.replace(\"\\r\", \"\n\")\n         \thtml = html.replace(\"\n\", \" \")\n         return html", "query": "html entities replace"}
{"id": "https://github.com/cfusting/fastgp/blob/6cf3c5d14abedaea064feef6ca434ee806a11756/fastgp/algorithms/evolutionary_feature_synthesis.py#L314-L332", "method_name": "build_operation_stack", "code": "def build_operation_stack(string):\n     stack = []\n     start = 0\n     for i, s in enumerate(string):\n         if s == \"(\":\n             substring = string[start:i]\n             start = i + 1\n             stack.append(substring)\n         elif s == \",\":\n             if i != start:\n                 substring = string[start:i]\n                 stack.append(substring)\n             start = i + 1\n         elif s == \")\":\n             if i != start:\n                 substring = string[start:i]\n                 stack.append(substring)\n             start = i + 1\n     return stack", "query": "positions of substrings in string"}
{"id": "https://github.com/AshleySetter/optoanalysis/blob/9d390acc834d70024d47b574aea14189a5a5714e/optoanalysis/optoanalysis/optoanalysis.py#L3311-L3329", "method_name": "_GetRealImagArray", "code": "def _GetRealImagArray(Array):\n     ImagArray = _np.array([num.imag for num in Array])\n     RealArray = _np.array([num.real for num in Array])\n     return RealArray, ImagArray", "query": "initializing array"}
{"id": "https://github.com/dlancer/django-crispy-contact-form/blob/3d422556add5aea3607344a034779c214f84da04/contact_form/helpers.py#L8-L19", "method_name": "get_user_ip", "code": "def get_user_ip(request):\n     ip = get_real_ip(request)\n     if ip is None:\n         ip = get_ip(request)\n         if ip is None:\n             ip = \"127.0.0.1\"\n     return ip", "query": "get current ip address"}
{"id": "https://github.com/klahnakoski/mo-logs/blob/0971277ac9caf28a755b766b70621916957d4fea/mo_logs/strings.py#L294-L315", "method_name": "find", "code": "def find(value, find, start=0):\n     l = len(value)\n     if is_list(find):\n         m = l\n         for f in find:\n             i = value.find(f, start)\n             if i == -1:\n                 continue\n             m = min(m, i)\n         return m\n     else:\n         i = value.find(find, start)\n         if i == -1:\n             return l\n         return i", "query": "find int in string"}
{"id": "https://github.com/shidenggui/easytrader/blob/e5ae4daeda4ea125763a95b280dd694c7f68257d/easytrader/helpers.py#L137-L139", "method_name": "str2num", "code": "def str2num(num_str, convert_type=\"float\"):\n    num = float(grep_comma(num_str))\n    return num if convert_type == \"float\" else int(num)", "query": "convert string to number"}
{"id": "https://github.com/crs4/pydoop/blob/f375be2a06f9c67eaae3ce6f605195dbca143b2b/pydoop/__init__.py#L183-L193", "method_name": "read_properties", "code": "def read_properties(fname):\n     parser = configparser.SafeConfigParser()\n     parser.optionxform = str  \n     try:\n         with open(fname) as f:\n             parser_read(parser, AddSectionWrapper(f))\n     except IOError as e:\n         if e.errno != errno.ENOENT:\n             raise\n         return None  \n     return dict(parser.items(AddSectionWrapper.SEC_NAME))", "query": "read properties file"}
{"id": "https://github.com/wtsi-hgi/python-common/blob/0376a6b574ff46e82e509e90b6cb3693a3dbb577/hgicommon/data_source/static_from_file.py#L72-L82", "method_name": "no_error_extract_data_from_file", "code": "def no_error_extract_data_from_file(self, file_path: str) -> Iterable[DataSourceType]:\n         try:\n             return self.extract_data_from_file(file_path)\n         except Exception as e:\n             logging.warning(e)\n             return []", "query": "extracting data from a text file"}
{"id": "https://github.com/thomasdelaet/python-velbus/blob/af2f8af43f1a24bf854eff9f3126fd7b5c41b3dd/velbus/messages/write_module_address_and_serial_number.py#L47-L54", "method_name": "data_to_binary", "code": "def data_to_binary(self):\n         return chr(COMMAND_CODE) + chr(self.module_type) + \\\n             struct.pack(\">L\", self.current_serial)[2:] + \\\n             chr(self.module_address) + \\\n             struct.pack(\">L\", self.new_serial)[2:]", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/niccokunzmann/ledtable/blob/a94d276f8a06e0f7f05f5cc704018c899e56bd9f/python/ledtable/SerialLEDTable.py#L54-L63", "method_name": "default_line", "code": "def default_line(self, line):\n         if line.endswith(b\"\n\"):\n             line = line[:-1]\n         if line.endswith(b\"\\r\"):\n             line = line[:-1]\n         try:\n             line = line.decode(\"ASCII\")\n         except UnicodeDecodeError:\n             pass\n         print(line)", "query": "read text file line by line"}
{"id": "https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L24-L47", "method_name": "pick", "code": "def pick(self):\n         v = random.uniform(0, self.ub)\n         d = self.dist\n         c = self.vc - 1\n         s = self.vc\n         while True:\n             s = s / 2\n             if s == 0:\n                 break\n             if v <= d[c][1]:\n                 c -= s\n             else:\n                 c += s\n                 while len(d) <= c:\n                     s = s / 2\n                     c -= s\n                     if s == 0:\n                         break\n         if c == len(d) or v <= d[c][1]:\n             c -= 1\n         return d[c][0]", "query": "randomly pick a number"}
{"id": "https://github.com/EdwinvO/pyutillib/blob/6d773c31d1f27cc5256d47feb8afb5c3ae5f0db5/pyutillib/date_utils.py#L35-L84", "method_name": "datestr2date", "code": "def datestr2date(date_str):\n     if any(c not in \"0123456789-/\" for c in date_str):\n         raise ValueError(\"Illegal character in date string\")\n     if \"/\" in date_str:\n         try:\n             m, d, y = date_str.split(\"/\")\n         except:\n             raise ValueError(\"Date {} must have no or exactly 2 slashes. {}\".\n                     format(date_str, VALID_DATE_FORMATS_TEXT))\n     elif \"-\" in date_str:\n         try:\n             d, m, y = date_str.split(\"-\")\n         except:\n             raise ValueError(\"Date {} must have no or exactly 2 dashes. {}\".\n                     format(date_str, VALID_DATE_FORMATS_TEXT))\n     elif len(date_str) == 8 or len(date_str) == 6:\n         d = date_str[-2:]\n         m = date_str[-4:-2]\n         y = date_str[:-4]\n     else:\n         raise ValueError(\"Date format not recognised. {}\".format(\n                 VALID_DATE_FORMATS_TEXT))\n     if len(y) == 2:\n         year = 2000 + int(y)\n     elif len(y) == 4:\n         year = int(y)\n     else:\n         raise ValueError(\"year must be 2 or 4 digits\")\n     for s in (m, d):\n         if 1 <= len(s) <= 2:\n             month, day = int(m), int(d)\n         else:\n             raise ValueError(\"m and d must be 1 or 2 digits\")\n     try:\n         return datetime.date(year, month, day)\n     except ValueError:\n         raise ValueError(\"Invalid date {}. {}\".format(date_str, \n                 VALID_DATE_FORMATS_TEXT))", "query": "string to date"}
{"id": "https://github.com/janpipek/physt/blob/6dd441b073514e7728235f50b2352d56aacf38d4/physt/examples/__init__.py#L12-L22", "method_name": "normal_h1", "code": "def normal_h1(size: int = 10000, mean: float = 0, sigma: float = 1) -> Histogram1D:\n     data = np.random.normal(mean, sigma, (size,))\n     return h1(data, name=\"normal\", axis_name=\"x\", title=\"1D normal distribution\")", "query": "normal distribution"}
{"id": "https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/tree_graph.py#L39-L44", "method_name": "get_parents", "code": "def get_parents(self, node):\n         parent = self.parents.get(node)\n         if parent == None:\n             return set()\n         else:\n             return {parent}", "query": "get all parents of xml node"}
{"id": "https://github.com/fermiPy/fermipy/blob/9df5e7e3728307fd58c5bba36fd86783c39fbad4/fermipy/timing.py#L14-L21", "method_name": "elapsed_time", "code": "def elapsed_time(self):\n         if self._t0 is not None:\n             return self._time + self._get_time()\n         else:\n             return self._time", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/markfinger/django-node/blob/a2f56bf027fd3c4cbc6a0213881922a50acae1d6/django_node/utils.py#L183-L193", "method_name": "decode_html_entities", "code": "def decode_html_entities(html):\n     if not html:\n         return html\n     for entity, char in six.iteritems(html_entity_map):\n         html = html.replace(entity, char)\n     return html", "query": "html entities replace"}
{"id": "https://github.com/samuelcolvin/buildpg/blob/33cccff45279834d02ec7e97d8417da8fd2a875d/buildpg/funcs.py#L84-L85", "method_name": "position", "code": "def position(substring, string):\n    return logic.Func(\"position\", logic.SqlBlock(substring).in_(string))", "query": "positions of substrings in string"}
{"id": "https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/html_metadata_extractor.py#L40-L90", "method_name": "extract", "code": "def extract(self, html_text: str,\n                 extract_title: bool = False,\n                 extract_meta: bool = False,\n                 extract_microdata: bool = False,\n                 microdata_base_url: str = \"\",\n                 extract_json_ld: bool = False,\n                 extract_rdfa: bool = False,\n                 rdfa_base_url: str = \"\") \\\n             -> List[Extraction]:\n         res = list()\n         soup = BeautifulSoup(html_text, \"html.parser\")\n         if soup.title and extract_title:\n             title = self._wrap_data(\"title\", soup.title.string.encode(\"utf-8\").decode(\"utf-8\"))\n             res.append(title)\n         if soup.title and extract_meta:\n             meta_content = self._wrap_meta_content(soup.find_all(\"meta\"))\n             meta_data = self._wrap_data(\"meta\", meta_content)\n             res.append(meta_data)\n         if extract_microdata:\n             mde = MicrodataExtractor()\n             mde_data = self._wrap_data(\"microdata\", mde.extract(html_text, microdata_base_url))\n             res.append(mde_data)\n         if extract_json_ld:\n             jslde = JsonLdExtractor()\n             jslde_data = self._wrap_data(\"json-ld\", jslde.extract(html_text))\n             res.append(jslde_data)\n         if extract_rdfa:\n             rdfae = RDFaExtractor()\n             rdfae_data = self._wrap_data(\"rdfa\", rdfae.extract(html_text, rdfa_base_url))\n             res.append(rdfae_data)\n         return res", "query": "extract data from html content"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/smart_contract/native_contract/governance.py#L403-L414", "method_name": "to_json", "code": "def to_json(self):\n         map = dict()\n         map[\"peer_pubkey\"] = self.peer_pubkey\n         map[\"max_authorize\"] = self.max_authorize\n         map[\"old_peerCost\"] = self.old_peerCost\n         map[\"new_peer_cost\"] = self.new_peer_cost\n         map[\"set_cost_view\"] = self.set_cost_view\n         map[\"field1\"] = self.field1\n         map[\"field2\"] = self.field2\n         map[\"field3\"] = self.field3\n         map[\"field4\"] = self.field4\n         return map", "query": "map to json"}
{"id": "https://github.com/shidenggui/easytrader/blob/e5ae4daeda4ea125763a95b280dd694c7f68257d/easytrader/helpers.py#L137-L139", "method_name": "str2num", "code": "def str2num(num_str, convert_type=\"float\"):\n    num = float(grep_comma(num_str))\n    return num if convert_type == \"float\" else int(num)", "query": "convert string to number"}
{"id": "https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/dynamic.py#L257-L265", "method_name": "summary", "code": "def summary(self):\n         print(\"Varying Keys: %r\" % self.key)\n         print(\"Maximum steps allowed: %d\" % self.max_steps)\n         self._trace_summary()\n         (val, arg) = (self.trace[-1])\n         if self._termination_info:\n             (success, best_val, arg) = self._termination_info\n             condition =  \"Successfully converged.\" if success else \"Maximum step limit reached.\"\n             print(\"%s Minimum value of %r at %s=%r.\" % (condition, best_val, self.key, arg))", "query": "print model summary"}
{"id": "https://github.com/merenlab/illumina-utils/blob/246d0611f976471783b83d2aba309b0cb57210f6/IlluminaUtils/lib/fastalib.py#L111-L128", "method_name": "init_unique_hash", "code": "def init_unique_hash(self):\n         while self.next_regular():\n             hash = hashlib.sha1(self.seq.upper().encode(\"utf-8\")).hexdigest()\n             if hash in self.unique_hash_dict:\n                 self.unique_hash_dict[hash][\"ids\"].append(self.id)\n                 self.unique_hash_dict[hash][\"count\"] += 1\n             else:\n                 self.unique_hash_dict[hash] = {\"id\": self.id,\n                                                \"ids\": [self.id],\n                                                \"seq\": self.seq,\n                                                \"count\": 1}\n         self.unique_hash_list = [i[1] for i in sorted([(self.unique_hash_dict[hash][\"count\"], hash)\\\n                         for hash in self.unique_hash_dict], reverse=True)]\n         self.total_unique = len(self.unique_hash_dict)\n         self.reset()", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/lark-parser/lark/blob/a798dec77907e74520dd7e90c7b6a4acc680633a/examples/error_reporting_lalr.py#L32-L58", "method_name": "parse", "code": "def parse(json_text):\n     try:\n         j = json_parser.parse(json_text)\n     except UnexpectedInput as u:\n         exc_class = u.match_examples(json_parser.parse, {\n             JsonMissingOpening: [\"{\"foo\": ]}\",\n                                  \"{\"foor\": }}\",\n                                  \"{\"foo\": }\"],\n             JsonMissingClosing: [\"{\"foo\": [}\",\n                                  \"{\",\n                                  \"{\"a\": 1\",\n                                  \"[1\"],\n             JsonMissingComma: [\"[1 2]\",\n                                \"[false 1]\",\n                                \"[\"b\" 1]\",\n                                \"{\"a\":true 1:4}\",\n                                \"{\"a\":1 1:4}\",\n                                \"{\"a\":\"b\" 1:4}\"],\n             JsonTrailingComma: [\"[,]\",\n                                 \"[1,]\",\n                                 \"[1,2,]\",\n                                 \"{\"foo\":1,}\",\n                                 \"{\"foo\":false,\"bar\":true,}\"]\n         })\n         if not exc_class:\n             raise\n         raise exc_class(u.get_context(json_text), u.line, u.column)", "query": "parse json file"}
{"id": "https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/utils/hparam.py#L558-L572", "method_name": "parse_json", "code": "def parse_json(self, values_json):\n     values_map = json.loads(values_json)\n     return self.override_from_dict(values_map)", "query": "map to json"}
{"id": "https://github.com/IAMconsortium/pyam/blob/4077929ca6e7be63a0e3ecf882c5f1da97b287bf/pyam/core.py#L1049-L1070", "method_name": "to_excel", "code": "def to_excel(self, excel_writer, sheet_name=\"data\",\n                  iamc_index=False, **kwargs):\n         if not isinstance(excel_writer, pd.ExcelWriter):\n             close = True\n             excel_writer = pd.ExcelWriter(excel_writer)\n         self._to_file_format(iamc_index)\\\n             .to_excel(excel_writer, sheet_name=sheet_name, index=False,\n                       **kwargs)\n         if close:\n             excel_writer.close()", "query": "export to excel"}
{"id": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L9-L14", "method_name": "reverse_words", "code": "def reverse_words(string):\n     arr = string.strip().split()  \n     n = len(arr)\n     reverse(arr, 0, n-1)\n     return \" \".join(arr)", "query": "reverse a string"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/referenceanalysis.py#L85-L88", "method_name": "toPdf", "code": "def toPdf(self):\n         html = safe_unicode(self.template()).encode(\"utf-8\")\n         pdf_data = createPdf(html)\n         return pdf_data", "query": "convert html to pdf"}
{"id": "https://github.com/facelessuser/backrefs/blob/3b3d60f5d57b02044f880aa29c9c5add0e31a34f/tools/unidatadownload.py#L32-L44", "method_name": "unzip_unicode", "code": "def unzip_unicode(output, version):\n     unzipper = zipfile.ZipFile(os.path.join(output, \"unicodedata\", \"%s.zip\" % version))\n     target = os.path.join(output, \"unicodedata\", version)\n     print(\"Unzipping %s.zip...\" % version)\n     os.makedirs(target)\n     for f in unzipper.namelist():\n         unzipper.extract(f, target)", "query": "unzipping large files"}
{"id": "https://github.com/costastf/locationsharinglib/blob/dcd74b0cdb59b951345df84987238763e50ef282/_CI/library/core_library.py#L186-L206", "method_name": "get_binary_path", "code": "def get_binary_path(executable, logging_level=\"INFO\"):\n     if sys.platform == \"win32\":\n         if executable == \"start\":\n             return executable\n         executable = executable + \".exe\"\n         if executable in os.listdir(\".\"):\n             binary = os.path.join(os.getcwd(), executable)\n         else:\n             binary = next((os.path.join(path, executable)\n                            for path in os.environ[\"PATH\"].split(os.pathsep)\n                            if os.path.isfile(os.path.join(path, executable))), None)\n     else:\n         venv_parent = get_venv_parent_path()\n         venv_bin_path = os.path.join(venv_parent, \".venv\", \"bin\")\n         if not venv_bin_path in os.environ.get(\"PATH\"):\n             if logging_level == \"DEBUG\":\n                 print(f\"Adding path {venv_bin_path} to environment PATH variable\")\n             os.environ[\"PATH\"] = os.pathsep.join([os.environ[\"PATH\"], venv_bin_path])\n         binary = shutil.which(executable)\n     return binary if binary else None", "query": "get executable path"}
{"id": "https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/jx_python/containers/doc_store.py#L144-L174", "method_name": "_sort", "code": "def _sort(self, short_list, sorts):\n         sort_values = self._index_columns(sorts)\n         output = []\n         def _sort_more(short_list, i, sorts):\n             if len(sorts) == 0:\n                 output.extend(short_list)\n             sort = sorts[0]\n             index = self._index[sort_values[i]]\n             if sort.sort == 1:\n                 sorted_keys = sorted(index.keys())\n             elif sort.sort == -1:\n                 sorted_keys = reversed(sorted(index.keys()))\n             else:\n                 sorted_keys = list(index.keys())\n             for k in sorted_keys:\n                 self._sort(index[k] & short_list, i + 1, sorts[1:])\n         _sort_more(short_list, 0, sorts)\n         return output", "query": "sort string list"}
{"id": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L359-L361", "method_name": "assert_not_checked_checkbox", "code": "def assert_not_checked_checkbox(step, value):\n    check_box = find_field(world.browser, \"checkbox\", value)\n    assert_true(step, not check_box.is_selected())", "query": "check if a checkbox is checked"}
{"id": "https://github.com/markperdue/pyvesync/blob/7552dd1a6dd5ebc452acf78e33fd8f6e721e8cfc/src/pyvesync/helpers.py#L122-L127", "method_name": "calculate_hex", "code": "def calculate_hex(hex_string):\n         hex_conv = hex_string.split(\":\")\n         converted_hex = (int(hex_conv[0], 16) + int(hex_conv[1], 16))/8192\n         return converted_hex", "query": "convert decimal to hex"}
{"id": "https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/processors.py#L104-L108", "method_name": "__call__", "code": "def __call__(self, value):\n         for substring in self.substrings:\n             if value.endswith(substring):\n                 return value[:-len(substring)]\n         return value", "query": "positions of substrings in string"}
{"id": "https://github.com/flo-compbio/genometools/blob/dd962bb26d60a0f14ca14d8c9a4dd75768962c7d/genometools/expression/matrix.py#L295-L319", "method_name": "get_figure", "code": "def get_figure(self, heatmap_kw=None, **kwargs):\n         if heatmap_kw is not None:\n             assert isinstance(heatmap_kw, dict)\n         if heatmap_kw is None:\n             heatmap_kw = {}\n         return self.get_heatmap(**heatmap_kw).get_figure(**kwargs)", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/html5lib/html5lib-python/blob/4b2275497b624c6e97150fa2eb16a7db7ed42111/utils/entities.py#L82-L89", "method_name": "make_entities_code", "code": "def make_entities_code(entities):\n     entities_text = \"\n\".join(\"    \\\"%s\\\": u\\\"%s\\\",\" % (\n         name, entities[name].encode(\n             \"unicode-escape\").replace(\"\\\"\", \"\\\\\\\"\"))\n         for name in sorted(entities.keys()))\n     return  % entities_text", "query": "html entities replace"}
{"id": "https://github.com/MechanicalSoup/MechanicalSoup/blob/027a270febf5bcda6a75db60ea9838d631370f4b/mechanicalsoup/form.py#L99-L146", "method_name": "set_checkbox", "code": "def set_checkbox(self, data, uncheck_other_boxes=True):\n         for (name, value) in data.items():\n             checkboxes = self.find_by_type(\"input\", \"checkbox\", {\"name\": name})\n             if not checkboxes:\n                 raise InvalidFormMethod(\"No input checkbox named \" + name)\n             if uncheck_other_boxes:\n                 self.uncheck_all(name)\n             if not isinstance(value, list) and not isinstance(value, tuple):\n                 value = (value,)\n             for choice in value:\n                 choice_str = str(choice)  \n                 for checkbox in checkboxes:\n                     if checkbox.attrs.get(\"value\", \"on\") == choice_str:\n                         checkbox[\"checked\"] = \"\"\n                         break\n                     elif choice is True:\n                         checkbox[\"checked\"] = \"\"\n                         break\n                     elif choice is False:\n                         if \"checked\" in checkbox.attrs:\n                             del checkbox.attrs[\"checked\"]\n                         break\n                 else:\n                     raise LinkNotFoundError(\n                         \"No input checkbox named %s with choice %s\" %\n                         (name, choice)\n                     )", "query": "check if a checkbox is checked"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_disp.py#L7-L12", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser()\n     parser.add_argument( \"xdatcar\" )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/fadhiilrachman/line-py/blob/b7f5f2b3fc09fa3fbf6088d7ebdaf9e44d96ba69/linepy/server.py#L18-L19", "method_name": "urlEncode", "code": "def urlEncode(self, url, path, params=[]):\n        return url + path + \"?\" + urllib.parse.urlencode(params)", "query": "encode url"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/pdf.py#L232-L402", "method_name": "make_pdf_from_html", "code": "def make_pdf_from_html(\n         on_disk: bool,\n         html: str,\n         output_path: str = None,\n         header_html: str = None,\n         footer_html: str = None,\n         wkhtmltopdf_filename: str = _WKHTMLTOPDF_FILENAME,\n         wkhtmltopdf_options: Dict[str, Any] = None,\n         file_encoding: str = \"utf-8\",\n         debug_options: bool = False,\n         debug_content: bool = False,\n         debug_wkhtmltopdf_args: bool = True,\n         fix_pdfkit_encoding_bug: bool = None,\n         processor: str = _DEFAULT_PROCESSOR) -> Union[bytes, bool]:\n     wkhtmltopdf_options = wkhtmltopdf_options or {}  \n     assert_processor_available(processor)\n     if debug_content:\n         log.debug(\"html: {}\", html)\n         log.debug(\"header_html: {}\", header_html)\n         log.debug(\"footer_html: {}\", footer_html)\n     if fix_pdfkit_encoding_bug is None:\n         fix_pdfkit_encoding_bug = get_default_fix_pdfkit_encoding_bug()\n     if processor == Processors.XHTML2PDF:\n         if on_disk:\n             with open(output_path, mode=\"wb\") as outfile:\n                 xhtml2pdf.document.pisaDocument(html, outfile)\n             return True\n         else:\n             memfile = io.BytesIO()\n             xhtml2pdf.document.pisaDocument(html, memfile)\n             memfile.seek(0)\n             return memfile.read()\n     elif processor == Processors.WEASYPRINT:\n         if on_disk:\n             return weasyprint.HTML(string=html).write_pdf(output_path)\n         else:\n             return weasyprint.HTML(string=html).write_pdf()\n     elif processor == Processors.PDFKIT:\n         if not wkhtmltopdf_filename:\n             config = None\n         else:\n             if fix_pdfkit_encoding_bug:  \n                 log.debug(\"Attempting to fix bug in pdfkit (e.g. version 0.5.0)\"\n                           \" by encoding wkhtmltopdf_filename to UTF-8\")\n                 config = pdfkit.configuration(\n                     wkhtmltopdf=wkhtmltopdf_filename.encode(\"utf-8\"))\n             else:\n                 config = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_filename)\n         h_filename = None\n         f_filename = None\n         try:\n             if header_html:\n                 h_fd, h_filename = tempfile.mkstemp(suffix=\".html\")\n                 os.write(h_fd, header_html.encode(file_encoding))\n                 os.close(h_fd)\n                 wkhtmltopdf_options[\"header-html\"] = h_filename\n             if footer_html:\n                 f_fd, f_filename = tempfile.mkstemp(suffix=\".html\")\n                 os.write(f_fd, footer_html.encode(file_encoding))\n                 os.close(f_fd)\n                 wkhtmltopdf_options[\"footer-html\"] = f_filename\n             if debug_options:\n                 log.debug(\"wkhtmltopdf config: {!r}\", config)\n                 log.debug(\"wkhtmltopdf_options: {}\",\n                           pformat(wkhtmltopdf_options))\n             kit = pdfkit.pdfkit.PDFKit(html, \"string\", configuration=config,\n                                        options=wkhtmltopdf_options)\n             if on_disk:\n                 path = output_path\n             else:\n                 path = None\n             if debug_wkhtmltopdf_args:\n                 log.debug(\"Probable current user: {!r}\", getpass.getuser())\n                 log.debug(\"wkhtmltopdf arguments will be: {!r}\",\n                           kit.command(path=path))\n             return kit.to_pdf(path=path)\n         finally:\n             if h_filename:\n                 os.remove(h_filename)\n             if f_filename:\n                 os.remove(f_filename)\n     else:\n         raise AssertionError(\"Unknown PDF engine\")", "query": "convert html to pdf"}
{"id": "https://github.com/profusion/sgqlc/blob/684afb059c93f142150043cafac09b7fd52bfa27/examples/github/github-agile-dashboard.py#L35-L39", "method_name": "json_pretty_print", "code": "def json_pretty_print(d, file=None):\n     args = {\"sort_keys\": True, \"indent\": 2, \"separators\": (\",\", \": \")}\n     if file:\n         return json.dump(d, file, **args)\n     return json.dumps(d, **args)", "query": "pretty print json"}
{"id": "https://github.com/lark-parser/lark/blob/a798dec77907e74520dd7e90c7b6a4acc680633a/examples/error_reporting_lalr.py#L32-L58", "method_name": "parse", "code": "def parse(json_text):\n     try:\n         j = json_parser.parse(json_text)\n     except UnexpectedInput as u:\n         exc_class = u.match_examples(json_parser.parse, {\n             JsonMissingOpening: [\"{\"foo\": ]}\",\n                                  \"{\"foor\": }}\",\n                                  \"{\"foo\": }\"],\n             JsonMissingClosing: [\"{\"foo\": [}\",\n                                  \"{\",\n                                  \"{\"a\": 1\",\n                                  \"[1\"],\n             JsonMissingComma: [\"[1 2]\",\n                                \"[false 1]\",\n                                \"[\"b\" 1]\",\n                                \"{\"a\":true 1:4}\",\n                                \"{\"a\":1 1:4}\",\n                                \"{\"a\":\"b\" 1:4}\"],\n             JsonTrailingComma: [\"[,]\",\n                                 \"[1,]\",\n                                 \"[1,2,]\",\n                                 \"{\"foo\":1,}\",\n                                 \"{\"foo\":false,\"bar\":true,}\"]\n         })\n         if not exc_class:\n             raise\n         raise exc_class(u.get_context(json_text), u.line, u.column)", "query": "parse json file"}
{"id": "https://github.com/firstprayer/monsql/blob/6285c15b574c8664046eae2edfeb548c7b173efd/monsql/wrapper_postgresql.py#L48-L50", "method_name": "get_table_obj", "code": "def get_table_obj(self, name):\n        table = PostgreSQLTable(db=self.db, name=name, mode=self.mode)\n        return table", "query": "get database table name"}
{"id": "https://github.com/rfinnie/dsari/blob/cd7b07c30876467393e0ec18f1ff45d86bcb1676/dsari/render.py#L75-L86", "method_name": "read_output", "code": "def read_output(filename):\n     if os.path.isfile(filename):\n         with open(filename, \"rb\") as f:\n             return f.read().decode(\"utf-8\")\n     elif os.path.isfile(\"{}.gz\".format(filename)):\n         with gzip.open(\"{}.gz\".format(filename), \"rb\") as f:\n             return f.read().decode(\"utf-8\")\n     elif HAS_LZMA and os.path.isfile(\"{}.xz\".format(filename)):\n         with open(\"{}.xz\".format(filename), \"rb\") as f:\n             return lzma.LZMADecompressor().decompress(f.read()).decode(\"utf-8\")\n     else:\n         return None", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP/blob/84dc8114bf8a79c3acb3f7f59128247b9fc97243/numpy_indexed/grouping.py#L343-L382", "method_name": "median", "code": "def median(self, values, axis=0, average=True):\n         mid_2 = self.index.start + self.index.stop\n         hi = (mid_2    ) // 2\n         lo = (mid_2 - 1) // 2\n         sorted_group_rank_per_key = self.index.sorted_group_rank_per_key\n         def median1d(slc):\n             slc    = slc[self.index.sorter]\n             sorter = np.lexsort((slc, sorted_group_rank_per_key))\n             slc    = slc[sorter]\n             return (slc[lo]+slc[hi]) / 2 if average else slc[hi]\n         values = np.asarray(values)\n         if values.ndim>1:   \n             values = np.apply_along_axis(median1d, axis, values)\n         else:\n             values = median1d(values)\n         return self.unique, values", "query": "deducting the median from each column"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L67-L79", "method_name": "utc_epoch", "code": "def utc_epoch():\n     d = dt.datetime(1970, 1, 1)\n     d = d.replace(tzinfo=utc)\n     return d", "query": "convert a utc time to epoch"}
{"id": "https://github.com/bulkan/robotframework-requests/blob/11baa3277f1cb728712e26d996200703c15254a8/src/RequestsLibrary/RequestsKeywords.py#L460-L477", "method_name": "to_json", "code": "def to_json(self, content, pretty_print=False):\n         if PY3:\n             if isinstance(content, bytes):\n                 content = content.decode(encoding=\"utf-8\")\n         if pretty_print:\n             json_ = self._json_pretty_print(content)\n         else:\n             json_ = json.loads(content)\n         logger.info(\"To JSON using : content=%s \" % (content))\n         logger.info(\"To JSON using : pretty_print=%s \" % (pretty_print))\n         return json_", "query": "pretty print json"}
{"id": "https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L471-L491", "method_name": "findAllSubstrings", "code": "def findAllSubstrings(string, substring):\n     start = 0\n     positions = []\n     while True:\n         start = string.find(substring, start)\n         if start == -1:\n             break\n         positions.append(start)\n         start += 1\n     return positions", "query": "positions of substrings in string"}
{"id": "https://github.com/nikcub/paths/blob/2200b85273d07d7a3c8b15ceb3b03cbb5c11439a/paths/__init__.py#L85-L107", "method_name": "find_executable", "code": "def find_executable(executable, path=None):\n   if path is None:\n     path = os.environ[\"PATH\"]\n   paths = path.split(os.pathsep)\n   base, ext = os.path.splitext(executable)\n   if (sys.platform == \"win32\" or os.name == \"os2\") and (ext != \".exe\"):\n     executable = executable + \".exe\"\n   if not os.path.isfile(executable):\n     for p in paths:\n       f = os.path.join(p, executable)\n       if os.path.isfile(f):\n         return f\n     return None\n   else:\n     return executable", "query": "get executable path"}
{"id": "https://github.com/qacafe/cdrouter.py/blob/aacf2c6ab0b987250f7b1892f4bba14bb2b7dbe5/cdrouter/cdrouter.py#L282-L288", "method_name": "list", "code": "def list(self, base, filter=None, type=None, sort=None, limit=None, page=None, format=None): \n         if sort != None:\n             if not isinstance(sort, list):\n                 sort = [sort]\n             sort = \",\".join(sort)\n         return self.get(base, params={\"filter\": filter, \"type\": type, \"sort\": sort, \"limit\": limit,\n                                       \"page\": page, \"format\": format})", "query": "sort string list"}
{"id": "https://github.com/buildinspace/peru/blob/76e4012c6c34e85fb53a4c6d85f4ac3633d93f77/peru/resources/plugins/curl/curl_plugin.py#L117-L137", "method_name": "extract_zip", "code": "def extract_zip(archive_path, dest):\n     with zipfile.ZipFile(archive_path) as z:\n         validate_filenames(z.namelist())\n         z.extractall(dest)\n         for info in z.filelist:\n             if not info.filename.endswith(\"/\"):\n                 mode = (info.external_attr >> 16) & 0o777\n                 if mode & stat.S_IXUSR:\n                     os.chmod(os.path.join(dest, info.filename), 0o755)", "query": "extract zip file recursively"}
{"id": "https://github.com/BerkeleyAutomation/perception/blob/03d9b37dd6b66896cdfe173905c9413c8c3c5df6/perception/image.py#L252-L291", "method_name": "from_array", "code": "def from_array(x, frame=\"unspecified\"):\n         if not Image.can_convert(x):\n             raise ValueError(\"Cannot convert array to an Image!\")\n         dtype = x.dtype\n         height = x.shape[0]\n         width = x.shape[1]\n         channels = 1\n         if len(x.shape) == 3:\n             channels = x.shape[2]\n         if dtype == np.uint8:\n             if channels == 1:\n                 if np.any((x % BINARY_IM_MAX_VAL) > 0):\n                     return GrayscaleImage(x, frame)\n                 return BinaryImage(x, frame)\n             elif channels == 3:\n                 return ColorImage(x, frame)\n             else:\n                 raise ValueError(\n                     \"No available image conversion for uint8 array with 2 channels\")\n         elif dtype == np.uint16:\n             if channels != 1:\n                 raise ValueError(\n                     \"No available image conversion for uint16 array with 2 or 3 channels\")\n             return GrayscaleImage(x, frame)\n         elif dtype == np.float32 or dtype == np.float64:\n             if channels == 1:\n                 return DepthImage(x, frame)\n             elif channels == 2:\n                 return GdImage(x, frame)\n             elif channels == 3:\n                 logging.warning(\"Converting float array to uint8\")\n                 return ColorImage(x.astype(np.uint8), frame)\n             return RgbdImage(x, frame)\n         else:\n             raise ValueError(\n                 \"Conversion for dtype %s not supported!\" %\n                 (str(dtype)))", "query": "converting uint8 array to image"}
{"id": "https://github.com/rueckstiess/mtools/blob/a6a22910c3569c0c8a3908660ca218a4557e4249/mtools/mplotqueries/plottypes/base_type.py#L79-L91", "method_name": "group", "code": "def group(self):\n         if hasattr(self, \"group_by\"):\n             group_by = self.group_by\n         else:\n             group_by = self.default_group_by\n             if self.args[\"group\"] is not None:\n                 group_by = self.args[\"group\"]\n         self.groups = Grouping(self.logevents, group_by)\n         self.groups.move_items(None, \"others\")\n         self.groups.sort_by_size(group_limit=self.args[\"group_limit\"],\n                                  discard_others=self.args[\"no_others\"])", "query": "group by count"}
{"id": "https://github.com/ajyoon/blur/blob/25fcf083af112bb003956a7a7e1c6ff7d8fef279/blur/rand.py#L252-L300", "method_name": "normal_distribution", "code": "def normal_distribution(mean, variance,\n                         minimum=None, maximum=None, weight_count=23):\n     standard_deviation = math.sqrt(variance)\n     min_x = (standard_deviation * -5) + mean\n     max_x = (standard_deviation * 5) + mean\n     step = (max_x - min_x) / weight_count\n     current_x = min_x\n     weights = []\n     while current_x < max_x:\n         weights.append(\n             (current_x, _normal_function(current_x, mean, variance))\n         )\n         current_x += step\n     if minimum is not None or maximum is not None:\n         return bound_weights(weights, minimum, maximum)\n     else:\n         return weights", "query": "normal distribution"}
{"id": "https://github.com/PSPC-SPAC-buyandsell/von_anchor/blob/78ac1de67be42a676274f4bf71fe12f66e72f309/von_anchor/frill.py#L30-L45", "method_name": "ppjson", "code": "def ppjson(dumpit: Any, elide_to: int = None) -> str:\n     if elide_to is not None:\n         elide_to = max(elide_to, 3) \n     try:\n         rv = json.dumps(json.loads(dumpit) if isinstance(dumpit, str) else dumpit, indent=4)\n     except TypeError:\n         rv = \"{}\".format(pformat(dumpit, indent=4, width=120))\n     return rv if elide_to is None or len(rv) <= elide_to else \"{}...\".format(rv[0 : elide_to - 3])", "query": "pretty print json"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/proc_poscar.py#L7-L21", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Manipulates VASP POSCAR files\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\" )\n     parser.add_argument( \"-l\", \"--label\", type=int, choices=[ 1, 4 ], help=\"label coordinates with atom name at position {1,4}\" )\n     parser.add_argument( \"-c\", \"--coordinates-only\", help=\"only output coordinates\", action=\"store_true\" )\n     parser.add_argument( \"-t\", \"--coordinate-type\", type=str, choices=[ \"c\", \"cartesian\", \"d\", \"direct\" ], default=\"direct\", help=\"specify coordinate type for output {(c)artesian (d)irect} [default = (d)irect]\" )\n     parser.add_argument( \"-g\", \"--group\", help=\"group atoms within supercell\", action=\"store_true\" )\n     parser.add_argument( \"-s\", \"--supercell\", type=int, nargs=3, metavar=( \"h\", \"k\", \"l\" ), help=\"construct supercell by replicating (h,k,l) times along [a b c]\" )\n     parser.add_argument( \"-b\", \"--bohr\", action=\"store_true\", help=\"assumes the input file is in Angstrom, and converts everything to bohr\")\n     parser.add_argument( \"-n\", \"--number-atoms\", action=\"store_true\", help=\"label coordinates with atom number\" )\n     parser.add_argument( \"--scale\", action=\"store_true\", help=\"scale the lattice parameters by the scaling factor\" )\n     parser.add_argument( \"--selective\", choices=[ \"T\", \"F\" ], help=\"generate Selective Dynamics POSCAR with all values set to T / F\" )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/pkkid/python-plexapi/blob/9efbde96441c2bfbf410eacfb46e811e108e8bbc/plexapi/settings.py#L132-L139", "method_name": "_getEnumValues", "code": "def _getEnumValues(self, data):\n         enumstr = data.attrib.get(\"enumValues\")\n         if not enumstr:\n             return None\n         if \":\" in enumstr:\n             return {self._cast(k): v for k, v in [kv.split(\":\") for kv in enumstr.split(\" \")]}\n         return enumstr.split(\" \")", "query": "get name of enumerated value"}
{"id": "https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/tune/automlboard/common/utils.py#L58-L92", "method_name": "parse_multiple_json", "code": "def parse_multiple_json(json_file, offset=None):\n     json_info_list = []\n     if not os.path.exists(json_file):\n         return json_info_list\n     try:\n         with open(json_file, \"r\") as f:\n             if offset:\n                 f.seek(offset)\n             for line in f:\n                 if line[-1] != \"\n\":\n                     break\n                 json_info = json.loads(line)\n                 json_info_list.append(json_info)\n                 offset += len(line)\n     except BaseException as e:\n         logging.error(e.message)\n     return json_info_list, offset", "query": "parse json file"}
{"id": "https://github.com/jborean93/smbprotocol/blob/d8eb00fbc824f97d0f4946e3f768c5e6c723499a/smbprotocol/structure.py#L775-L785", "method_name": "_to_string", "code": "def _to_string(self):\n         enum_name = None\n         value = self._get_calculated_value(self.value)\n         for enum, enum_value in vars(self.enum_type).items():\n             if value == enum_value:\n                 enum_name = enum\n                 break\n         if enum_name is None:\n             return \"(%d) UNKNOWN_ENUM\" % value\n         else:\n             return \"(%d) %s\" % (value, enum_name)", "query": "get name of enumerated value"}
{"id": "https://github.com/etcher-be/elib_run/blob/c9d8ba9f067ab90c5baa27375a92b23f1b97cdde/elib_run/_find_exe.py#L18-L62", "method_name": "find_executable", "code": "def find_executable(executable: str, *paths: str) -> typing.Optional[Path]:\n     if not executable.endswith(\".exe\"):\n         executable = f\"{executable}.exe\"\n     if executable in _KNOWN_EXECUTABLES:\n         return _KNOWN_EXECUTABLES[executable]\n     output = f\"{executable}\"\n     if not paths:\n         path = os.environ[\"PATH\"]\n         paths = tuple([str(Path(sys.exec_prefix, \"Scripts\").absolute())] + path.split(os.pathsep))\n     executable_path = Path(executable).absolute()\n     if not executable_path.is_file():\n         for path_ in paths:\n             executable_path = Path(path_, executable).absolute()\n             if executable_path.is_file():\n                 break\n         else:\n             _LOGGER.error(\"%s -> not found\", output)\n             return None\n     _KNOWN_EXECUTABLES[executable] = executable_path\n     _LOGGER.info(\"%s -> %s\", output, str(executable_path))\n     return executable_path", "query": "get executable path"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L72-L76", "method_name": "aes_cbc_encrypt", "code": "def aes_cbc_encrypt(plain_text: bytes, key: bytes, iv: bytes = b\"\"):\n         if len(iv) == 0:\n             iv = AESHandler.generate_iv()\n         cipher = AES.new(key=key, mode=AES.MODE_CBC, iv=iv)\n         return cipher.IV, cipher.encrypt(pad(plain_text, AES.block_size))", "query": "aes encryption"}
{"id": "https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/mover.py#L203-L208", "method_name": "get_ip", "code": "def get_ip(self):\n         if self._ip is not None:\n             ret = self._ip\n         else:\n             ret = self.ip_addr\n         return ret", "query": "get current ip address"}
{"id": "https://github.com/adsabs/adsutils/blob/fb9d6b4f6ed5e6ca19c552efc3cdd6466c587fdb/adsutils/Unicode.py#L153-L159", "method_name": "encode", "code": "def encode(self,str):\n         data = self.u2ent(str)\n         data = data.replace(\"&\", \"&amp;\")\n         data = data.replace(\"<\", \"&lt;\")\n         data = data.replace(\"\\\"\", \"&quot;\")\n         data = data.replace(\">\", \"&gt;\")\n         return data", "query": "html encode string"}
{"id": "https://github.com/mailgun/talon/blob/cdd84563dd329c4f887591807870d10015e0c7a7/talon/quotations.py#L207-L216", "method_name": "extract_from", "code": "def extract_from(msg_body, content_type=\"text/plain\"):\n     try:\n         if content_type == \"text/plain\":\n             return extract_from_plain(msg_body)\n         elif content_type == \"text/html\":\n             return extract_from_html(msg_body)\n     except Exception:\n         log.exception(\"ERROR extracting message\")\n     return msg_body", "query": "extract data from html content"}
{"id": "https://github.com/kencochrane/django-defender/blob/e3e547dbb83235e0d564a6d64652c7df00412ff2/defender/utils.py#L41-L50", "method_name": "get_ip", "code": "def get_ip(request):\n     if config.BEHIND_REVERSE_PROXY:\n         ip_address = request.META.get(config.REVERSE_PROXY_HEADER, \"\")\n         ip_address = ip_address.split(\",\", 1)[0].strip()\n         if ip_address == \"\":\n             ip_address = get_ip_address_from_request(request)\n     else:\n         ip_address = get_ip_address_from_request(request)\n     return ip_address", "query": "get current ip address"}
{"id": "https://github.com/mathiasertl/django-ca/blob/976d7ea05276320f20daed2a6d59c8f5660fe976/ca/django_ca/utils.py#L211-L222", "method_name": "int_to_hex", "code": "def int_to_hex(i):\n     s = hex(i)[2:].upper()\n     if six.PY2 is True and isinstance(i, long):  \n         s = s[:-1]\n     return add_colons(s)", "query": "convert decimal to hex"}
{"id": "https://github.com/pereorga/csvshuf/blob/70fdd4f512ef980bffe9cc51bfe59fea116d7c2f/csvshuf/csvshuf.py#L19-L24", "method_name": "shuffle_sattolo", "code": "def shuffle_sattolo(items):\n     _randrange = random.randrange\n     for i in reversed(range(1, len(items))):\n         j = _randrange(i)  \n         items[j], items[i] = items[i], items[j]", "query": "randomly extract x items from a list"}
{"id": "https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/controllers/monsoon.py#L562-L578", "method_name": "from_text_file", "code": "def from_text_file(file_path):\n         results = []\n         with io.open(file_path, \"r\", encoding=\"utf-8\") as f:\n             data_strs = f.read().split(MonsoonData.delimiter)\n             for data_str in data_strs:\n                 results.append(MonsoonData.from_string(data_str))\n         return results", "query": "extracting data from a text file"}
{"id": "https://github.com/oauthlib/oauthlib/blob/30321dd3c0ca784d3508a1970cf90d9f76835c79/oauthlib/common.py#L83-L89", "method_name": "urlencode", "code": "def urlencode(params):\n     utf8_params = encode_params_utf8(params)\n     urlencoded = _urlencode(utf8_params)\n     if isinstance(urlencoded, unicode_type):  \n         return urlencoded\n     else:\n         return urlencoded.decode(\"utf-8\")", "query": "encode url"}
{"id": "https://github.com/rjrequina/Cebuano-POS-Tagger/blob/fb1b46448c40b23ac8e8e373f073f1f8934b6dc6/eval/evaluator.py#L72-L92", "method_name": "confusion_matrix", "code": "def confusion_matrix(actual=[], pred=[]):\n     idx = {\n         \"ADJ\" : 0,\n         \"ADV\" : 1,\n         \"CONJ\": 2,\n         \"DET\" : 3,\n         \"NOUN\": 4,\n         \"NUM\" : 5,\n         \"OTH\" : 6,\n         \"PART\": 7,\n         \"PRON\": 8,\n         \"SYM\" : 9,\n         \"VERB\": 10\n     }\n     matrix = [[0 for i in range(11)] for j in range(11)]\n     for i in range(0, len(actual)):\n        matrix[idx[actual[i]]][idx[pred[i]]] += 1\n     return matrix", "query": "confusion matrix"}
{"id": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeregression.py#L292-L310", "method_name": "regression", "code": "def regression(self, slope=None):\n         self._calculate_averages()\n         clock_model = base_regression(self.tree.root.Q, slope)\n         clock_model[\"r_val\"] = self.explained_variance()\n         return clock_model", "query": "linear regression"}
{"id": "https://github.com/rwl/pylon/blob/916514255db1ae1661406f0283df756baf960d14/contrib/pylontk.py#L377-L381", "method_name": "on_excel", "code": "def on_excel(self):\n         from pylon.io.excel import ExcelWriter\n         filename = asksaveasfilename(filetypes=[(\"Excel file\", \".xls\")])\n         if filename:\n             ExcelWriter(self.case).write(filename)", "query": "export to excel"}
{"id": "https://github.com/bakwc/JamSpell/blob/bfdfd889436df4dbcd9ec86b20d2baeb815068bd/evaluate/utils.py#L25-L28", "method_name": "loadText", "code": "def loadText(fname):\n     with codecs.open(fname, \"r\", \"utf-8\") as f:\n         data = f.read()\n         return normalize(data).split()", "query": "extracting data from a text file"}
{"id": "https://github.com/JesseAldridge/clipmon/blob/5e16a31cf38f64db2d9b7a490968f922aca57512/clipmon.py#L19-L38", "method_name": "test_replacements", "code": "def test_replacements(clip_str, path_exists):\n   replaced_str = clip_str\n   for find_regex, replace_str in conf.find_replace_map:\n     replaced_str = re.sub(find_regex, replace_str, replaced_str)\n   match = re.search(\n   if match and path_exists(os.path.expanduser(match.group(1))):\n     return \":\".join([match.group(1), match.group(2)])\n   match = re.search(\n   if match and path_exists(os.path.expanduser(match.group(1))):\n     return \":\".join([match.group(1), match.group(2)])", "query": "replace in file"}
{"id": "https://github.com/mirukan/lunafind/blob/77bdfe02df98a7f74d0ae795fee3b1729218995d/lunafind/order.py#L53-L79", "method_name": "sort", "code": "def sort(posts: List[Post], by: str) -> List[Post]:\n     by_val  = by.replace(\"asc_\", \"\").replace(\"desc_\", \"\")\n     in_dict = (ORDER_NUM   if by_val in ORDER_NUM   else\n                ORDER_DATE  if by_val in ORDER_DATE  else\n                ORDER_FUNCS if by_val in ORDER_FUNCS else None)\n     if not in_dict:\n         raise ValueError(\n             f\"Got {by_val!r} as ordering method, must be one of: %s\" %\n             \", \".join(set(ORDER_NUM)   set(ORDER_DATE)   set(ORDER_FUNCS))\n         )\n     if in_dict == ORDER_FUNCS:\n         posts.sort(key=ORDER_FUNCS[by], reverse=(by != \"random\"))\n         return posts\n     by_full = by if by.startswith(\"asc_\") or by.startswith(\"desc_\") else \\\n               f\"%s_{by}\" % in_dict[by][0]\n     def sort_key(post: Post) -> int:\n         key = in_dict[by_val][1]\n         key = post.info[key] if not callable(key) else key(post.info)\n         return pend.parse(key) if in_dict == ORDER_DATE else key\n     posts.sort(key=sort_key, reverse=by_full.startswith(\"desc_\"))\n     return posts", "query": "sort string list"}
{"id": "https://github.com/census-instrumentation/opencensus-python/blob/992b223f7e34c5dcb65922b7d5c827e7a1351e7d/opencensus/trace/propagation/binary_format.py#L95-L136", "method_name": "from_header", "code": "def from_header(self, binary):\n         if binary is None:\n             return span_context_module.SpanContext(from_header=False)\n         try:\n             data = Header._make(struct.unpack(BINARY_FORMAT, binary))\n         except struct.error:\n             logging.warning(\n                 \"Cannot parse the incoming binary data {}, \"\n                 \"wrong format. Total bytes length should be {}.\".format(\n                     binary, FORMAT_LENGTH\n                 )\n             )\n             return span_context_module.SpanContext(from_header=False)\n         trace_id = str(binascii.hexlify(data.trace_id).decode(UTF8))\n         span_id = str(binascii.hexlify(data.span_id).decode(UTF8))\n         trace_options = TraceOptions(data.trace_option)\n         span_context = span_context_module.SpanContext(\n                 trace_id=trace_id,\n                 span_id=span_id,\n                 trace_options=trace_options,\n                 from_header=True)\n         return span_context", "query": "parse binary file to custom class"}
{"id": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/trendmicroav.py#L144-L194", "method_name": "_ConvertToTimestamp", "code": "def _ConvertToTimestamp(self, date, time):\n     if len(date) != 8:\n       raise ValueError(\n           \"Unsupported length of date string: {0!s}\".format(repr(date)))\n     if len(time) < 3 or len(time) > 4:\n       raise ValueError(\n           \"Unsupported length of time string: {0!s}\".format(repr(time)))\n     try:\n       year = int(date[:4], 10)\n       month = int(date[4:6], 10)\n       day = int(date[6:8], 10)\n     except (TypeError, ValueError):\n       raise ValueError(\"Unable to parse date string: {0!s}\".format(repr(date)))\n     try:\n       hour = int(time[:-2], 10)\n       minutes = int(time[-2:], 10)\n     except (TypeError, ValueError):\n       raise ValueError(\"Unable to parse time string: {0!s}\".format(repr(date)))\n     time_elements_tuple = (year, month, day, hour, minutes, 0)\n     date_time = dfdatetime_time_elements.TimeElements(\n         time_elements_tuple=time_elements_tuple)\n     date_time.is_local_time = True\n     date_time._precision = dfdatetime_definitions.PRECISION_1_MINUTE  \n     return date_time", "query": "string to date"}
{"id": "https://github.com/openfisca/openfisca-core/blob/92ce9396e29ae5d9bac5ea604cfce88517c6b35c/openfisca_core/variables.py#L427-L433", "method_name": "default_array", "code": "def default_array(self, array_size):\n         array = np.empty(array_size, dtype = self.dtype)\n         if self.value_type == Enum:\n             array.fill(self.default_value.index)\n             return EnumArray(array, self.possible_values)\n         array.fill(self.default_value)\n         return array", "query": "empty array"}
{"id": "https://github.com/teepark/greenhouse/blob/8fd1be4f5443ba090346b5ec82fdbeb0a060d956/greenhouse/io/sockets.py#L346-L377", "method_name": "recvfrom", "code": "def recvfrom(self, bufsize, flags=0):\n         with self._registered(\"re\"):\n             while 1:\n                 if self._closed:\n                     raise socket.error(errno.EBADF, \"Bad file descriptor\")\n                 try:\n                     return self._sock.recvfrom(bufsize, flags)\n                 except socket.error, exc:\n                     if not self._blocking or exc[0] not in _BLOCKING_OP:\n                         raise\n                     sys.exc_clear()\n                     if self._readable.wait(self.gettimeout()):\n                         raise socket.timeout(\"timed out\")\n                     if scheduler.state.interrupted:\n                         raise IOError(errno.EINTR, \"interrupted system call\")", "query": "socket recv timeout"}
{"id": "https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/tag.py#L155-L159", "method_name": "innerHTML", "code": "def innerHTML(self) -> str:\n         if self._inner_element:\n             return self._inner_element.innerHTML\n         return super().innerHTML", "query": "get inner html"}
{"id": "https://github.com/teitei-tk/Simple-AES-Cipher/blob/cb4bc69d762397f3a26f6b009e2b7fa5d706ed5f/simple_aes_cipher/cipher.py#L15-L19", "method_name": "encrypt", "code": "def encrypt(self, raw, mode=AES.MODE_CBC):\n         raw = self._pad(raw, AES.block_size)\n         iv = Random.new().read(AES.block_size)\n         cipher = AES.new(self.key, mode, iv)\n         return base64.b64encode(iv + cipher.encrypt(raw)).decode(\"utf-8\")", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/models.py#L48-L53", "method_name": "get_table", "code": "def get_table(self, database_name, table_name):\n         database = self.get_database(database_name)\n         try:\n             return database.tables[table_name]\n         except KeyError:\n             raise TableNotFoundException(table_name)", "query": "get database table name"}
{"id": "https://github.com/joelfrederico/SciSalt/blob/7bf57c49c7dde0a8b0aa337fbd2fbd527ce7a67f/scisalt/facettools/print2elog.py#L13-L23", "method_name": "_copy_file", "code": "def _copy_file(filepath, fulltime):\n     if filepath is None:\n         filepath_out = \"\"\n     else:\n         filename  = _os.path.basename(filepath)\n         root, ext = _os.path.splitext(filename)\n         filepath_out = fulltime + ext\n         copypath = _os.path.join(basedir, filepath_out)\n         _shutil.copyfile(filepath, copypath)\n     return filepath_out", "query": "copying a file to a path"}
{"id": "https://github.com/swharden/SWHLab/blob/a86c3c65323cec809a4bd4f81919644927094bf5/swhlab/indexing/indexing.py#L239-L256", "method_name": "html_index", "code": "def html_index(self,launch=True):\n         html=\"<h1>MENU</h1>\"\n         htmlFiles=[x for x in self.files2 if x.endswith(\".html\")]\n         for htmlFile in cm.abfSort(htmlFiles):\n             if not htmlFile.endswith(\"_basic.html\"):\n                 continue\n             name=htmlFile.split(\"_\")[0]\n             if name in self.groups.keys():\n                 html+=\"<a href=\"%s\" target=\"content\">%s</a> \"%(htmlFile,name)\n                 html+=\"<span style=\"color: \n                 html+=\"[<a href=\"%s\" target=\"content\">int</a>]\"%(name+\"_plot.html\")\n                 html+=\"</span>\"\n                 html+=\"<br>\"\n         style.save(html,os.path.abspath(self.folder2+\"/index_menu.html\"))\n         html=\"<h1>SPLASH</h1>\"\n         style.save(html,os.path.abspath(self.folder2+\"/index_splash.html\"))\n         style.frames(os.path.abspath(self.folder2+\"/index.html\"),launch=launch)\n         return", "query": "output to html file"}
{"id": "https://github.com/alexhayes/django-pdfkit/blob/02774ae2cb67d05dd5e4cb50661c56464ebb2413/django_pdfkit/views.py#L55-L74", "method_name": "render_pdf", "code": "def render_pdf(self, *args, **kwargs):\n         html = self.render_html(*args, **kwargs)\n         options = self.get_pdfkit_options()\n         if \"debug\" in self.request.GET and settings.DEBUG:\n             options[\"debug-javascript\"] = 1\n         kwargs = {}\n         wkhtmltopdf_bin = os.environ.get(\"WKHTMLTOPDF_BIN\")\n         if wkhtmltopdf_bin:\n             kwargs[\"configuration\"] = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_bin)\n         pdf = pdfkit.from_string(html, False, options, **kwargs)\n         return pdf", "query": "convert html to pdf"}
{"id": "https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/tag.py#L148-L152", "method_name": "html", "code": "def html(self) -> str:\n         if self._inner_element:\n             return self.start_tag + self._inner_element.html + self.end_tag\n         return super().html", "query": "get inner html"}
{"id": "https://github.com/nigma/django-easy-pdf/blob/327605b91a445b453d8969b341ef74b12ab00a83/easy_pdf/rendering.py#L51-L78", "method_name": "html_to_pdf", "code": "def html_to_pdf(content, encoding=\"utf-8\",\n                 link_callback=fetch_resources, **kwargs):\n     src = BytesIO(content.encode(encoding))\n     dest = BytesIO()\n     pdf = pisa.pisaDocument(src, dest, encoding=encoding,\n                             link_callback=link_callback, **kwargs)\n     if pdf.err:\n         logger.error(\"Error rendering PDF document\")\n         for entry in pdf.log:\n             if entry[0] == xhtml2pdf.default.PML_ERROR:\n                 logger_x2p.error(\"line %s, msg: %s, fragment: %s\", entry[1], entry[2], entry[3])\n         raise PDFRenderingError(\"Errors rendering PDF\", content=content, log=pdf.log)\n     if pdf.warn:\n         for entry in pdf.log:\n             if entry[0] == xhtml2pdf.default.PML_WARNING:\n                 logger_x2p.warning(\"line %s, msg: %s, fragment: %s\", entry[1], entry[2], entry[3])\n     return dest.getvalue()", "query": "convert html to pdf"}
{"id": "https://github.com/why2pac/dp-tornado/blob/a5948f5693f6ee2d9bab31f611fedc074e1caa96/dp_tornado/helper/datetime/date.py#L74-L76", "method_name": "yyyymmdd", "code": "def yyyymmdd(self, auto=None, datetime=None, timezone=None, timestamp=None, ms=False, concat=\"\"):\n        datetime = self.convert(auto=auto, datetime=datetime, timezone=timezone, timestamp=timestamp, ms=ms)\n        return \"%04d%s%02d%s%02d\" % (datetime.year, concat, datetime.month, concat, datetime.day)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/softlayer/softlayer-python/blob/9f181be08cc3668353b05a6de0cb324f52cff6fa/SoftLayer/CLI/vpn/ipsec/detail.py#L63-L84", "method_name": "_get_address_translations_table", "code": "def _get_address_translations_table(address_translations):\n     table = formatting.Table([\"id\",\n                               \"static IP address\",\n                               \"static IP address id\",\n                               \"remote IP address\",\n                               \"remote IP address id\",\n                               \"note\"])\n     for address_translation in address_translations:\n         table.add_row([address_translation.get(\"id\", \"\"),\n                        address_translation.get(\"internalIpAddressRecord\", {})\n                        .get(\"ipAddress\", \"\"),\n                        address_translation.get(\"internalIpAddressId\", \"\"),\n                        address_translation.get(\"customerIpAddressRecord\", {})\n                        .get(\"ipAddress\", \"\"),\n                        address_translation.get(\"customerIpAddressId\", \"\"),\n                        address_translation.get(\"notes\", \"\")])\n     return table", "query": "get current ip address"}
{"id": "https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/optimization/optimizer.py#L36-L61", "method_name": "optimize", "code": "def optimize(self, x0, f=None, df=None, f_df=None):\n         import scipy.optimize\n         if f_df is None and df is not None: f_df = lambda x: float(f(x)), df(x)\n         if f_df is not None:\n             def _f_df(x):\n                 return f(x), f_df(x)[1][0]\n         if f_df is None and df is None:\n             res = scipy.optimize.fmin_l_bfgs_b(f, x0=x0, bounds=self.bounds,approx_grad=True, maxiter=self.maxiter)\n         else:\n             res = scipy.optimize.fmin_l_bfgs_b(_f_df, x0=x0, bounds=self.bounds, maxiter=self.maxiter)\n         if res[2][\"task\"] == b\"ABNORMAL_TERMINATION_IN_LNSRCH\":\n             result_x  = np.atleast_2d(x0)\n             result_fx =  np.atleast_2d(f(x0))\n         else:\n             result_x = np.atleast_2d(res[0])\n             result_fx = np.atleast_2d(res[1])\n         return result_x, result_fx", "query": "nelder mead optimize"}
{"id": "https://github.com/modin-project/modin/blob/5b77d242596560c646b8405340c9ce64acb183cb/modin/backends/pandas/query_compiler.py#L1251-L1263", "method_name": "median", "code": "def median(self, **kwargs):\n         if self._is_transposed:\n             kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n             return self.transpose().median(**kwargs)\n         axis = kwargs.get(\"axis\", 0)\n         func = self._build_mapreduce_func(pandas.DataFrame.median, **kwargs)\n         return self._full_axis_reduce(axis, func)", "query": "deducting the median from each column"}
{"id": "https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/expr/api.py#L1943-L1962", "method_name": "_string_substr", "code": "def _string_substr(self, start, length=None):\n     op = ops.Substring(self, start, length)\n     return op.to_expr()", "query": "positions of substrings in string"}
{"id": "https://github.com/molmod/molmod/blob/a7b5b4364ed514ad4c465856c05b5eda1cb561e0/molmod/io/cp2k.py#L204-L213", "method_name": "readline", "code": "def readline(self, f):\n         while True:\n             line = f.readline()\n             if len(line) == 0:\n                 raise EOFError\n             line = line[:line.find(\"\n             line = line.strip()\n             if len(line) > 0:\n                 return line", "query": "read text file line by line"}
{"id": "https://github.com/tmoerman/arboreto/blob/3ff7b6f987b32e5774771751dea646fa6feaaa52/arboreto/core.py#L105-L141", "method_name": "fit_model", "code": "def fit_model(regressor_type,\n               regressor_kwargs,\n               tf_matrix,\n               target_gene_expression,\n               early_stop_window_length=EARLY_STOP_WINDOW_LENGTH,\n               seed=DEMON_SEED):\n     regressor_type = regressor_type.upper()\n     assert tf_matrix.shape[0] == len(target_gene_expression)\n     def do_sklearn_regression():\n         regressor = SKLEARN_REGRESSOR_FACTORY[regressor_type](random_state=seed, **regressor_kwargs)\n         with_early_stopping = is_oob_heuristic_supported(regressor_type, regressor_kwargs)\n         if with_early_stopping:\n             regressor.fit(tf_matrix, target_gene_expression, monitor=EarlyStopMonitor(early_stop_window_length))\n         else:\n             regressor.fit(tf_matrix, target_gene_expression)\n         return regressor\n     if is_sklearn_regressor(regressor_type):\n         return do_sklearn_regression()\n     else:\n         raise ValueError(\"Unsupported regressor type: {0}\".format(regressor_type))", "query": "linear regression"}
{"id": "https://github.com/vstinner/perf/blob/cf096c0c0c955d0aa1c893847fa6393ba4922ada/perf/_utils.py#L481-L484", "method_name": "median_abs_dev", "code": "def median_abs_dev(values):\n     median = float(statistics.median(values))\n     return statistics.median([abs(median - sample) for sample in values])", "query": "deducting the median from each column"}
{"id": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfobjects/pdftext.py#L165-L172", "method_name": "_underline", "code": "def _underline(self): \n         up = self.font.underline_position \n         ut = self.font.underline_thickness \n         w = self.font._string_width(self.text) \n         s = \"%.2f %.2f %.2f %.2f re f\" % (self.cursor.x, \n             self.cursor.y_prime - up, w, ut) \n         return s", "query": "underline text in label widget"}
{"id": "https://github.com/NoviceLive/intellicoder/blob/6cac5ebfce65c370dbebe47756a1789b120ef982/intellicoder/transformers.py#L59-L66", "method_name": "replace_source", "code": "def replace_source(self, body, name):\n         logging.debug(_(\"Processing function body: %s\"), name)\n         replaced = re.sub(\n             self.FUNC_NAME_RE, self._func_replacer, body)\n         replaced = re.sub(\n             self.STR_LITERAL_RE, self._string_replacer, replaced)\n         return self._build_strings() + replaced\n         return replaced", "query": "replace in file"}
{"id": "https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/osid/markers.py#L309-L318", "method_name": "is_effective", "code": "def is_effective(self):\n         now = DateTime.utcnow()\n         return self.get_start_date() <= now and self.get_end_date() >= now", "query": "get current date"}
{"id": "https://github.com/jobec/rfc5424-logging-handler/blob/9c4f669c5e54cf382936cd950e2204caeb6d05f0/rfc5424logging/handler.py#L277-L281", "method_name": "get_procid", "code": "def get_procid(self, record):\n         procid = getattr(record, \"procid\", self.procid)\n         if procid is None or procid == \"\":\n             procid = getattr(record, \"process\", NILVALUE)\n         return self.filter_printusascii(str(procid))", "query": "get current process id"}
{"id": "https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149", "method_name": "_dt_to_epoch", "code": "def _dt_to_epoch(self, dt):\n         if PY2:\n             time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n             return int(time_delta.total_seconds())\n         else:\n             return int(dt.timestamp())", "query": "convert a utc time to epoch"}
{"id": "https://github.com/City-of-Helsinki/django-munigeo/blob/a358f0dd0eae3add660b650abec2acb4dd86d04c/munigeo/api.py#L222-L241", "method_name": "parse_lat_lon", "code": "def parse_lat_lon(query_params):\n     lat = query_params.get(\"lat\", None)\n     lon = query_params.get(\"lon\", None)\n     if not lat and not lon:\n         return None\n     if not lat or not lon:\n         raise ParseError(\"you must supply both \"lat\" and \"lon\"\")\n     try:\n         lat = float(lat)\n         lon = float(lon)\n     except ValueError:\n         raise ParseError(\"\"lat\" and \"lon\" must be floating point numbers\")\n     point = Point(lon, lat, srid=DEFAULT_SRID)\n     if DEFAULT_SRID != DATABASE_SRID:\n         ct = CoordTransform(SpatialReference(DEFAULT_SRID),\n                             SpatialReference(DATABASE_SRID))\n         point.transform(ct)\n     return point", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/utils/serialization.py#L47-L60", "method_name": "deserialize", "code": "def deserialize(s):\n     s = s.strip()\n     try:\n         doc = etree.fromstring(s)\n         if is_tmdd(doc):\n             from ..converter.tmdd import tmdd_to_json\n             return (tmdd_to_json(doc), \"json\")\n         return (doc, \"xml\")\n     except etree.XMLSyntaxError:\n         try:\n             return (json.loads(s), \"json\")\n         except ValueError:\n             raise Exception(\"Doesn\"t look like either JSON or XML\")", "query": "deserialize json"}
{"id": "https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/mothur.py#L391-L399", "method_name": "_set_WorkingDir", "code": "def _set_WorkingDir(self, path):\n         self._curr_working_dir = path\n         try:\n             mkdir(self.WorkingDir)\n         except OSError:\n             pass", "query": "set working directory"}
{"id": "https://github.com/ml4ai/delphi/blob/6d03d8aafeab99610387c51b89c99738ff2abbe3/delphi/translators/for2py/format.py#L99-L138", "method_name": "read_line", "code": "def read_line(self, line):\n         if not self._read_line_init:\n             self.init_read_line()\n         match = self._re.match(line)\n         assert match is not None, f\"Format mismatch (line = {line})\"\n         matched_values = []\n         for i in range(self._re.groups):\n             cvt_re = self._match_exps[i]\n             cvt_div = self._divisors[i]\n             cvt_fn = self._in_cvt_fns[i]\n             match_str = match.group(i + 1)\n             match0 = re.match(cvt_re, match_str)\n             if match0 is not None:\n                 if cvt_fn == \"float\":\n                     if \".\" in match_str:\n                         val = float(match_str)\n                     else:\n                         val = int(match_str) / cvt_div\n                 elif cvt_fn == \"int\":\n                     val = int(match_str)\n                 else:\n                     sys.stderr.write(\n                         f\"Unrecognized conversion function: {cvt_fn}\n\"\n                     )\n             else:\n                 sys.stderr.write(\n                     f\"Format conversion failed: {match_str}\n\"\n                 )\n             matched_values.append(val)\n         return tuple(matched_values)", "query": "read text file line by line"}
{"id": "https://github.com/markfinger/django-node/blob/a2f56bf027fd3c4cbc6a0213881922a50acae1d6/django_node/utils.py#L183-L193", "method_name": "decode_html_entities", "code": "def decode_html_entities(html):\n     if not html:\n         return html\n     for entity, char in six.iteritems(html_entity_map):\n         html = html.replace(entity, char)\n     return html", "query": "html entities replace"}
{"id": "https://github.com/pasztorpisti/json-cfg/blob/4627b14a92521ef8a39bbedaa7af8d380d406d07/src/jsoncfg/tree_python.py#L61-L69", "method_name": "default_number_converter", "code": "def default_number_converter(number_str):\n     is_int = (number_str.startswith(\"-\") and number_str[1:].isdigit()) or number_str.isdigit()\n     return int(number_str) if is_int else float(number_str)", "query": "convert string to number"}
{"id": "https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/server.py#L25-L53", "method_name": "maps_json", "code": "def maps_json():\n     map_sources = {\n         id: {\n                 \"id\": map_source.id,\n                 \"name\": map_source.name,\n                 \"folder\": map_source.folder,\n                 \"min_zoom\": map_source.min_zoom,\n                 \"max_zoom\": map_source.max_zoom,\n                 \"layers\": [\n                     {\n                         \"min_zoom\": layer.min_zoom,\n                         \"max_zoom\": layer.max_zoom,\n                         \"tile_url\": layer.tile_url.replace(\"$\", \"\"),\n                     } for layer in map_source.layers\n                     ]\n             } for id, map_source in app.config[\"mapsources\"].items()\n         }\n     return jsonify(map_sources)", "query": "map to json"}
{"id": "https://github.com/elifesciences/elife-tools/blob/4b9e38cbe485c61a4ed7cbd8970c6b318334fd86/elifetools/parseJATS.py#L2064-L2067", "method_name": "keywords_json", "code": "def keywords_json(soup, html_flag=True):\n     convert = lambda xml_string: xml_to_html(html_flag, xml_string)\n     return list(map(convert, full_keywords(soup)))", "query": "json to xml conversion"}
{"id": "https://github.com/AirtestProject/Poco/blob/2c559a586adf3fd11ee81cabc446d4d3f6f2d119/poco/utils/simplerpc/transport/tcp/main.py#L34-L40", "method_name": "recv", "code": "def recv(self):\n         try:\n             msg_bytes = self.c.recv()\n         except socket.timeout:\n             msg_bytes = b\"\"\n         return self.prot.input(msg_bytes)", "query": "socket recv timeout"}
{"id": "https://github.com/petermelias/valhalla/blob/b08d7a4a94fd8d85a6f5ea86200ec60ef4525e3d/valhalla/filters/strings.py#L90-L99", "method_name": "regex", "code": "def regex(regex, case=False, _value=None, *args, **kwargs):\n     if kwargs.get(\"case\"):\n         regex = re.compile(regex)\n     else:\n         regex = re.compile(regex, re.IGNORECASE)\n     if not regex.match(_value):\n         raise ValidationError(\"The _value must match the regex %s\" % regex)\n     return _value", "query": "regex case insensitive"}
{"id": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/helpers/utils.py#L43-L65", "method_name": "extract_zipdir", "code": "def extract_zipdir(zip_file):\n     if not os.path.exists(zip_file):\n         raise ValueError(\"{} does not exist\".format(zip_file))\n     directory = os.path.dirname(zip_file)\n     filename = os.path.basename(zip_file)\n     dirpath = os.path.join(directory, filename.replace(\".zip\", \"\"))\n     with zipfile.ZipFile(zip_file, \"r\", zipfile.ZIP_DEFLATED) as zipf:\n         zipf.extractall(dirpath)\n     return dirpath", "query": "extract zip file recursively"}
{"id": "https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/plugin_html.py#L177-L183", "method_name": "tag_to_dict", "code": "def tag_to_dict(html):\n     element = document_fromstring(html).xpath(\"//html/body/child::*\")[0]\n     attributes = dict(element.attrib)\n     attributes[\"text\"] = element.text_content()\n     return attributes", "query": "reading element from html - <td>"}
{"id": "https://github.com/TestInABox/stackInABox/blob/63ee457401e9a88d987f85f513eb512dcb12d984/stackinabox/util/requests_mock/core.py#L63-L75", "method_name": "get_reason_for_status", "code": "def get_reason_for_status(status_code):\n         if status_code in requests.status_codes.codes:\n             return requests.status_codes._codes[status_code][0].replace(\"_\",\n                                                                         \" \")\n         else:\n             return \"Unknown status code - {0}\".format(status_code)", "query": "get the description of a http status code"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L72-L76", "method_name": "aes_cbc_encrypt", "code": "def aes_cbc_encrypt(plain_text: bytes, key: bytes, iv: bytes = b\"\"):\n         if len(iv) == 0:\n             iv = AESHandler.generate_iv()\n         cipher = AES.new(key=key, mode=AES.MODE_CBC, iv=iv)\n         return cipher.IV, cipher.encrypt(pad(plain_text, AES.block_size))", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/openstax/cnx-epub/blob/f648a309eff551b0a68a115a98ddf7858149a2ea/cnxepub/epub.py#L349-L376", "method_name": "from_file", "code": "def from_file(cls, file):\n         opf_xml = etree.parse(file)\n         if hasattr(file, \"read\"):\n             name = os.path.basename(file.name)\n             root = os.path.abspath(os.path.dirname(file.name))\n         else:  \n             name = os.path.basename(file)\n             root = os.path.abspath(os.path.dirname(file))\n         parser = OPFParser(opf_xml)\n         manifest = opf_xml.xpath(\"/opf:package/opf:manifest/opf:item\",\n                                  namespaces=EPUB_OPF_NAMESPACES)\n         pkg_items = []\n         for item in manifest:\n             absolute_filepath = os.path.join(root, item.get(\"href\"))\n             properties = item.get(\"properties\", \"\").split()\n             is_navigation = \"nav\" in properties\n             media_type = item.get(\"media-type\")\n             pkg_items.append(Item.from_file(absolute_filepath,\n                                             media_type=media_type,\n                                             is_navigation=is_navigation,\n                                             properties=properties))\n         return cls(name, pkg_items, parser.metadata)", "query": "read properties file"}
{"id": "https://github.com/janpipek/physt/blob/6dd441b073514e7728235f50b2352d56aacf38d4/physt/examples/__init__.py#L12-L22", "method_name": "normal_h1", "code": "def normal_h1(size: int = 10000, mean: float = 0, sigma: float = 1) -> Histogram1D:\n     data = np.random.normal(mean, sigma, (size,))\n     return h1(data, name=\"normal\", axis_name=\"x\", title=\"1D normal distribution\")", "query": "normal distribution"}
{"id": "https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/operation/datashader.py#L1058-L1069", "method_name": "uint8_to_uint32", "code": "def uint8_to_uint32(self, element):\n         img = np.dstack([element.dimension_values(d, flat=False)\n                          for d in element.vdims])\n         if img.shape[2] == 3: \n             alpha = np.ones(img.shape[:2])\n             if img.dtype.name == \"uint8\":\n                 alpha = (alpha*255).astype(\"uint8\")\n             img = np.dstack([img, alpha])\n         if img.dtype.name != \"uint8\":\n             img = (img*255).astype(np.uint8)\n         N, M, _ = img.shape\n         return img.view(dtype=np.uint32).reshape((N, M))", "query": "converting uint8 array to image"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/noaa_lpd.py#L546-L556", "method_name": "__convert_num", "code": "def __convert_num(number):\n         try:\n             return float(number)\n         except ValueError as e:\n             logger_noaa_lpd.warn(\"convert_num: ValueError: {}\".format(e))\n             return number", "query": "convert string to number"}
{"id": "https://github.com/alejandroesquiva/AutomaticApiRest-PythonConnector/blob/90e55d5cf953dc8d7186fc6df82a6c6a089a3bbe/build/lib/aarpy/AARConnector.py#L36-L38", "method_name": "printJson", "code": "def printJson(self):\n        jsonobject = self.getJson()\n        print(json.dumps(jsonobject, indent=1, sort_keys=True))", "query": "pretty print json"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/excel.py#L1227-L1269", "method_name": "compile_geometry", "code": "def compile_geometry(lat, lon, elev):\n     logger_excel.info(\"enter compile_geometry\")\n     lat = _remove_geo_placeholders(lat)\n     lon = _remove_geo_placeholders(lon)\n     if len(lat) == 2 and len(lon) == 2:\n         logger_excel.info(\"found 4 coordinates\")\n         geo_dict = geometry_linestring(lat, lon, elev)\n     elif len(lat) == 1 and len(lon) == 1:\n         logger_excel.info(\"found 2 coordinates\")\n         geo_dict = geometry_point(lat, lon, elev)\n     elif (None in lon and None not in lat) or (len(lat) > 0 and len(lon) == 0):\n         geo_dict = geometry_range(lat, elev, \"lat\")\n     elif (None in lat and None not in lon) or (len(lon) > 0 and len(lat) == 0):\n         geo_dict = geometry_range(lat, elev, \"lon\")\n     else:\n         geo_dict = {}\n         logger_excel.warn(\"compile_geometry: invalid coordinates: lat: {}, lon: {}\".format(lat, lon))\n     logger_excel.info(\"exit compile_geometry\")\n     return geo_dict", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/wandb/client/blob/7d08954ed5674fee223cd85ed0d8518fe47266b2/wandb/data_types.py#L727-L746", "method_name": "to_uint8", "code": "def to_uint8(self, data):\n         np = util.get_module(\n             \"numpy\", required=\"wandb.Image requires numpy if not supplying PIL Images: pip install numpy\")\n         dmin = np.min(data)\n         if dmin < 0:\n             data = (data - np.min(data)) / np.ptp(data)\n         if np.max(data) <= 1.0:\n             data = (data * 255).astype(np.int32)\n         return data.clip(0, 255).astype(np.uint8)", "query": "converting uint8 array to image"}
{"id": "https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/requests/cookies.py#L441-L474", "method_name": "create_cookie", "code": "def create_cookie(name, value, **kwargs):\n     result = {\n         \"version\": 0,\n         \"name\": name,\n         \"value\": value,\n         \"port\": None,\n         \"domain\": \"\",\n         \"path\": \"/\",\n         \"secure\": False,\n         \"expires\": None,\n         \"discard\": True,\n         \"comment\": None,\n         \"comment_url\": None,\n         \"rest\": {\"HttpOnly\": None},\n         \"rfc2109\": False,\n     }\n     badargs = set(kwargs) - set(result)\n     if badargs:\n         err = \"create_cookie() got unexpected keyword arguments: %s\"\n         raise TypeError(err % list(badargs))\n     result.update(kwargs)\n     result[\"port_specified\"] = bool(result[\"port\"])\n     result[\"domain_specified\"] = bool(result[\"domain\"])\n     result[\"domain_initial_dot\"] = result[\"domain\"].startswith(\".\")\n     result[\"path_specified\"] = bool(result[\"path\"])\n     return cookielib.Cookie(**result)", "query": "create cookie"}
{"id": "https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/sma.py#L69-L75", "method_name": "get_monitor_observables", "code": "def get_monitor_observables(self, name):\n         try:\n             lines = self.items(name)\n         except NoSectionError:\n             return []\n         lines = [ItemLine(key, value) for key, value in lines]\n         return get_observables_from_lines(lines)", "query": "get current observable value"}
{"id": "https://github.com/buildbot/buildbot/blob/5df3cfae6d760557d99156633c32b1822a1e130c/master/buildbot/process/metrics.py#L74-L76", "method_name": "__init__", "code": "def __init__(self, timer, elapsed):\n        self.timer = timer\n        self.elapsed = elapsed", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/components/periphery.py#L163-L168", "method_name": "copy_to_clipboard", "code": "def copy_to_clipboard(self, event):\n         log.critical(\"Copy to clipboard\")\n         text = self.text.get(\"1.0\", tkinter.END)\n         print(text)\n         self.root.clipboard_clear()\n         self.root.clipboard_append(text)", "query": "copy to clipboard"}
{"id": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/parsing/base.py#L314-L327", "method_name": "read_text", "code": "def read_text(self, text: str) -> bool:\n         if self.read_eof():\n             return False\n         self._stream.save_context()\n         if self.peek_text(text):\n             self._stream.incpos(len(text))\n             return self._stream.validate_context()\n         return self._stream.restore_context()", "query": "buffered file reader read text"}
{"id": "https://github.com/cltk/cltk/blob/ed9c025b7ec43c949481173251b70e05e4dffd27/cltk/ir/query.py#L24-L37", "method_name": "_regex_span", "code": "def _regex_span(_regex, _str, case_insensitive=True):\n     if case_insensitive:\n         flags = regex.IGNORECASE   regex.FULLCASE   regex.VERSION1\n     else:\n         flags = regex.VERSION1\n     comp = regex.compile(_regex, flags=flags)\n     matches = comp.finditer(_str)\n     for match in matches:\n         yield match", "query": "regex case insensitive"}
{"id": "https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/ofctl_utils.py#L206-L210", "method_name": "to_match_packet_type", "code": "def to_match_packet_type(value):\n     if isinstance(value, (list, tuple)):\n         return str_to_int(value[0]) << 16   str_to_int(value[1])\n     else:\n         return str_to_int(value)", "query": "convert int to string"}
{"id": "https://github.com/lorien/grab/blob/8b301db2a08c830245b61c589e58af6234f4db79/grab/cookie.py#L118-L152", "method_name": "create_cookie", "code": "def create_cookie(name, value, domain, httponly=None, **kwargs):\n     if domain == \"localhost\":\n         domain = \"\"\n     config = dict(\n         name=name,\n         value=value,\n         version=0,\n         port=None,\n         domain=domain,\n         path=\"/\",\n         secure=False,\n         expires=None,\n         discard=True,\n         comment=None,\n         comment_url=None,\n         rfc2109=False,\n         rest={\"HttpOnly\": httponly},\n     )\n     for key in kwargs:\n         if key not in config:\n             raise GrabMisuseError(\"Function `create_cookie` does not accept \"\n                                   \"`%s` argument\" % key)\n     config.update(**kwargs)\n     config[\"rest\"][\"HttpOnly\"] = httponly\n     config[\"port_specified\"] = bool(config[\"port\"])\n     config[\"domain_specified\"] = bool(config[\"domain\"])\n     config[\"domain_initial_dot\"] = (config[\"domain\"] or \"\").startswith(\".\")\n     config[\"path_specified\"] = bool(config[\"path\"])\n     return Cookie(**config)", "query": "create cookie"}
{"id": "https://github.com/rigetti/grove/blob/dc6bf6ec63e8c435fe52b1e00f707d5ce4cdb9b3/grove/tomography/operator_utils.py#L70-L94", "method_name": "make_diagonal_povm", "code": "def make_diagonal_povm(pi_basis, confusion_rate_matrix):\n     confusion_rate_matrix = np.asarray(confusion_rate_matrix)\n     if not np.allclose(confusion_rate_matrix.sum(axis=0), np.ones(confusion_rate_matrix.shape[1])):\n         raise CRMUnnormalizedError(\"Unnormalized confusion matrix:\n{}\".format(\n             confusion_rate_matrix))\n     if not (confusion_rate_matrix >= 0).all() or not (confusion_rate_matrix <= 1).all():\n         raise CRMValueError(\"Confusion matrix must have values in [0, 1]:\"\n                             \"\n{}\".format(confusion_rate_matrix))\n     ops = [sum((pi_j * pjk for (pi_j, pjk) in izip(pi_basis.ops, pjs)), 0)\n            for pjs in confusion_rate_matrix]\n     return DiagonalPOVM(pi_basis=pi_basis, confusion_rate_matrix=confusion_rate_matrix, ops=ops)", "query": "confusion matrix"}
{"id": "https://github.com/pandas-profiling/pandas-profiling/blob/003d236daee8b7aca39c62708b18d59bced0bc03/pandas_profiling/__init__.py#L106-L122", "method_name": "to_file", "code": "def to_file(self, outputfile=DEFAULT_OUTPUTFILE):\n         if outputfile != NO_OUTPUTFILE:\n             if outputfile == DEFAULT_OUTPUTFILE:\n                 outputfile = \"profile_\" + str(hash(self)) + \".html\"\n             with codecs.open(outputfile, \"w+b\", encoding=\"utf8\") as self.file:\n                 self.file.write(templates.template(\"wrapper\").render(content=self.html))", "query": "output to html file"}
{"id": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L705-L720", "method_name": "multiply", "code": "def multiply(self, matrix):\n         if not isinstance(matrix, DenseMatrix):\n             raise ValueError(\"Only multiplication with DenseMatrix \"\n                              \"is supported.\")\n         return IndexedRowMatrix(self._java_matrix_wrapper.call(\"multiply\", matrix))", "query": "matrix multiply"}
{"id": "https://github.com/totalgood/nlpia/blob/efa01126275e9cd3c3a5151a644f1c798a9ec53f/src/nlpia/book/examples/ch05_sms_spam_linear_regression.py#L161-L173", "method_name": "sentiment_scatter", "code": "def sentiment_scatter(sms=sms):\n     plt.figure(figsize=(10, 7.5))\n     ax = plt.subplot(1, 1, 1)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"line\", ax=ax, color=\"g\", marker=\"+\", alpha=.6)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"sgd\", ax=ax, color=\"r\", marker=\"x\", alpha=.4)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"vader\", ax=ax, color=\"k\", marker=\".\", alpha=.3)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"sgd\", ax=ax, color=\"c\", marker=\"s\", alpha=.6)\n     ax = sms.plot.scatter(x=\"topic4\", y=\"pca_lda_spaminess\", ax=ax, color=\"b\", marker=\"o\", alpha=.6)\n     plt.ylabel(\"Sentiment\")\n     plt.xlabel(\"Topic 4\")\n     plt.legend([\"LinearRegressor\", \"SGDRegressor\", \"Vader\", \"OneNeuronRegresor\", \"PCA->LDA->spaminess\"])\n     plt.tight_layout()\n     plt.show()", "query": "scatter plot"}
{"id": "https://github.com/theislab/scanpy/blob/9e4e5ee02e04cf618872d9b098e24f0542e8b227/scanpy/plotting/_anndata.py#L30-L179", "method_name": "scatter", "code": "def scatter(\n         adata,\n         x=None,\n         y=None,\n         color=None,\n         use_raw=None,\n         layers=\"X\",\n         sort_order=True,\n         alpha=None,\n         basis=None,\n         groups=None,\n         components=None,\n         projection=\"2d\",\n         legend_loc=\"right margin\",\n         legend_fontsize=None,\n         legend_fontweight=None,\n         color_map=None,\n         palette=None,\n         frameon=None,\n         right_margin=None,\n         left_margin=None,\n         size=None,\n         title=None,\n         show=None,\n         save=None,\n         ax=None):\n     if basis is not None:\n         axs = _scatter_obs(\n             adata=adata,\n             x=x,\n             y=y,\n             color=color,\n             use_raw=use_raw,\n             layers=layers,\n             sort_order=sort_order,\n             alpha=alpha,\n             basis=basis,\n             groups=groups,\n             components=components,\n             projection=projection,\n             legend_loc=legend_loc,\n             legend_fontsize=legend_fontsize,\n             legend_fontweight=legend_fontweight,\n             color_map=color_map,\n             palette=palette,\n             frameon=frameon,\n             right_margin=right_margin,\n             left_margin=left_margin,\n             size=size,\n             title=title,\n             show=show,\n             save=save,\n             ax=ax)\n     elif x is not None and y is not None:\n         if ((x in adata.obs.keys() or x in adata.var.index)\n             and (y in adata.obs.keys() or y in adata.var.index)\n             and (color is None or color in adata.obs.keys() or color in adata.var.index)):\n             axs = _scatter_obs(\n                 adata=adata,\n                 x=x,\n                 y=y,\n                 color=color,\n                 use_raw=use_raw,\n                 layers=layers,\n                 sort_order=sort_order,\n                 alpha=alpha,\n                 basis=basis,\n                 groups=groups,\n                 components=components,\n                 projection=projection,\n                 legend_loc=legend_loc,\n                 legend_fontsize=legend_fontsize,\n                 legend_fontweight=legend_fontweight,\n                 color_map=color_map,\n                 palette=palette,\n                 frameon=frameon,\n                 right_margin=right_margin,\n                 left_margin=left_margin,\n                 size=size,\n                 title=title,\n                 show=show,\n                 save=save,\n                 ax=ax)\n         elif ((x in adata.var.keys() or x in adata.obs.index)\n                 and (y in adata.var.keys() or y in adata.obs.index)\n                 and (color is None or color in adata.var.keys() or color in adata.obs.index)):\n             axs = _scatter_var(\n                 adata=adata,\n                 x=x,\n                 y=y,\n                 color=color,\n                 use_raw=use_raw,\n                 layers=layers,\n                 sort_order=sort_order,\n                 alpha=alpha,\n                 basis=basis,\n                 groups=groups,\n                 components=components,\n                 projection=projection,\n                 legend_loc=legend_loc,\n                 legend_fontsize=legend_fontsize,\n                 legend_fontweight=legend_fontweight,\n                 color_map=color_map,\n                 palette=palette,\n                 frameon=frameon,\n                 right_margin=right_margin,\n                 left_margin=left_margin,\n                 size=size,\n                 title=title,\n                 show=show,\n                 save=save,\n                 ax=ax)\n         else:\n             raise ValueError(\n                 \"`x`, `y`, and potential `color` inputs must all come from either `.obs` or `.var`\")\n     else:\n         raise ValueError(\"Either provide a `basis` or `x` and `y`.\")\n     return axs", "query": "scatter plot"}
{"id": "https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/pipeline/disambiguate/run.py#L35-L39", "method_name": "nat_cmp", "code": "def nat_cmp(a, b):\n     convert = lambda text: int(text) if text.isdigit() else text \n     alphanum_key = lambda key: [ convert(c) for c in re.split(\"([0-9]+)\", key) ] \n     return (alphanum_key(a) > alphanum_key(b))-(alphanum_key(a) < alphanum_key(b))", "query": "convert string to number"}
{"id": "https://github.com/vladcalin/gemstone/blob/325a49d17621b9d45ffd2b5eca6f0de284de8ba4/gemstone/discovery/default.py#L19-L31", "method_name": "make_jsonrpc_call", "code": "def make_jsonrpc_call(self, url, method, params):\n         client = HTTPClient()\n         body = json.dumps({\n             \"jsonrpc\": \"2.0\",\n             \"method\": method,\n             \"params\": params,\n             \"id\": \"\".join([random.choice(string.ascii_letters) for _ in range(10)])\n         })\n         request = HTTPRequest(url, method=\"POST\", headers={\"content-type\": \"application/json\"},\n                               body=body)\n         result = client.fetch(request)\n         return result", "query": "httpclient post json"}
{"id": "https://github.com/KelSolaar/Foundations/blob/5c141330faf09dad70a12bc321f4c564917d0a91/foundations/decorators.py#L80-L129", "method_name": "memoize", "code": "def memoize(cache=None):\n     if cache is None:\n         cache = {}\n     def memoize_decorator(object):\n         @functools.wraps(object)\n         def memoize_wrapper(*args, **kwargs):\n             if kwargs:\n                 key = args, frozenset(kwargs.iteritems())\n             else:\n                 key = args\n             if key not in cache:\n                 cache[key] = object(*args, **kwargs)\n             return cache[key]\n         return memoize_wrapper\n     return memoize_decorator", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/plotter.py#L249-L274", "method_name": "heatmap", "code": "def heatmap(self, partition=None, cmap=CM.Blues):\n         if isinstance(self.dm, DistanceMatrix):\n             length = self.dm.values.shape[0]\n         else:\n             length = self.dm.shape[0]\n         datamax = float(np.abs(self.dm).max())\n         fig = plt.figure()\n         ax = fig.add_subplot(111)\n         ticks_at = [0, 0.5 * datamax, datamax]\n         if partition:\n             sorting = flatten_list(partition.get_membership())\n             self.dm = self.dm.reorder(sorting)\n         cax = ax.imshow(\n             self.dm.values,\n             interpolation=\"nearest\",\n             origin=\"lower\",\n             extent=[0., length, 0., length],\n             vmin=0,\n             vmax=datamax,\n             cmap=cmap,\n         )\n         cbar = fig.colorbar(cax, ticks=ticks_at, format=\"%1.2g\")\n         cbar.set_label(\"Distance\")\n         return fig", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/gambogi/CSHLDAP/blob/09cb754b1e72437834e0d8cb4c7ac1830cfa6829/CSHLDAP.py#L336-L348", "method_name": "dateFromLDAPTimestamp", "code": "def dateFromLDAPTimestamp(timestamp):\n     numberOfCharacters = len(\"YYYYmmdd\")\n     timestamp = timestamp[:numberOfCharacters]\n     try:\n         day = datetime.strptime(timestamp, \"%Y%m%d\")\n         return date(year=day.year, month=day.month, day=day.day)\n     except:\n         print(timestamp)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/seleniumbase/SeleniumBase/blob/62e5b43ee1f90a9ed923841bdd53b1b38358f43a/seleniumbase/fixtures/email_manager.py#L454-L481", "method_name": "replace_entities", "code": "def replace_entities(self, html):\n         def fixup(text):\n             text = text.group(0)\n             if text[:2] == \"&\n                 try:\n                     if text[:3] == \"&\n                         return chr(int(text[3:-1], 16))\n                     else:\n                         return chr(int(text[2:-1]))\n                 except ValueError:\n                     pass\n             else:\n                 try:\n                     text = chr(htmlentitydefs.name2codepoint[text[1:-1]])\n                 except KeyError:\n                     pass\n             return text  \n         return re.sub(r\"&", "query": "html entities replace"}
{"id": "https://github.com/clach04/python-tuya/blob/7b89d38c56f6e25700e2a333000d25bc8d923622/pytuya/__init__.py#L408-L420", "method_name": "_hexvalue_to_rgb", "code": "def _hexvalue_to_rgb(hexvalue):\n         r = int(hexvalue[0:2], 16)\n         g = int(hexvalue[2:4], 16)\n         b = int(hexvalue[4:6], 16)\n         return (r, g, b)", "query": "convert decimal to hex"}
{"id": "https://github.com/wtsi-hgi/python-common/blob/0376a6b574ff46e82e509e90b6cb3693a3dbb577/hgicommon/data_source/static_from_file.py#L72-L82", "method_name": "no_error_extract_data_from_file", "code": "def no_error_extract_data_from_file(self, file_path: str) -> Iterable[DataSourceType]:\n         try:\n             return self.extract_data_from_file(file_path)\n         except Exception as e:\n             logging.warning(e)\n             return []", "query": "extracting data from a text file"}
{"id": "https://github.com/klahnakoski/mo-logs/blob/0971277ac9caf28a755b766b70621916957d4fea/mo_logs/strings.py#L294-L315", "method_name": "find", "code": "def find(value, find, start=0):\n     l = len(value)\n     if is_list(find):\n         m = l\n         for f in find:\n             i = value.find(f, start)\n             if i == -1:\n                 continue\n             m = min(m, i)\n         return m\n     else:\n         i = value.find(find, start)\n         if i == -1:\n             return l\n         return i", "query": "find int in string"}
{"id": "https://github.com/amperser/proselint/blob/cb619ee4023cc7856f5fb96aec2a33a2c9f1a2e2/proselint/tools.py#L99-L147", "method_name": "memoize", "code": "def memoize(f):\n     cache_dirname = os.path.join(_get_xdg_cache_home(), \"proselint\")\n     legacy_cache_dirname = os.path.join(os.path.expanduser(\"~\"), \".proselint\")\n     if not os.path.isdir(cache_dirname):\n         if os.path.isdir(legacy_cache_dirname):\n             os.rename(legacy_cache_dirname, cache_dirname)\n         else:\n             os.makedirs(cache_dirname)\n     cache_filename = f.__module__ + \".\" + f.__name__\n     cachepath = os.path.join(cache_dirname, cache_filename)\n     @functools.wraps(f)\n     def wrapped(*args, **kwargs):\n         if hasattr(f, \"__self__\"):\n             args = args[1:]\n         signature = (f.__module__ + \".\" + f.__name__).encode(\"utf-8\")\n         tempargdict = inspect.getcallargs(f, *args, **kwargs)\n         for item in list(tempargdict.items()):\n             signature += item[1].encode(\"utf-8\")\n         key = hashlib.sha256(signature).hexdigest()\n         try:\n             cache = _get_cache(cachepath)\n             return cache[key]\n         except KeyError:\n             value = f(*args, **kwargs)\n             cache[key] = value\n             cache.sync()\n             return value\n         except TypeError:\n             call_to = f.__module__ + \".\" + f.__name__\n             print(\"Warning: could not disk cache call to %s;\"\n                   \"it probably has unhashable args. Error: %s\" %\n                   (call_to, traceback.format_exc()))\n             return f(*args, **kwargs)\n     return wrapped", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/caktus/django-timepiece/blob/52515dec027664890efbc535429e1ba1ee152f40/timepiece/entries/views.py#L44-L55", "method_name": "get_dates", "code": "def get_dates(self):\n         today = datetime.date.today()\n         day = today\n         if \"week_start\" in self.request.GET:\n             param = self.request.GET.get(\"week_start\")\n             try:\n                 day = datetime.datetime.strptime(param, \"%Y-%m-%d\").date()\n             except:\n                 pass\n         week_start = utils.get_week_start(day)\n         week_end = week_start + relativedelta(days=6)\n         return today, week_start, week_end", "query": "get current date"}
{"id": "https://github.com/MozillaSecurity/laniakea/blob/7e80adc6ae92c6c1332d4c08473bb271fb3b6833/laniakea/core/userdata.py#L43-L51", "method_name": "convert_str_to_int", "code": "def convert_str_to_int(arg):\n         for k, v in list(arg.items()):  \n             try:\n                 arg[String(k)] = int(v)\n             except ValueError:\n                 pass\n         return arg", "query": "convert int to string"}
{"id": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Model_Data.py#L176-L203", "method_name": "linear_regression", "code": "def linear_regression(self):\n         model = LinearRegression()\n         scores = []\n         kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n         for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)):\n             model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])\n             scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test]))\n         mean_score = sum(scores) / len(scores)\n         self.models.append(model)\n         self.model_names.append(\"Linear Regression\")\n         self.max_scores.append(mean_score)\n         self.metrics[\"Linear Regression\"] = {}\n         self.metrics[\"Linear Regression\"][\"R2\"] = mean_score\n         self.metrics[\"Linear Regression\"][\"Adj R2\"] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])", "query": "linear regression"}
{"id": "https://github.com/abilian/abilian-core/blob/0a71275bf108c3d51e13ca9e093c0249235351e3/abilian/web/admin/panels/audit.py#L29-L33", "method_name": "format_date_for_input", "code": "def format_date_for_input(date):\n     date_fmt = get_locale().date_formats[\"short\"].pattern\n     date_fmt = date_fmt.replace(\"MMMM\", \"MM\").replace(\"MMM\", \"MM\")\n     return format_date(date, date_fmt)", "query": "format date"}
{"id": "https://github.com/vecnet/vecnet.simulation/blob/3a4b3df7b12418c6fa8a7d9cd49656a1c031fc0e/vecnet/simulation/sim_status.py#L47-L54", "method_name": "get_description", "code": "def get_description(status_code):\n     description = _descriptions.get(status_code)\n     if description is None:\n         description = \"code = %s (no description)\" % str(status_code)\n     return description", "query": "get the description of a http status code"}
{"id": "https://github.com/tanghaibao/goatools/blob/407682e573a108864a79031f8ca19ee3bf377626/goatools/obo_parser.py#L205-L211", "method_name": "get_all_parents", "code": "def get_all_parents(self):\n         all_parents = set()\n         for parent in self.parents:\n             all_parents.add(parent.item_id)\n             all_parents  = parent.get_all_parents()\n         return all_parents", "query": "get all parents of xml node"}
{"id": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/utils.py#L129-L139", "method_name": "median_interp", "code": "def median_interp(interp_object):\n     new_grid = np.sort(np.concatenate([interp_object.x[:-1] + 0.1*ii*np.diff(interp_object.x)\n                                        for ii in range(10)]).flatten())\n     tmp_prop = np.exp(-(interp_object(new_grid)-interp_object.y.min()))\n     tmp_cumsum = np.cumsum(0.5*(tmp_prop[1:]+tmp_prop[:-1])*np.diff(new_grid))\n     median_index = min(len(tmp_cumsum)-3, max(2,np.searchsorted(tmp_cumsum, tmp_cumsum[-1]*0.5)+1))\n     return new_grid[median_index]", "query": "deducting the median from each column"}
{"id": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/html.py#L375-L386", "method_name": "escape", "code": "def escape(t):\n     return (t\n             .replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n             .replace(, \"&quot;\")\n             .replace(\"  \", \"&nbsp; \")\n             .replace(\"  \", \"&nbsp; \")\n         )", "query": "html entities replace"}
{"id": "https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/utils.py#L112-L121", "method_name": "pprint", "code": "def pprint(j, no_pretty):\n     if not no_pretty:\n         click.echo(\n             json.dumps(j, cls=PotionJSONEncoder, sort_keys=True, indent=4, separators=(\",\", \": \"))\n         )\n     else:\n         click.echo(j)", "query": "pretty print json"}
{"id": "https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/hardware.py#L106-L119", "method_name": "_readline", "code": "def _readline(self, ignore_comments=True):\n         while True:\n             line = self._det_file.readline()\n             if line == \"\":\n                 return line    \n             line = line.strip()\n             if line == \"\":\n                 continue    \n             if line.startswith(\"\n                 if not ignore_comments:\n                     return line\n             else:\n                 return line", "query": "read text file line by line"}
{"id": "https://github.com/talkincode/txradius/blob/b86fdbc9be41183680b82b07d3a8e8ea10926e01/txradius/mschap/mschap.py#L94-L101", "method_name": "convert_to_hex_string", "code": "def convert_to_hex_string(string):\n     hex_str = \"\"\n     for c in string:\n         hex_tmp = hex(ord(c))[2:]\n         if len(hex_tmp) == 1:\n             hex_tmp = \"0\" + hex_tmp\n         hex_str += hex_tmp\n     return hex_str.upper()", "query": "convert decimal to hex"}
{"id": "https://github.com/konomae/lastpass-python/blob/5063911b789868a1fd9db9922db82cdf156b938a/lastpass/parser.py#L269-L284", "method_name": "decode_aes256", "code": "def decode_aes256(cipher, iv, data, encryption_key):\n     if cipher == \"cbc\":\n         aes = AES.new(encryption_key, AES.MODE_CBC, iv)\n     elif cipher == \"ecb\":\n         aes = AES.new(encryption_key, AES.MODE_ECB)\n     else:\n         raise ValueError(\"Unknown AES mode\")\n     d = aes.decrypt(data)\n     unpad = lambda s: s[0:-ord(d[-1:])]\n     return unpad(d)", "query": "aes encryption"}
{"id": "https://github.com/shawnbot/py-organ/blob/1663b0943dd9d35a43618b4914c839f7c07d7966/organ/__init__.py#L45-L52", "method_name": "multisorter", "code": "def multisorter(*sorts):\n     def _sort(aa, bb):\n         for sort in sorts:\n             order = sort(aa, bb)\n             if order != 0:\n                 return order\n         return 0\n     return _sort", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L238-L241", "method_name": "_parse_url", "code": "def _parse_url(url):\n     p = urlsplit(url)\n     query = {k: v[0] for k, v in parse_qs(p.query).items() if len(v) == 1}\n     return \"\".join([p.netloc, p.path]), query", "query": "parse query string in url"}
{"id": "https://github.com/release-engineering/productmd/blob/49256bf2e8c84124f42346241140b986ad7bfc38/productmd/common.py#L302-L315", "method_name": "parse_file", "code": "def parse_file(self, f):\n         if hasattr(f, \"seekable\"):\n             if f.seekable():\n                 f.seek(0)\n         elif hasattr(f, \"seek\"):\n             f.seek(0)\n         if six.PY3 and isinstance(f, six.moves.http_client.HTTPResponse):\n             reader = codecs.getreader(\"utf-8\")\n             parser = json.load(reader(f))\n         else:\n             parser = json.load(f)\n         return parser", "query": "parse json file"}
{"id": "https://github.com/commonwealth-of-puerto-rico/libre/blob/5b32f4ab068b515d2ea652b182e161271ba874e8/libre/apps/data_drivers/models.py#L341-L369", "method_name": "_convert_value", "code": "def _convert_value(self, item):\n         if item.ctype == 3:  \n             try:\n                 return datetime.datetime(*xlrd.xldate_as_tuple(item.value, self._book.datemode))\n             except ValueError:\n                 return item.value\n         if item.ctype == 2:  \n             if item.value % 1 == 0:  \n                 return int(item.value)\n             else:\n                 return item.value\n         return item.value", "query": "convert int to string"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/arrayeditor.py#L526-L530", "method_name": "copy", "code": "def copy(self): \n         cliptxt = self._sel_to_text( self.selectedIndexes() ) \n         clipboard = QApplication.clipboard() \n         clipboard.setText(cliptxt)", "query": "copy to clipboard"}
{"id": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/websitemanagementservice.py#L103-L114", "method_name": "get_site", "code": "def get_site(self, webspace_name, website_name):\n         return self._perform_get(self._get_sites_details_path(webspace_name,\n                                                               website_name),\n                                  Site)", "query": "get html of website"}
{"id": "https://github.com/pasztorpisti/json-cfg/blob/4627b14a92521ef8a39bbedaa7af8d380d406d07/src/jsoncfg/tree_python.py#L61-L69", "method_name": "default_number_converter", "code": "def default_number_converter(number_str):\n     is_int = (number_str.startswith(\"-\") and number_str[1:].isdigit()) or number_str.isdigit()\n     return int(number_str) if is_int else float(number_str)", "query": "convert string to number"}
{"id": "https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/math.py#L224-L229", "method_name": "circ_permutation", "code": "def circ_permutation(items):\n     permutations = []\n     for i in range(len(items)):\n         permutations.append(items[i:] + items[:i])\n     return permutations", "query": "all permutations of a list"}
{"id": "https://github.com/mozilla/Marketplace.Python/blob/88176b12201f766b6b96bccc1e4c3e82f0676283/example/main.py#L29-L61", "method_name": "main", "code": "def main():\n     parser = argparse.ArgumentParser(\n         description=\"Command line Marketplace client\")\n     parser.add_argument(\"method\", type=str,\n                         help=\"command to be run on arguments\",\n                         choices=COMMANDS.keys())\n     parser.add_argument(\"attrs\", metavar=\"attr\", type=str, nargs=\"*\",\n                         help=\"command arguments\")\n     parser.add_argument(\"-v\", action=\"store_true\", default=False,\n                         dest=\"verbose\",\n                         help=\"Switch to verbose mode\")\n     args = parser.parse_args()\n     client = marketplace.Client(\n         domain=config.MARKETPLACE_DOMAIN,\n         protocol=config.MARKETPLACE_PROTOCOL,\n         port=config.MARKETPLACE_PORT,\n         consumer_key=config.CONSUMER_KEY,\n         consumer_secret=config.CONSUMER_SECRET)\n     if args.verbose:\n         logger.setLevel(logging.DEBUG)\n     if args.attrs:\n         result = COMMANDS[args.method](client, *args.attrs)\n     else:\n         result = COMMANDS[args.method](client)\n     if result[\"success\"]:\n         sys.stdout.write(\"%s\n\" % result[\"message\"])\n     else:\n         sys.stderr.write(\"%s\n\" % result[\"message\"])\n         sys.exit(1)", "query": "parse command line argument"}
{"id": "https://github.com/bigchaindb/bigchaindb/blob/835fdfcf598918f76139e3b88ee33dd157acaaa7/bigchaindb/commands/utils.py#L53-L76", "method_name": "_convert", "code": "def _convert(value, default=None, convert=None):\n     def convert_bool(value):\n         if value.lower() in (\"true\", \"t\", \"yes\", \"y\"):\n             return True\n         if value.lower() in (\"false\", \"f\", \"no\", \"n\"):\n             return False\n         raise ValueError(\"{} cannot be converted to bool\".format(value))\n     if value == \"\":\n         value = None\n     if convert is None:\n         if default is not None:\n             convert = type(default)\n         else:\n             convert = str\n     if convert == bool:\n         convert = convert_bool\n     if value is None:\n         return default\n     else:\n         return convert(value)", "query": "convert int to bool"}
{"id": "https://github.com/tango-controls/pytango/blob/9cf78c517c9cdc1081ff6d080a9646a740cc1d36/tango/device_proxy.py#L704-L744", "method_name": "__DeviceProxy__get_property_list", "code": "def __DeviceProxy__get_property_list(self, filter, array=None):\n     if array is None:\n         new_array = StdStringVector()\n         self._get_property_list(filter, new_array)\n         return new_array\n     if isinstance(array, StdStringVector):\n         self._get_property_list(filter, array)\n         return array\n     elif isinstance(array, collections_abc.Sequence):\n         new_array = StdStringVector()\n         self._get_property_list(filter, new_array)\n         StdStringVector_2_seq(new_array, array)\n         return array\n     raise TypeError(\"array must be a mutable sequence<string>\")", "query": "filter array"}
{"id": "https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2510-L2520", "method_name": "distinct_counts", "code": "def distinct_counts(self):\n         k = [hash(self.values[:, i].tobytes()) for i in range(self.shape[1])]\n         counts = sorted(collections.Counter(k).values(), reverse=True)\n         return np.asarray(counts)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/jcassee/django-geckoboard/blob/6ebdaa86015fe645360abf1ba1290132de4cf6d6/django_geckoboard/decorators.py#L446-L460", "method_name": "_encrypt", "code": "def _encrypt(data):\n     BS = AES.block_size\n     def pad(s):\n         n = BS - len(s) % BS\n         char = chr(n).encode(\"utf8\")\n         return s + n * char\n     password = settings.GECKOBOARD_PASSWORD\n     salt = Random.new().read(BS - len(\"Salted__\"))\n     key, iv = _derive_key_and_iv(password, salt, 32, BS)\n     cipher = AES.new(key, AES.MODE_CBC, iv)\n     encrypted = b\"Salted__\" + salt + cipher.encrypt(pad(data))\n     return base64.b64encode(encrypted)", "query": "aes encryption"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547", "method_name": "check_checkbox", "code": "def check_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     if not check_box.is_selected():\n         check_box.click()", "query": "check if a checkbox is checked"}
{"id": "https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2492-L2508", "method_name": "distinct", "code": "def distinct(self):\n         d = collections.defaultdict(set)\n         for i in range(self.shape[1]):\n             k = hash(self.values[:, i].tobytes())\n             d[k].add(i)\n         return sorted(d.values(), key=len, reverse=True)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/data/stats.py#L8-L21", "method_name": "lreg", "code": "def lreg(self, xcol, ycol, name=\"Regression\"):\n         try:\n             x = self.df[xcol].values.reshape(-1, 1)\n             y = self.df[ycol]\n             lm = linear_model.LinearRegression()\n             lm.fit(x, y)\n             predictions = lm.predict(x)\n             self.df[name] = predictions\n         except Exception as e:\n             self.err(e, \"Can not calculate linear regression\")", "query": "linear regression"}
{"id": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/utils.py#L129-L139", "method_name": "median_interp", "code": "def median_interp(interp_object):\n     new_grid = np.sort(np.concatenate([interp_object.x[:-1] + 0.1*ii*np.diff(interp_object.x)\n                                        for ii in range(10)]).flatten())\n     tmp_prop = np.exp(-(interp_object(new_grid)-interp_object.y.min()))\n     tmp_cumsum = np.cumsum(0.5*(tmp_prop[1:]+tmp_prop[:-1])*np.diff(new_grid))\n     median_index = min(len(tmp_cumsum)-3, max(2,np.searchsorted(tmp_cumsum, tmp_cumsum[-1]*0.5)+1))\n     return new_grid[median_index]", "query": "deducting the median from each column"}
{"id": "https://github.com/boriel/zxbasic/blob/23b28db10e41117805bdb3c0f78543590853b132/asmlex.py#L226-L239", "method_name": "t_BIN", "code": "def t_BIN(self, t):\n         r\"(%[01]+) ([01]+[bB])\"  \n         if t.value[0] == \"%\":\n             t.value = t.value[1:]  \n         else:\n             t.value = t.value[:-1]  \n         t.value = int(t.value, 2)  \n         t.type = \"INTEGER\"\n         return t", "query": "convert decimal to hex"}
{"id": "https://github.com/Games-and-Simulations/sc-docker/blob/1d7adb9b5839783655564afc4bbcd204a0055dcb/scbw/utils.py#L18-L37", "method_name": "levenshtein_dist", "code": "def levenshtein_dist(s1: str, s2: str) -> int:\n     if len(s1) < len(s2):\n         return levenshtein_dist(s2, s1)\n     if len(s2) == 0:\n         return len(s1)\n     previous_row = range(len(s2) + 1)\n     for i, c1 in enumerate(s1):\n         current_row = [i + 1]\n         for j, c2 in enumerate(s2):\n             insertions = previous_row[j + 1] + 1\n             deletions = current_row[j] + 1  \n             substitutions = previous_row[j] + (c1 != c2)\n             current_row.append(min(insertions, deletions, substitutions))\n         previous_row = current_row\n     return previous_row[-1]", "query": "string similarity levenshtein"}
{"id": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/html.py#L375-L386", "method_name": "escape", "code": "def escape(t):\n     return (t\n             .replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n             .replace(, \"&quot;\")\n             .replace(\"  \", \"&nbsp; \")\n             .replace(\"  \", \"&nbsp; \")\n         )", "query": "html entities replace"}
{"id": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/parsing/base.py#L314-L327", "method_name": "read_text", "code": "def read_text(self, text: str) -> bool:\n         if self.read_eof():\n             return False\n         self._stream.save_context()\n         if self.peek_text(text):\n             self._stream.incpos(len(text))\n             return self._stream.validate_context()\n         return self._stream.restore_context()", "query": "buffered file reader read text"}
{"id": "https://github.com/gpagliuca/pyfas/blob/5daa1199bd124d315d02bef0ad3888a8f58355b2/build/lib/pyfas/ppl.py#L127-L151", "method_name": "to_excel", "code": "def to_excel(self, *args): \n         path = os.getcwd() \n         fname = self.fname.replace(\".ppl\", \"_ppl\") + \".xlsx\" \n         if len(args) > 0 and args[0] != \"\": \n             path = args[0] \n             if os.path.exists(path) == False: \n                 os.mkdir(path) \n         xl_file = pd.ExcelWriter(path + os.sep + fname) \n         for idx in self.filter_data(\"\"): \n             self.extract(idx) \n         labels = list(self.filter_data(\"\").values()) \n         for prof in self.data: \n             data_df = pd.DataFrame() \n             data_df[\"X\"] = self.data[prof][0] \n             for timestep, data in zip(self.time, self.data[prof][1]): \n                 data_df[timestep] = data \n             myvar = labels[prof-1].split(\" \")[0] \n             br_label = labels[prof-1].split(\"\\\"\")[5] \n             unit = labels[prof-1].split(\"\\\"\")[7].replace(\"/\", \"-\") \n             mylabel = \"{} - {} - {}\".format(myvar, br_label, unit) \n             data_df.to_excel(xl_file, sheet_name=mylabel) \n         xl_file.save()", "query": "export to excel"}
{"id": "https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/Facade.py#L594-L597", "method_name": "create_label_widget", "code": "def create_label_widget(self, text=None):\n         label_widget = LabelWidget(self.__ui)\n         label_widget.text = text\n         return label_widget", "query": "underline text in label widget"}
{"id": "https://github.com/fvalverd/AutoApi-client-Python/blob/a6b04947f80bf988c1515d622c78c587b341b3f4/auto_api_client/__init__.py#L91-L94", "method_name": "post", "code": "def post(self, json=None):\n         response = self._http(requests.post, json=json)\n         if response.status_code == 201:\n             return response.json()", "query": "httpclient post json"}
{"id": "https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/tag.py#L155-L159", "method_name": "innerHTML", "code": "def innerHTML(self) -> str:\n         if self._inner_element:\n             return self._inner_element.innerHTML\n         return super().innerHTML", "query": "get inner html"}
{"id": "https://github.com/Fizzadar/pyinfra/blob/006f751f7db2e07d32522c0285160783de2feb79/pyinfra/modules/postgresql.py#L48-L123", "method_name": "role", "code": "def role(\n     state, host, name,\n     present=True,\n     password=None, login=True, superuser=False, inherit=False,\n     createdb=False, createrole=False, replication=False, connection_limit=None,\n     postgresql_user=None, postgresql_password=None,\n     postgresql_host=None, postgresql_port=None,\n ):\n     roles = host.fact.postgresql_roles(\n         postgresql_user, postgresql_password,\n         postgresql_host, postgresql_port,\n     )\n     is_present = name in roles\n     if not present:\n         if is_present:\n             yield make_execute_psql_command(\n                 \"DROP ROLE {0}\".format(name),\n                 user=postgresql_user,\n                 password=postgresql_password,\n                 host=postgresql_host,\n                 port=postgresql_port,\n             )\n         return\n     if not is_present:\n         sql_bits = [\"CREATE ROLE {0}\".format(name)]\n         for key, value in (\n             (\"LOGIN\", login),\n             (\"SUPERUSER\", superuser),\n             (\"INHERIT\", inherit),\n             (\"CREATEDB\", createdb),\n             (\"CREATEROLE\", createrole),\n             (\"REPLICATION\", replication),\n         ):\n             if value:\n                 sql_bits.append(key)\n         if connection_limit:\n             sql_bits.append(\"CONNECTION LIMIT {0}\".format(connection_limit))\n         if password:\n             sql_bits.append(\"PASSWORD \"{0}\"\".format(password))\n         yield make_execute_psql_command(\n             \" \".join(sql_bits),\n             user=postgresql_user,\n             password=postgresql_password,\n             host=postgresql_host,\n             port=postgresql_port,\n         )", "query": "postgresql connection"}
{"id": "https://github.com/bxlab/bx-python/blob/09cb725284803df90a468d910f2274628d8647de/lib/bx/align/lav.py#L287-L296", "method_name": "fetch_line", "code": "def fetch_line(self,strip=True,requireLine=True,report=\"\"):\n \t\tif   (strip == None): line = self.file.readline()\n \t\telif (strip == True): line = self.file.readline().strip()\n \t\telse:                 line = self.file.readline().strip().strip(strip)\n \t\tself.lineNumber += 1\n \t\tif (requireLine):\n \t\t\tassert (line), \\\n \t\t\t       \"unexpected blank line or end of file%s (line %d)\" \\\n \t\t\t     % (report,self.lineNumber)\n \t\treturn line", "query": "read text file line by line"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/referenceanalysis.py#L85-L88", "method_name": "toPdf", "code": "def toPdf(self):\n         html = safe_unicode(self.template()).encode(\"utf-8\")\n         pdf_data = createPdf(html)\n         return pdf_data", "query": "convert html to pdf"}
{"id": "https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/osid/markers.py#L309-L318", "method_name": "is_effective", "code": "def is_effective(self):\n         now = DateTime.utcnow()\n         return self.get_start_date() <= now and self.get_end_date() >= now", "query": "get current date"}
{"id": "https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L306-L311", "method_name": "write_csv", "code": "def write_csv(filename, T, header = None):\n     with open(filename,\"w\") as fh:\n         csv_writer = csv.writer(fh, delimiter=\",\")\n         if header != None:\n             csv_writer.writerow(header)\n         [csv_writer.writerow(T[i]) for i in range(len(T))]", "query": "write csv"}
{"id": "https://github.com/wdecoster/nanoplotter/blob/80908dd1be585f450da5a66989de9de4d544ec85/nanoplotter/plot.py#L18-L24", "method_name": "encode", "code": "def encode(self):\n         if self.html:\n             return self.html\n         elif self.fig:\n             return self.encode2()\n         else:\n             return self.encode1()", "query": "html encode string"}
{"id": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Model_Data.py#L176-L203", "method_name": "linear_regression", "code": "def linear_regression(self):\n         model = LinearRegression()\n         scores = []\n         kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n         for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)):\n             model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])\n             scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test]))\n         mean_score = sum(scores) / len(scores)\n         self.models.append(model)\n         self.model_names.append(\"Linear Regression\")\n         self.max_scores.append(mean_score)\n         self.metrics[\"Linear Regression\"] = {}\n         self.metrics[\"Linear Regression\"][\"R2\"] = mean_score\n         self.metrics[\"Linear Regression\"][\"Adj R2\"] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])", "query": "linear regression"}
{"id": "https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/htmls.py#L106-L116", "method_name": "get_body", "code": "def get_body(doc):\n     [ elem.drop_tree() for elem in doc.xpath(\".//script   .//link   .//style\") ]\n     raw_html = tostring(doc.body or doc).decode(\"utf-8\")\n     print(raw_html)\n     cleaned = clean_attributes(raw_html)\n     try:\n         return cleaned\n     except Exception: \n---------\n%s\" % (raw_html, cleaned))\n         return raw_html", "query": "reading element from html - <td>"}
{"id": "https://github.com/OnroerendErfgoed/skosprovider_getty/blob/5aa0b5a8525d607e07b631499ff31bac7a0348b7/skosprovider_getty/providers.py#L390-L396", "method_name": "_sort", "code": "def _sort(self, items, sort, language=\"en\", reverse=False):\n         if sort is None:\n             sort = \"id\"\n         if sort == \"sortlabel\":\n             sort=\"label\"\n         items.sort(key=lambda item: item[sort], reverse=reverse)\n         return items", "query": "sort string list"}
{"id": "https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L163-L170", "method_name": "dec2str", "code": "def dec2str(n):\n     s = hex(int(n))[2:].rstrip(\"L\")\n     if len(s) % 2 != 0:\n         s = \"0\" + s\n     return hex2str(s)", "query": "convert decimal to hex"}
{"id": "https://github.com/SpotlightData/preprocessing/blob/180c6472bc2642afbd7a1ece08d0b0d14968a708/preprocessing/spellcheck.py#L91-L101", "method_name": "find_word_prob", "code": "def find_word_prob(word_string, word_total=sum(WORD_DISTRIBUTION.values())):\n     if word_string is None:\n         return 0\n     elif isinstance(word_string, str):\n         return WORD_DISTRIBUTION[word_string] / word_total\n     else:\n         raise InputError(\"string or none type variable not passed as argument to find_word_prob\")", "query": "determine a string is a valid word"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/collectionseditor.py#L1242-L1252", "method_name": "paste", "code": "def paste(self): \n         clipboard = QApplication.clipboard() \n         cliptext = \"\" \n         if clipboard.mimeData().hasText(): \n             cliptext = to_text_string(clipboard.text()) \n         if cliptext.strip(): \n             self.import_from_string(cliptext, title=_(\"Import from clipboard\")) \n         else: \n             QMessageBox.warning(self, _( \"Empty clipboard\"), \n                                 _(\"Nothing to be imported from clipboard.\"))", "query": "copy to clipboard"}
{"id": "https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_pretty_print.py#L8-L14", "method_name": "json_pretty_print", "code": "def json_pretty_print(s):\n     s = json.loads(s)\n     return json.dumps(s,\n                       sort_keys=True,\n                       indent=4,\n                       separators=(\",\", \": \"))", "query": "pretty print json"}
{"id": "https://github.com/ccxt/ccxt/blob/23062efd7a5892c79b370c9d951c03cf8c0ddf23/python/ccxt/base/exchange.py#L692-L693", "method_name": "filterBy", "code": "def filterBy(self, array, key, value=None):\n        return Exchange.filter_by(array, key, value)", "query": "filter array"}
{"id": "https://github.com/tjguk/winshell/blob/1509d211ab3403dd1cff6113e4e13462d6dec35b/winshell.py#L266-L294", "method_name": "copy_file", "code": "def copy_file(\n     source_path,\n     target_path,\n     allow_undo=True,\n     no_confirm=False,\n     rename_on_collision=True,\n     silent=False,\n     extra_flags=0,\n     hWnd=None\n ):\n     return _file_operation(\n         shellcon.FO_COPY,\n         source_path,\n         target_path,\n         allow_undo,\n         no_confirm,\n         rename_on_collision,\n         silent,\n         extra_flags,\n         hWnd\n     )", "query": "copying a file to a path"}
{"id": "https://github.com/SignalN/language/blob/5c50c78f65bcc2c999b44d530e7412185248352d/language/ngrams.py#L150-L151", "method_name": "word_similarity", "code": "def word_similarity(s1, s2, n=3):\n    return __similarity(s1, s2, word_ngrams, n=n)", "query": "string similarity levenshtein"}
{"id": "https://github.com/CellProfiler/centrosome/blob/7bd9350a2d4ae1b215b81eabcecfe560bbb1f32a/centrosome/filter.py#L1427-L1476", "method_name": "permutations", "code": "def permutations(x):\n     x = np.array(x)\n     a = np.arange(len(x))\n     while True:\n         ak_lt_ak_next = np.argwhere(a[:-1] < a[1:])\n         if len(ak_lt_ak_next) == 0:\n             raise StopIteration()\n         k = ak_lt_ak_next[-1, 0]\n         ak_lt_al = np.argwhere(a[k] < a)\n         l =  ak_lt_al[-1, 0]\n         a[k], a[l]  = (a[l], a[k])\n         if k < len(x)-1:\n             a[k+1:] = a[:k:-1].copy()\n         yield x[a].tolist()", "query": "all permutations of a list"}
{"id": "https://github.com/LuminosoInsight/langcodes/blob/0cedf9ca257ebf7250de5d3a63ec33a7d198db58/langcodes/registry_parser.py#L6-L25", "method_name": "parse_file", "code": "def parse_file(file):\n     lines = []\n     for line in file:\n         line = line.rstrip(\"\n\")\n         if line == \"%%\":\n             yield from parse_item(lines)\n             lines.clear()\n         elif line.startswith(\"  \"):\n             lines[-1] += line[1:]\n         else:\n             lines.append(line)\n     yield from parse_item(lines)", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/vfxetc/sgactions/blob/e2c6ee14a1f8092e97f57a501650581428a6ac6e/sgactions/dispatch.py#L14-L33", "method_name": "parse_url", "code": "def parse_url(url):\n     m = re.match(r\"^(?:(\\w+):)?(.*?)(?:/(.*?))?(?:\\?(.*))?$\", url)\n     scheme, netloc, path, query = m.groups()\n     kwargs = urlparse.parse_qs(query, keep_blank_values=True) if query else {}\n     for k, v in kwargs.iteritems():\n         if len(v) == 1 and k not in (\"cols\", \"column_display_names\"):\n             kwargs[k] = v = v[0]\n         if k.endswith(\"_id\") and v.isdigit():\n             kwargs[k] = int(v)\n     m = re.match(r\"^([\\w.]+:\\w+)$\", netloc)\n     if not m:\n         raise ValueError(\"entrypoint must be like \"package.module:function\"; got \"%s\"\" % netloc)\n         return 1\n     return m.group(1), kwargs", "query": "parse query string in url"}
{"id": "https://github.com/crazy-canux/arguspy/blob/e9486b5df61978a990d56bf43de35f3a4cdefcc3/scripts/check_wmi_sh.py#L226-L238", "method_name": "__get_current_datetime", "code": "def __get_current_datetime(self):\n         self.wql_time = \"SELECT LocalDateTime FROM Win32_OperatingSystem\"\n         self.current_time = self.query(self.wql_time)\n         self.current_time_string = str(\n             self.current_time[0].get(\"LocalDateTime\").split(\".\")[0])\n         self.current_time_format = datetime.datetime.strptime(\n             self.current_time_string, \"%Y%m%d%H%M%S\")\n         return self.current_time_format", "query": "get current date"}
{"id": "https://github.com/fkarb/xltable/blob/7a592642d27ad5ee90d2aa8c26338abaa9d84bea/xltable/workbook.py#L82-L123", "method_name": "to_excel", "code": "def to_excel(self, xl_app=None, resize_columns=True):\n         from win32com.client import Dispatch, gencache\n         if xl_app is None:\n             xl_app = Dispatch(\"Excel.Application\")\n         xl_app = gencache.EnsureDispatch(xl_app)\n         assert self.worksheets, \"Can\"t export workbook with no worksheets\"\n         sheets_in_new_workbook = xl_app.SheetsInNewWorkbook\n         try:\n             xl_app.SheetsInNewWorkbook = float(len(self.worksheets))\n             self.workbook_obj = xl_app.Workbooks.Add()\n         finally:\n             xl_app.SheetsInNewWorkbook = sheets_in_new_workbook\n         sheet_names = {s.name for s in self.worksheets}\n         assert len(sheet_names) == len(self.worksheets), \"Worksheets must have unique names\"\n         for worksheet in self.workbook_obj.Sheets:\n             i = 1\n             original_name = worksheet.Name\n             while worksheet.Name in sheet_names:\n                 worksheet.Name = \"%s_%d\" % (original_name, i)\n                 i += 1\n         for worksheet, sheet in zip(self.workbook_obj.Sheets, self.worksheets):\n             worksheet.Name = sheet.name\n         for worksheet, sheet in zip(self.workbook_obj.Sheets, self.itersheets()):\n             worksheet.Select()\n             sheet.to_excel(workbook=self,\n                            worksheet=worksheet,\n                            xl_app=xl_app,\n                            rename=False,\n                            resize_columns=resize_columns)\n         return self.workbook_obj", "query": "export to excel"}
{"id": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/queues/priority_queue.py#L38-L49", "method_name": "push", "code": "def push(self, item, priority=None):\n         priority = item if priority is None else priority\n         node = PriorityQueueNode(item, priority)\n         for index, current in enumerate(self.priority_queue_list):\n             if current.priority < node.priority:\n                 self.priority_queue_list.insert(index, node)\n                 return\n         self.priority_queue_list.append(node)", "query": "priority queue"}
{"id": "https://github.com/alvinwan/md2py/blob/7f28a008367d7d9b5b3c8bbf7baf17985271489f/md2py/md2py.py#L140-L150", "method_name": "fromHTML", "code": "def fromHTML(html, *args, **kwargs):\n         source = BeautifulSoup(html, \"html.parser\", *args, **kwargs)\n         return TOC(\"[document]\",\n             source=source,\n             descendants=source.children)", "query": "reading element from html - <td>"}
{"id": "https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/xlsx.py#L152-L193", "method_name": "export_to_xlsx", "code": "def export_to_xlsx(table, filename_or_fobj=None, sheet_name=\"Sheet1\", *args, **kwargs):\n     workbook = Workbook()\n     sheet = workbook.active\n     sheet.title = sheet_name\n     prepared_table = prepare_to_export(table, *args, **kwargs)\n     field_names = next(prepared_table)\n     for col_index, field_name in enumerate(field_names):\n         cell = sheet.cell(row=1, column=col_index + 1)\n         cell.value = field_name\n     _convert_row = _python_to_cell(list(map(table.fields.get, field_names)))\n     for row_index, row in enumerate(prepared_table, start=1):\n         for col_index, (value, number_format) in enumerate(_convert_row(row)):\n             cell = sheet.cell(row=row_index + 1, column=col_index + 1)\n             cell.value = value\n             if number_format is not None:\n                 cell.number_format = number_format\n     return_result = False\n     if filename_or_fobj is None:\n         filename_or_fobj = BytesIO()\n         return_result = True\n     source = Source.from_file(filename_or_fobj, mode=\"wb\", plugin_name=\"xlsx\")\n     workbook.save(source.fobj)\n     source.fobj.flush()\n     if return_result:\n         source.fobj.seek(0)\n         result = source.fobj.read()\n     else:\n         result = source.fobj\n     if source.should_close:\n         source.fobj.close()\n     return result", "query": "export to excel"}
{"id": "https://github.com/BernardFW/bernard/blob/9c55703e5ffe5717c9fa39793df59dbfa5b4c5ab/src/bernard/i18n/_formatter.py#L80-L85", "method_name": "format_date", "code": "def format_date(self, value, format_):\n         date_ = make_date(value)\n         return dates.format_date(date_, format_, locale=self.lang)", "query": "format date"}
{"id": "https://github.com/ianlini/bistiming/blob/46a78ec647723c3516fc4fc73f2619ab41f647f2/examples/stopwatch_examples.py#L107-L119", "method_name": "cumulative_elapsed_time_example", "code": "def cumulative_elapsed_time_example():\n     print(\"[cumulative_elapsed_time_example] use python logging module with different log level\")\n     timer = Stopwatch(\"Waiting\")\n     with timer:\n         sleep(1)\n     sleep(1)\n     with timer:\n         sleep(1)\n         timer.log_elapsed_time(prefix=\"timer.log_elapsed_time(): \")  \n         print(\"timer.get_elapsed_time():\", timer.get_elapsed_time())  \n     print(\"timer.split_elapsed_time:\", timer.split_elapsed_time)\n     print(\"timer.get_cumulative_elapsed_time():\", timer.get_cumulative_elapsed_time())", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/gurumate-2.8.6-py2.7.egg/gurumate/linux2/procs.py#L43-L46", "method_name": "get_pid", "code": "def get_pid(PROCNAME):\n     for proc in psutil.process_iter():\n         if proc.name == PROCNAME:\n             return proc.pid", "query": "get current process id"}
{"id": "https://github.com/jazzband/sorl-thumbnail/blob/22ccd9781462a820f963f57018ad3dcef85053ed/sorl/thumbnail/helpers.py#L55-L58", "method_name": "deserialize", "code": "def deserialize(s):\n     if isinstance(s, bytes):\n         return json.loads(s.decode(\"utf-8\"))\n     return json.loads(s)", "query": "deserialize json"}
{"id": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/audiofile.py#L330-L376", "method_name": "read_properties", "code": "def read_properties(self):\n         self.log(u\"Reading properties...\")\n         if not gf.file_can_be_read(self.file_path):\n             self.log_exc(u\"File \"%s\" cannot be read\" % (self.file_path), None, True, OSError)\n         self.log([u\"Getting file size for \"%s\"\", self.file_path])\n         self.file_size = gf.file_size(self.file_path)\n         self.log([u\"File size for \"%s\" is \"%d\"\", self.file_path, self.file_size])\n         try:\n             self.log(u\"Reading properties with FFPROBEWrapper...\")\n             properties = FFPROBEWrapper(\n                 rconf=self.rconf,\n                 logger=self.logger\n             ).read_properties(self.file_path)\n             self.log(u\"Reading properties with FFPROBEWrapper... done\")\n         except FFPROBEPathError:\n             self.log_exc(u\"Unable to call ffprobe executable\", None, True, AudioFileProbeError)\n         except (FFPROBEUnsupportedFormatError, FFPROBEParsingError):\n             self.log_exc(u\"Audio file format not supported by ffprobe\", None, True, AudioFileUnsupportedFormatError)\n         self.audio_length = TimeValue(properties[FFPROBEWrapper.STDOUT_DURATION])\n         self.audio_format = properties[FFPROBEWrapper.STDOUT_CODEC_NAME]\n         self.audio_sample_rate = gf.safe_int(properties[FFPROBEWrapper.STDOUT_SAMPLE_RATE])\n         self.audio_channels = gf.safe_int(properties[FFPROBEWrapper.STDOUT_CHANNELS])\n         self.log([u\"Stored audio_length: \"%s\"\", self.audio_length])\n         self.log([u\"Stored audio_format: \"%s\"\", self.audio_format])\n         self.log([u\"Stored audio_sample_rate: \"%s\"\", self.audio_sample_rate])\n         self.log([u\"Stored audio_channels: \"%s\"\", self.audio_channels])\n         self.log(u\"Reading properties... done\")", "query": "read properties file"}
{"id": "https://github.com/dswah/pyGAM/blob/b3e5c3cd580f0a3ad69f9372861624f67760c325/pygam/pygam.py#L1572-L1661", "method_name": "summary", "code": "def summary(self):\n         if not self._is_fitted:\n             raise AttributeError(\"GAM has not been fitted. Call fit first.\")\n         width_details = 47\n         width_results = 58\n         model_fmt = [\n             (self.__class__.__name__, \"model_details\", width_details),\n             (\"\", \"model_results\", width_results)\n             ]\n         model_details = []\n         objective = \"UBRE\" if self.distribution._known_scale else \"GCV\"\n         model_details.append({\"model_details\": space_row(\"Distribution:\", self.distribution.__class__.__name__, total_width=width_details),\n                               \"model_results\": space_row(\"Effective DoF:\", str(np.round(self.statistics_[\"edof\"], 4)), total_width=width_results)})\n         model_details.append({\"model_details\": space_row(\"Link Function:\", self.link.__class__.__name__, total_width=width_details),\n                               \"model_results\": space_row(\"Log Likelihood:\", str(np.round(self.statistics_[\"loglikelihood\"], 4)), total_width=width_results)})\n         model_details.append({\"model_details\": space_row(\"Number of Samples:\", str(self.statistics_[\"n_samples\"]), total_width=width_details),\n                               \"model_results\": space_row(\"AIC: \", str(np.round(self.statistics_[\"AIC\"], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(\"AICc: \", str(np.round(self.statistics_[\"AICc\"], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(objective + \":\", str(np.round(self.statistics_[objective], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(\"Scale:\", str(np.round(self.statistics_[\"scale\"], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(\"Pseudo R-Squared:\", str(np.round(self.statistics_[\"pseudo_r2\"][\"explained_deviance\"], 4)), total_width=width_results)})\n         data = []\n         for i, term in enumerate(self.terms):\n             if len(self.statistics_[\"edof_per_coef\"]) == len(self.coef_):\n                 idx = self.terms.get_coef_indices(i)\n                 edof = np.round(self.statistics_[\"edof_per_coef\"][idx].sum(), 1)\n             else:\n                 edof = \"\"\n             term_data = {\n                         \"feature_func\": repr(term),\n                         \"lam\": \"\" if term.isintercept else np.round(flatten(term.lam), 4),\n                         \"rank\": \"{}\".format(term.n_coefs),\n                         \"edof\": \"{}\".format(edof),\n                         \"p_value\": \"%.2e\"%(self.statistics_[\"p_values\"][i]),\n                         \"sig_code\": sig_code(self.statistics_[\"p_values\"][i])\n                         }\n             data.append(term_data)\n         fmt = [\n             (\"Feature Function\", \"feature_func\", 33),\n             (\"Lambda\", \"lam\", 20),\n             (\"Rank\", \"rank\", 12),\n             (\"EDoF\", \"edof\", 12),\n             (\"P > x\", \"p_value\", 12),\n             (\"Sig. Code\", \"sig_code\", 12)\n             ]\n         print( TablePrinter(model_fmt, ul=\"=\", sep=\" \")(model_details) )\n         print(\"=\"*106)\n         print( TablePrinter(fmt, ul=\"=\")(data) )\n         print(\"=\"*106)\n         print(\"Significance codes:  0 \"***\" 0.001 \"**\" 0.01 \"*\" 0.05 \".\" 0.1 \" \" 1\")\n         print()\n         print(\"WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n\" \\\n               \"         which can cause p-values to appear significant when they are not.\")\n         print()\n         print(\"WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n\" \\\n               \"         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n\" \\\n               \"         are typically lower than they should be, meaning that the tests reject the null too readily.\")\n         warnings.warn(\"KNOWN BUG: p-values computed in this summary are likely \"\\\n                       \"much smaller than they should be. \n\"\\\n                       \"Please do not make inferences based on these values! \n\"\\\n                       \"Collaborate on a solution, and stay up to date at: \n\"\\\n                       \"github.com/dswah/pyGAM/issues/163 \n\", stacklevel=2)", "query": "print model summary"}
{"id": "https://github.com/jcassee/django-geckoboard/blob/6ebdaa86015fe645360abf1ba1290132de4cf6d6/django_geckoboard/decorators.py#L446-L460", "method_name": "_encrypt", "code": "def _encrypt(data):\n     BS = AES.block_size\n     def pad(s):\n         n = BS - len(s) % BS\n         char = chr(n).encode(\"utf8\")\n         return s + n * char\n     password = settings.GECKOBOARD_PASSWORD\n     salt = Random.new().read(BS - len(\"Salted__\"))\n     key, iv = _derive_key_and_iv(password, salt, 32, BS)\n     cipher = AES.new(key, AES.MODE_CBC, iv)\n     encrypted = b\"Salted__\" + salt + cipher.encrypt(pad(data))\n     return base64.b64encode(encrypted)", "query": "aes encryption"}
{"id": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/grid/grid_search.py#L439-L457", "method_name": "summary", "code": "def summary(self, header=True):\n         table = []\n         for model in self.models:\n             model_summary = model._model_json[\"output\"][\"model_summary\"]\n             r_values = list(model_summary.cell_values[0])\n             r_values[0] = model.model_id\n             table.append(r_values)\n         print()\n         if header:\n             print(\"Grid Summary:\")\n         print()\n         H2ODisplay(table, [\"Model Id\"] + model_summary.col_header[1:], numalign=\"left\", stralign=\"left\")", "query": "print model summary"}
{"id": "https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/utils.py#L1300-L1316", "method_name": "find_windows_executable", "code": "def find_windows_executable(bin_path, exe_name):\n     requested_path = get_windows_path(bin_path, exe_name)\n     if os.path.isfile(requested_path):\n         return requested_path\n     try:\n         pathext = os.environ[\"PATHEXT\"]\n     except KeyError:\n         pass\n     else:\n         for ext in pathext.split(os.pathsep):\n             path = get_windows_path(bin_path, exe_name + ext.strip().lower())\n             if os.path.isfile(path):\n                 return path\n     return find_executable(exe_name)", "query": "get executable path"}
{"id": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Model_Data.py#L176-L203", "method_name": "linear_regression", "code": "def linear_regression(self):\n         model = LinearRegression()\n         scores = []\n         kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n         for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)):\n             model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])\n             scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test]))\n         mean_score = sum(scores) / len(scores)\n         self.models.append(model)\n         self.model_names.append(\"Linear Regression\")\n         self.max_scores.append(mean_score)\n         self.metrics[\"Linear Regression\"] = {}\n         self.metrics[\"Linear Regression\"][\"R2\"] = mean_score\n         self.metrics[\"Linear Regression\"][\"Adj R2\"] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])", "query": "linear regression"}
{"id": "https://github.com/maxweisspoker/simplebitcoinfuncs/blob/ad332433dfcc067e86d2e77fa0c8f1a27daffb63/simplebitcoinfuncs/miscbitcoinfuncs.py#L190-L199", "method_name": "inttoDER", "code": "def inttoDER(a):\n     o = dechex(a,1)\n     if int(o[:2],16) > 127:\n         o = \"00\" + o\n     olen = dechex(len(o)//2,1)\n     return \"02\" + olen + o", "query": "convert decimal to hex"}
{"id": "https://github.com/benedictpaten/sonLib/blob/1decb75bb439b70721ec776f685ce98e25217d26/bioio.py#L636-L641", "method_name": "padWord", "code": "def padWord(word, length=25):\n     if len(word) > length:\n         return word[:length]\n     if len(word) < length:\n         return word + \" \"*(length-len(word))\n     return word", "query": "determine a string is a valid word"}
{"id": "https://github.com/maartenbreddels/ipyvolume/blob/e68b72852b61276f8e6793bc8811f5b2432a155f/ipyvolume/widgets.py#L434-L441", "method_name": "scatter", "code": "def scatter(x, y, z, color=(1, 0, 0), s=0.01):\n     global _last_figure\n     fig = _last_figure\n     if fig is None:\n         fig = volshow(None)\n     fig.scatter = Scatter(x=x, y=y, z=z, color=color, size=s)\n     fig.volume.scatter = fig.scatter\n     return fig", "query": "scatter plot"}
{"id": "https://github.com/supercoderz/pyeurofx/blob/3f579bb6e4836dadb187df8c74a9d186ae7e39e7/eurofx/common.py#L88-L98", "method_name": "get_date", "code": "def get_date(date_string):\n     d=None\n     try:\n         d=datetime.datetime.strptime(date_string, \"%d %B %Y\").date()\n     except:\n         d=datetime.datetime.strptime(date_string, \"%Y-%m-%d\").date()\n     if d:\n         return d.strftime(\"%Y-%m-%d\")\n     else:\n         return date_string", "query": "string to date"}
{"id": "https://github.com/scivision/lowtran/blob/9954d859e53437436103f9ab54a7e2602ecaa1b7/lowtran/plots.py#L12-L49", "method_name": "plotscatter", "code": "def plotscatter(irrad: xarray.Dataset, c1: Dict[str, Any], log: bool = False):\n     fg = figure()\n     axs = fg.subplots(2, 1, sharex=True)\n     transtxt = \"Transmittance\"\n     ax = axs[0]\n     ax.plot(irrad.wavelength_nm, irrad[\"transmission\"].squeeze())\n     ax.set_title(transtxt)\n     ax.set_ylabel(\"Transmission (unitless)\")\n     ax.grid(True)\n     ax.legend(irrad.angle_deg.values)\n     ax = axs[1]\n     if plotNp:\n         Np = (irrad[\"pathscatter\"]*10000) * (irrad.wavelength_nm*1e9)/(h*c)\n         ax.plot(irrad.wavelength_nm, Np)\n         ax.set_ylabel(\"Photons [s$^{-1}$ \"+UNITS)\n     else:\n         ax.plot(irrad.wavelength_nm, irrad[\"pathscatter\"].squeeze())\n         ax.set_ylabel(\"Radiance [W \"+UNITS)\n     ax.set_xlabel(\"wavelength [nm]\")\n     ax.set_title(\"Single-scatter Path Radiance\")\n     ax.invert_xaxis()\n     ax.autoscale(True, axis=\"x\", tight=True)\n     ax.grid(True)\n     if log:\n         ax.set_yscale(\"log\")\n     try:\n         fg.suptitle(f\"Obs. to Space: zenith angle: {c1[\"angle\"]} deg., \")\n     except (AttributeError, TypeError):\n         pass", "query": "scatter plot"}
{"id": "https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L20-L25", "method_name": "encrypt", "code": "def encrypt(self, data):\n         if self.iv is None:\n             cipher = pyAES.new(self.key, self.mode)\n         else:\n             cipher = pyAES.new(self.key, self.mode, self.iv)\n         return cipher.encrypt(pad_data(AES.str_to_bytes(data)))", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/Guake/guake/blob/4153ef38f9044cbed6494075fce80acd5809df2b/guake/terminal.py#L136-L141", "method_name": "copy_clipboard", "code": "def copy_clipboard(self):\n         if self.get_has_selection():\n             super(GuakeTerminal, self).copy_clipboard()\n         elif self.matched_value:\n             guake_clipboard = Gtk.Clipboard.get_default(self.guake.window.get_display())\n             guake_clipboard.set_text(self.matched_value, len(self.matched_value))", "query": "copy to clipboard"}
{"id": "https://github.com/fdb/aufmachen/blob/f2986a0cf087ac53969f82b84d872e3f1c6986f4/aufmachen/websites/immoweb.py#L115-L130", "method_name": "find_number", "code": "def find_number(regex, s):\n     result = find_string(regex, s)\n     if result is None:\n         return None\n     return int(result)", "query": "find int in string"}
{"id": "https://github.com/mclarkk/lifxlan/blob/ead0e3114d6aa2e5e77dab1191c13c16066c32b0/lifxlan/message.py#L126-L130", "method_name": "convert_MAC_to_int", "code": "def convert_MAC_to_int(addr):\n     reverse_bytes_str = addr.split(\":\")\n     reverse_bytes_str.reverse()\n     addr_str = \"\".join(reverse_bytes_str)\n     return int(addr_str, 16)", "query": "reverse a string"}
{"id": "https://github.com/census-instrumentation/opencensus-python/blob/992b223f7e34c5dcb65922b7d5c827e7a1351e7d/contrib/opencensus-ext-django/examples/app/views.py#L110-L130", "method_name": "sqlalchemy_mysql_trace", "code": "def sqlalchemy_mysql_trace(request):\n     try:\n         engine = sqlalchemy.create_engine(\n             \"mysql+mysqlconnector://{}:{}@{}\".format(\"root\", MYSQL_PASSWORD,\n                                                      DB_HOST))\n         conn = engine.connect()\n         query = \"SELECT 2*3\"\n         result_set = conn.execute(query)\n         result = []\n         for item in result_set:\n             result.append(item)\n         return HttpResponse(str(result))\n     except Exception:\n         msg = \"Query failed. Check your env vars for connection settings.\"\n         return HttpResponse(msg, status=500)", "query": "connect to sql"}
{"id": "https://github.com/OnroerendErfgoed/skosprovider_getty/blob/5aa0b5a8525d607e07b631499ff31bac7a0348b7/skosprovider_getty/providers.py#L390-L396", "method_name": "_sort", "code": "def _sort(self, items, sort, language=\"en\", reverse=False):\n         if sort is None:\n             sort = \"id\"\n         if sort == \"sortlabel\":\n             sort=\"label\"\n         items.sort(key=lambda item: item[sort], reverse=reverse)\n         return items", "query": "sort string list"}
{"id": "https://github.com/dnanexus/dx-toolkit/blob/74befb53ad90fcf902d8983ae6d74580f402d619/doc/examples/dx-apps/report_example/src/report_example.py#L127-L137", "method_name": "generate_html", "code": "def generate_html():\n     html_file = open(html_filename, \"w\")\n     html_file.write(\"<html><body>\")\n     html_file.write(\"<h1>Here are some graphs for you!</h1>\")\n     for image in [lines_filename, bars_filename, histogram_filename]:\n         html_file.write(\"<div><h2>{0}</h2><img src=\"{0}\" /></div>\".format(image))\n     html_file.write(\"</body></html>\")\n     html_file.close()", "query": "output to html file"}
{"id": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L9-L14", "method_name": "reverse_words", "code": "def reverse_words(string):\n     arr = string.strip().split()  \n     n = len(arr)\n     reverse(arr, 0, n-1)\n     return \" \".join(arr)", "query": "reverse a string"}
{"id": "https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/plotlywrapper.py#L832-L850", "method_name": "heatmap", "code": "def heatmap(z, x=None, y=None, colorscale=\"Viridis\"):\n     z = np.atleast_1d(z)\n     data = [go.Heatmap(z=z, x=x, y=y, colorscale=colorscale)]\n     return Chart(data=data)", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/erikrose/more-itertools/blob/6a91b4e25c8e12fcf9fc2b53cf8ee0fba293e6f9/more_itertools/more.py#L528-L566", "method_name": "distinct_permutations", "code": "def distinct_permutations(iterable):\n     def make_new_permutations(permutations, e):\n         for permutation in permutations:\n             for j in range(len(permutation)):\n                 yield permutation[:j] + [e] + permutation[j:]\n                 if permutation[j] == e:\n                     break\n             else:\n                 yield permutation + [e]\n     permutations = [[]]\n     for e in iterable:\n         permutations = make_new_permutations(permutations, e)\n     return (tuple(t) for t in permutations)", "query": "all permutations of a list"}
{"id": "https://github.com/AFriemann/simple_tools/blob/27d0f838c23309ebfc8afb59511220c6f8bb42fe/simple_tools/contextmanagers/time.py#L16-L20", "method_name": "timer", "code": "def timer(name):\n     startTime = time.time()\n     yield\n     elapsedTime = time.time() - startTime\n     print(\"[{}] finished in {} ms\".format(name, int(elapsedTime * 1000)))", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/ronaldguillen/wave/blob/20bb979c917f7634d8257992e6d449dc751256a9/wave/renderers.py#L568-L571", "method_name": "get_description", "code": "def get_description(self, view, status_code):\n         if status_code in (status.HTTP_401_UNAUTHORIZED, status.HTTP_403_FORBIDDEN):\n             return \"\"\n         return view.get_view_description(html=True)", "query": "get the description of a http status code"}
{"id": "https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/acquisitions/base.py#L52-L60", "method_name": "optimize", "code": "def optimize(self, duplicate_manager=None):\n         if not self.analytical_gradient_acq:\n             out = self.optimizer.optimize(f=self.acquisition_function, duplicate_manager=duplicate_manager)\n         else:\n             out = self.optimizer.optimize(f=self.acquisition_function, f_df=self.acquisition_function_withGradients, duplicate_manager=duplicate_manager)\n         return out", "query": "nelder mead optimize"}
{"id": "https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/im.py#L171-L200", "method_name": "_do_put", "code": "def _do_put(self, uri, **kwargs):\n         scaleioapi_put_headers = {\"content-type\":\"application/json\"}\n         print \"_do_put()\"\n         if kwargs:\n             for key, value in kwargs.iteritems():\n                 if key == \"json\":\n                     payload = value\n         try:\n             self.logger.debug(\"do_put(): \" + \"{}\".format(uri))\n             response = self._session.put(url, headers=scaleioapi_put_headers, verify_ssl=self._im_verify_ssl, data=json.dumps(payload))\n             self.logger.debug(\"_do_put() - Response: \" + \"{}\".format(response.text))\n             if response.status_code == requests.codes.ok:\n                 return response\n             else:\n                 self.logger.error(\"_do_put() - HTTP response error: \" + \"{}\".format(response.status_code))\n                 raise RuntimeError(\"_do_put() - HTTP response error\" + response.status_code)\n         except:\n             raise RuntimeError(\"_do_put() - Communication error with ScaleIO gateway\")\n         return response", "query": "custom http error response"}
{"id": "https://github.com/jim-easterbrook/python-gphoto2/blob/345e0f4457442ecd1aa4dc2ce8f8272c0910f91e/examples/cam-conf-view-gui.py#L265-L274", "method_name": "get_json_filters", "code": "def get_json_filters(args):\n     jsonfilters = []\n     if hasattr(args, \"include_names_json\"):\n         splitnames = args.include_names_json.split(\",\")\n         for iname in splitnames:\n             splitnameeq = iname.split(\"=\") \n             splitnameeq = list(filter(None, splitnameeq))\n             jsonfilters.append(splitnameeq)\n     jsonfilters = list(filter(None, jsonfilters))\n     return jsonfilters", "query": "filter array"}
{"id": "https://github.com/cablehead/vanilla/blob/c9f5b86f45720a30e8840fb68b1429b919c4ca66/vanilla/message.py#L57-L58", "method_name": "recv_n", "code": "def recv_n(self, n, timeout=-1):\n        return self.recver.recv_n(n, timeout=timeout)", "query": "socket recv timeout"}
{"id": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/grid/grid_search.py#L439-L457", "method_name": "summary", "code": "def summary(self, header=True):\n         table = []\n         for model in self.models:\n             model_summary = model._model_json[\"output\"][\"model_summary\"]\n             r_values = list(model_summary.cell_values[0])\n             r_values[0] = model.model_id\n             table.append(r_values)\n         print()\n         if header:\n             print(\"Grid Summary:\")\n         print()\n         H2ODisplay(table, [\"Model Id\"] + model_summary.col_header[1:], numalign=\"left\", stralign=\"left\")", "query": "print model summary"}
{"id": "https://github.com/mattja/sdeint/blob/7cf807cdf97b3bb39d29e1c2dc834b519499b601/sdeint/_broadcast.py#L70-L108", "method_name": "broadcast_to", "code": "def broadcast_to(array, shape, subok=False):\n     return _broadcast_to(array, shape, subok=subok, readonly=True)", "query": "readonly array"}
{"id": "https://github.com/dswah/pyGAM/blob/b3e5c3cd580f0a3ad69f9372861624f67760c325/pygam/pygam.py#L1572-L1661", "method_name": "summary", "code": "def summary(self):\n         if not self._is_fitted:\n             raise AttributeError(\"GAM has not been fitted. Call fit first.\")\n         width_details = 47\n         width_results = 58\n         model_fmt = [\n             (self.__class__.__name__, \"model_details\", width_details),\n             (\"\", \"model_results\", width_results)\n             ]\n         model_details = []\n         objective = \"UBRE\" if self.distribution._known_scale else \"GCV\"\n         model_details.append({\"model_details\": space_row(\"Distribution:\", self.distribution.__class__.__name__, total_width=width_details),\n                               \"model_results\": space_row(\"Effective DoF:\", str(np.round(self.statistics_[\"edof\"], 4)), total_width=width_results)})\n         model_details.append({\"model_details\": space_row(\"Link Function:\", self.link.__class__.__name__, total_width=width_details),\n                               \"model_results\": space_row(\"Log Likelihood:\", str(np.round(self.statistics_[\"loglikelihood\"], 4)), total_width=width_results)})\n         model_details.append({\"model_details\": space_row(\"Number of Samples:\", str(self.statistics_[\"n_samples\"]), total_width=width_details),\n                               \"model_results\": space_row(\"AIC: \", str(np.round(self.statistics_[\"AIC\"], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(\"AICc: \", str(np.round(self.statistics_[\"AICc\"], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(objective + \":\", str(np.round(self.statistics_[objective], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(\"Scale:\", str(np.round(self.statistics_[\"scale\"], 4)), total_width=width_results)})\n         model_details.append({\"model_results\": space_row(\"Pseudo R-Squared:\", str(np.round(self.statistics_[\"pseudo_r2\"][\"explained_deviance\"], 4)), total_width=width_results)})\n         data = []\n         for i, term in enumerate(self.terms):\n             if len(self.statistics_[\"edof_per_coef\"]) == len(self.coef_):\n                 idx = self.terms.get_coef_indices(i)\n                 edof = np.round(self.statistics_[\"edof_per_coef\"][idx].sum(), 1)\n             else:\n                 edof = \"\"\n             term_data = {\n                         \"feature_func\": repr(term),\n                         \"lam\": \"\" if term.isintercept else np.round(flatten(term.lam), 4),\n                         \"rank\": \"{}\".format(term.n_coefs),\n                         \"edof\": \"{}\".format(edof),\n                         \"p_value\": \"%.2e\"%(self.statistics_[\"p_values\"][i]),\n                         \"sig_code\": sig_code(self.statistics_[\"p_values\"][i])\n                         }\n             data.append(term_data)\n         fmt = [\n             (\"Feature Function\", \"feature_func\", 33),\n             (\"Lambda\", \"lam\", 20),\n             (\"Rank\", \"rank\", 12),\n             (\"EDoF\", \"edof\", 12),\n             (\"P > x\", \"p_value\", 12),\n             (\"Sig. Code\", \"sig_code\", 12)\n             ]\n         print( TablePrinter(model_fmt, ul=\"=\", sep=\" \")(model_details) )\n         print(\"=\"*106)\n         print( TablePrinter(fmt, ul=\"=\")(data) )\n         print(\"=\"*106)\n         print(\"Significance codes:  0 \"***\" 0.001 \"**\" 0.01 \"*\" 0.05 \".\" 0.1 \" \" 1\")\n         print()\n         print(\"WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n\" \\\n               \"         which can cause p-values to appear significant when they are not.\")\n         print()\n         print(\"WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n\" \\\n               \"         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n\" \\\n               \"         are typically lower than they should be, meaning that the tests reject the null too readily.\")\n         warnings.warn(\"KNOWN BUG: p-values computed in this summary are likely \"\\\n                       \"much smaller than they should be. \n\"\\\n                       \"Please do not make inferences based on these values! \n\"\\\n                       \"Collaborate on a solution, and stay up to date at: \n\"\\\n                       \"github.com/dswah/pyGAM/issues/163 \n\", stacklevel=2)", "query": "print model summary"}
{"id": "https://github.com/CellProfiler/centrosome/blob/7bd9350a2d4ae1b215b81eabcecfe560bbb1f32a/centrosome/filter.py#L1427-L1476", "method_name": "permutations", "code": "def permutations(x):\n     x = np.array(x)\n     a = np.arange(len(x))\n     while True:\n         ak_lt_ak_next = np.argwhere(a[:-1] < a[1:])\n         if len(ak_lt_ak_next) == 0:\n             raise StopIteration()\n         k = ak_lt_ak_next[-1, 0]\n         ak_lt_al = np.argwhere(a[k] < a)\n         l =  ak_lt_al[-1, 0]\n         a[k], a[l]  = (a[l], a[k])\n         if k < len(x)-1:\n             a[k+1:] = a[:k:-1].copy()\n         yield x[a].tolist()", "query": "all permutations of a list"}
{"id": "https://github.com/tzutalin/labelImg/blob/6afd15aa88f89f41254e0004ed219b3965eb2c0d/labelImg.py#L1295-L1297", "method_name": "saveFileAs", "code": "def saveFileAs(self, _value=False):\n        assert not self.image.isNull(), \"cannot save empty image\"\n        self._saveFile(self.saveFileDialog())", "query": "save list to file"}
{"id": "https://github.com/jaredLunde/vital-tools/blob/ea924c9bbb6ec22aa66f8095f018b1ee0099ac04/vital/security/__init__.py#L79-L98", "method_name": "aes_encrypt", "code": "def aes_encrypt(value, secret, block_size=AES.block_size):\n     iv = os.urandom(block_size * 2)\n     cipher = AES.new(secret[:32], AES.MODE_CFB, iv[:block_size])\n     return b\"%s%s\" % (iv, cipher.encrypt(value))", "query": "aes encryption"}
{"id": "https://github.com/zeromake/aiko/blob/53b246fa88652466a9e38ac3d1a99a6198195b0f/aiko/request.py#L329-L336", "method_name": "url", "code": "def url(self) -> str:\n         url_str = self.parse_url.path or \"\"\n         if self.parse_url.querystring is not None:\n             url_str += \"?\" + self.parse_url.querystring\n         return url_str", "query": "parse query string in url"}
{"id": "https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/cookie.py#L369-L404", "method_name": "append_cookie", "code": "def append_cookie(self, cookie, name, payload, typ, domain=None, path=None,\n                       timestamp=\"\", max_age=0):\n         timestamp = str(int(time.time()))\n         try:\n             _payload = \"::\".join([payload, timestamp, typ])\n         except TypeError:\n             _payload = \"::\".join([payload[0], timestamp, typ])\n         content = make_cookie_content(name, _payload, self.sign_key, domain=domain,\n                                       path=path, timestamp=timestamp,\n                                       enc_key=self.enc_key, max_age=max_age,\n                                       sign_alg=self.sign_alg)\n         for name, args in content.items():\n             cookie[name] = args[\"value\"]\n             for key, value in args.items():\n                 if key == \"value\":\n                     continue\n                 cookie[name][key] = value\n         return cookie", "query": "create cookie"}
{"id": "https://github.com/jldantas/libmft/blob/65a988605fe7663b788bd81dcb52c0a4eaad1549/libmft/attribute.py#L1599-L1630", "method_name": "_from_binary_reparse", "code": "def _from_binary_reparse(cls, binary_stream):\n     reparse_tag, data_len = cls._REPR.unpack(binary_stream[:cls._REPR.size])\n     reparse_type = ReparseType(reparse_tag & 0x0000FFFF)\n     reparse_flags = ReparseFlags((reparse_tag & 0xF0000000) >> 28)\n     guid = None \n     if reparse_flags & ReparseFlags.IS_MICROSOFT:\n         if reparse_type is ReparseType.SYMLINK:\n             data = SymbolicLink.create_from_binary(binary_stream[cls._REPR.size:])\n         elif reparse_type is ReparseType.MOUNT_POINT:\n             data = JunctionOrMount.create_from_binary(binary_stream[cls._REPR.size:])\n         else:\n             data = binary_stream[cls._REPR.size:].tobytes()\n     else:\n         guid = UUID(bytes_le=binary_stream[cls._REPR.size:cls._REPR.size+16].tobytes())\n         data = binary_stream[cls._REPR.size+16:].tobytes()\n     nw_obj = cls((reparse_type, reparse_flags, data_len, guid, data))\n     _MOD_LOGGER.debug(\"Attempted to unpack REPARSE_POINT from \\\"%s\\\"\nResult: %s\", binary_stream.tobytes(), nw_obj)\n     return nw_obj", "query": "parse binary file to custom class"}
{"id": "https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/examples/aesctr_performance.py#L113-L115", "method_name": "main", "code": "def main():\n    runtest(\"AES - CTR Mode\", AES.CTREnc, AES.CTRDec)\n    runtest(\"AES - GCM Mode\", AES.GCMEnc, AES.GCMDec)", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L72-L76", "method_name": "aes_cbc_encrypt", "code": "def aes_cbc_encrypt(plain_text: bytes, key: bytes, iv: bytes = b\"\"):\n         if len(iv) == 0:\n             iv = AESHandler.generate_iv()\n         cipher = AES.new(key=key, mode=AES.MODE_CBC, iv=iv)\n         return cipher.IV, cipher.encrypt(pad(plain_text, AES.block_size))", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/JoseAntFer/pyny3d/blob/fb81684935a24f7e50c975cb4383c81a63ab56df/pyny3d/utils.py#L8-L33", "method_name": "sort_numpy", "code": "def sort_numpy(array, col=0, order_back=False): \n     x = array[:,col] \n     sorted_index = np.argsort(x, kind = \"quicksort\") \n     sorted_array = array[sorted_index] \n     if not order_back: \n         return sorted_array \n     else: \n         n_points = sorted_index.shape[0] \n         order_back = np.empty(n_points, dtype=int) \n         order_back[sorted_index] = np.arange(n_points) \n         return [sorted_array, order_back]", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/custom_destinations.py#L85-L209", "method_name": "ParseFileObject", "code": "def ParseFileObject(self, parser_mediator, file_object):\n     file_entry = parser_mediator.GetFileEntry()\n     display_name = parser_mediator.GetDisplayName()\n     file_header_map = self._GetDataTypeMap(\"custom_file_header\")\n     try:\n       file_header, file_offset = self._ReadStructureFromFileObject(\n           file_object, 0, file_header_map)\n     except (ValueError, errors.ParseError) as exception:\n       raise errors.UnableToParseFile((\n           \"Invalid Custom Destination: {0:s} - unable to parse file header \"\n           \"with error: {1!s}\").format(display_name, exception))\n     if file_header.unknown1 != 2:\n       raise errors.UnableToParseFile((\n           \"Unsupported Custom Destination file: {0:s} - invalid unknown1: \"\n           \"{1:d}.\").format(display_name, file_header.unknown1))\n     if file_header.header_values_type > 2:\n       raise errors.UnableToParseFile((\n           \"Unsupported Custom Destination file: {0:s} - invalid header value \"\n           \"type: {1:d}.\").format(display_name, file_header.header_values_type))\n     if file_header.header_values_type == 0:\n       data_map_name = \"custom_file_header_value_type_0\"\n     else:\n       data_map_name = \"custom_file_header_value_type_1_or_2\"\n     file_header_value_map = self._GetDataTypeMap(data_map_name)\n     try:\n       _, value_data_size = self._ReadStructureFromFileObject(\n           file_object, file_offset, file_header_value_map)\n     except (ValueError, errors.ParseError) as exception:\n       raise errors.UnableToParseFile((\n           \"Invalid Custom Destination: {0:s} - unable to parse file header \"\n           \"value with error: {1!s}\").format(display_name, exception))\n     file_offset += value_data_size\n     file_size = file_object.get_size()\n     remaining_file_size = file_size - file_offset\n     entry_header_map = self._GetDataTypeMap(\"custom_entry_header\")\n     file_footer_map = self._GetDataTypeMap(\"custom_file_footer\")\n     first_guid_checked = False\n     while remaining_file_size > 4:\n       try:\n         entry_header, entry_data_size = self._ReadStructureFromFileObject(\n             file_object, file_offset, entry_header_map)\n       except (ValueError, errors.ParseError) as exception:\n         if not first_guid_checked:\n           raise errors.UnableToParseFile((\n               \"Invalid Custom Destination file: {0:s} - unable to parse \"\n               \"entry header with error: {1!s}\").format(\n                   display_name, exception))\n         parser_mediator.ProduceExtractionWarning(\n             \"unable to parse entry header with error: {0!s}\".format(\n                 exception))\n         break\n       if entry_header.guid != self._LNK_GUID:\n         if not first_guid_checked:\n           raise errors.UnableToParseFile((\n               \"Unsupported Custom Destination file: {0:s} - invalid entry \"\n               \"header signature offset: 0x{1:08x}.\").format(\n                   display_name, file_offset))\n         try:\n           file_footer, _ = self._ReadStructureFromFileObject(\n               file_object, file_offset, file_footer_map)\n           if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n             parser_mediator.ProduceExtractionWarning(\n                 \"invalid entry header signature at offset: 0x{0:08x}\".format(\n                     file_offset))\n         except (ValueError, errors.ParseError) as exception:\n           parser_mediator.ProduceExtractionWarning((\n               \"unable to parse footer at offset: 0x{0:08x} with error: \"\n               \"{1!s}\").format(file_offset, exception))\n           break\n         break\n       first_guid_checked = True\n       file_offset += entry_data_size\n       remaining_file_size -= entry_data_size\n       lnk_file_size = self._ParseLNKFile(\n           parser_mediator, file_entry, file_offset, remaining_file_size)\n       file_offset += lnk_file_size\n       remaining_file_size -= lnk_file_size\n     try:\n       file_footer, _ = self._ReadStructureFromFileObject(\n           file_object, file_offset, file_footer_map)\n       if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n         parser_mediator.ProduceExtractionWarning(\n             \"invalid footer signature at offset: 0x{0:08x}\".format(file_offset))\n     except (ValueError, errors.ParseError) as exception:\n       parser_mediator.ProduceExtractionWarning((\n           \"unable to parse footer at offset: 0x{0:08x} with error: \"\n           \"{1!s}\").format(file_offset, exception))", "query": "parse binary file to custom class"}
{"id": "https://github.com/Shoobx/xmldiff/blob/ec7835bce9ba69ff4ce03ab6c11397183b6f8411/xmldiff/_diff_match_patch_py3.py#L1110-L1134", "method_name": "diff_levenshtein", "code": "def diff_levenshtein(self, diffs):\n     levenshtein = 0\n     insertions = 0\n     deletions = 0\n     for (op, data) in diffs:\n       if op == self.DIFF_INSERT:\n         insertions += len(data)\n       elif op == self.DIFF_DELETE:\n         deletions += len(data)\n       elif op == self.DIFF_EQUAL:\n         levenshtein += max(insertions, deletions)\n         insertions = 0\n         deletions = 0\n     levenshtein += max(insertions, deletions)\n     return levenshtein", "query": "string similarity levenshtein"}
{"id": "https://github.com/chaoss/grimoirelab-manuscripts/blob/94a3ad4f11bfbcd6c5190e01cb5d3e47a5187cd9/manuscripts/report.py#L812-L824", "method_name": "replace_text", "code": "def replace_text(filepath, to_replace, replacement):\n         with open(filepath) as file:\n             s = file.read()\n         s = s.replace(to_replace, replacement)\n         with open(filepath, \"w\") as file:\n             file.write(s)", "query": "replace in file"}
{"id": "https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L306-L311", "method_name": "write_csv", "code": "def write_csv(filename, T, header = None):\n     with open(filename,\"w\") as fh:\n         csv_writer = csv.writer(fh, delimiter=\",\")\n         if header != None:\n             csv_writer.writerow(header)\n         [csv_writer.writerow(T[i]) for i in range(len(T))]", "query": "write csv"}
{"id": "https://github.com/pymacaron/pymacaron/blob/af244f203f8216108b39d374d46bf8e1813f13d5/pymacaron/utils.py#L48-L60", "method_name": "to_epoch", "code": "def to_epoch(t):\n     if isinstance(t, str):\n         if \"+\" not in t:\n             t = t + \"+00:00\"\n         t = parser.parse(t)\n     elif t.tzinfo is None or t.tzinfo.utcoffset(t) is None:\n         t = t.replace(tzinfo=pytz.timezone(\"utc\"))\n     t0 = datetime.datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone(\"utc\"))\n     delta = t - t0\n     return int(delta.total_seconds())", "query": "convert a utc time to epoch"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/utils/stringmatching.py#L18-L47", "method_name": "get_search_regex", "code": "def get_search_regex(query, ignore_case=True):\n     regex_text = [char for char in query if char != \" \"]\n     regex_text = \".*\".join(regex_text)\n     regex = r\"({0})\".format(regex_text)\n     if ignore_case:\n         pattern = re.compile(regex, re.IGNORECASE)\n     else:\n         pattern = re.compile(regex)\n     return pattern", "query": "regex case insensitive"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/noaa_lpd.py#L546-L556", "method_name": "__convert_num", "code": "def __convert_num(number):\n         try:\n             return float(number)\n         except ValueError as e:\n             logger_noaa_lpd.warn(\"convert_num: ValueError: {}\".format(e))\n             return number", "query": "convert string to number"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/widgets/mixins.py#L499-L507", "method_name": "get_text_line", "code": "def get_text_line(self, line_nb): \n         try: \n             return to_text_string(self.toPlainText()).splitlines()[line_nb] \n         except IndexError: \n             return self.get_line_separator()", "query": "read text file line by line"}
{"id": "https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/util/url.py#L72-L103", "method_name": "_urlparse_qs", "code": "def _urlparse_qs(url):\n     querystring = urlparse(url)[4]\n     pairs = [s2 for s1 in querystring.split(\"&\") for s2 in s1.split(\";\")]\n     result = OrderedDefaultDict(list)\n     for name_value in pairs:\n         pair = name_value.split(\"=\", 1)\n         if len(pair) != 2:\n             continue\n         if len(pair[1]) > 0:\n             name = _unquote(pair[0].replace(\"+\", \" \"))\n             value = _unquote(pair[1].replace(\"+\", \" \"))\n             result[name].append(value)\n     return result", "query": "parse query string in url"}
{"id": "https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/controllers/monsoon.py#L562-L578", "method_name": "from_text_file", "code": "def from_text_file(file_path):\n         results = []\n         with io.open(file_path, \"r\", encoding=\"utf-8\") as f:\n             data_strs = f.read().split(MonsoonData.delimiter)\n             for data_str in data_strs:\n                 results.append(MonsoonData.from_string(data_str))\n         return results", "query": "extracting data from a text file"}
{"id": "https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/graph/cluster.py#L16-L20", "method_name": "unique", "code": "def unique(list):\n     unique = []; [unique.append(x) for x in list if x not in unique]\n     return unique", "query": "unique elements"}
{"id": "https://github.com/vstinner/perf/blob/cf096c0c0c955d0aa1c893847fa6393ba4922ada/perf/_utils.py#L481-L484", "method_name": "median_abs_dev", "code": "def median_abs_dev(values):\n     median = float(statistics.median(values))\n     return statistics.median([abs(median - sample) for sample in values])", "query": "deducting the median from each column"}
{"id": "https://github.com/ManiacalLabs/BiblioPixelAnimations/blob/fba81f6b94f5265272a53f462ef013df1ccdb426/BiblioPixelAnimations/strip/WhiteTwinkle.py#L68-L77", "method_name": "pick_led", "code": "def pick_led(self, inc):\n         idx = random.randrange(0, self.layout.numLEDs)\n         this_led = self.layout.get(idx)\n         r = this_led[0]\n         if random.randrange(0, self._maxLed) < self.density:\n             if r == 0:\n                 r += inc\n                 self.layout.set(idx, (2, 2, 2))", "query": "randomly pick a number"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/convert.py#L43-L73", "method_name": "convert_to_bool", "code": "def convert_to_bool(x: Any, default: bool = None) -> bool:\n     if isinstance(x, bool):\n         return x\n     if not x:  \n         return default\n     try:\n         return int(x) != 0\n     except (TypeError, ValueError):\n         pass\n     try:\n         return float(x) != 0\n     except (TypeError, ValueError):\n         pass\n     if not isinstance(x, str):\n         raise Exception(\"Unknown thing being converted to bool: {!r}\".format(x))\n     x = x.upper()\n     if x in [\"Y\", \"YES\", \"T\", \"TRUE\"]:\n         return True\n     if x in [\"N\", \"NO\", \"F\", \"FALSE\"]:\n         return False\n     raise Exception(\"Unknown thing being converted to bool: {!r}\".format(x))", "query": "convert int to bool"}
{"id": "https://github.com/defensio/defensio-python/blob/c1d2b64be941acb63c452a6d9a5526c59cb37007/defensio/__init__.py#L121-L125", "method_name": "_urlencode", "code": "def _urlencode(self, url):\n       if is_python3():\n         return urllib.parse.urlencode(url)\n       else:\n         return urllib.urlencode(url)", "query": "encode url"}
{"id": "https://github.com/GNS3/gns3-server/blob/a221678448fb5d24e977ef562f81d56aacc89ab1/gns3server/compute/vmware/vmware_vm.py#L75-L91", "method_name": "__json__", "code": "def __json__(self):\n         json = {\"name\": self.name,\n                 \"node_id\": self.id,\n                 \"console\": self.console,\n                 \"console_type\": self.console_type,\n                 \"project_id\": self.project.id,\n                 \"vmx_path\": self.vmx_path,\n                 \"headless\": self.headless,\n                 \"acpi_shutdown\": self.acpi_shutdown,\n                 \"adapters\": self._adapters,\n                 \"adapter_type\": self.adapter_type,\n                 \"use_any_adapter\": self.use_any_adapter,\n                 \"status\": self.status,\n                 \"node_directory\": self.working_path,\n                 \"linked_clone\": self.linked_clone}\n         return json", "query": "map to json"}
{"id": "https://github.com/timothyb0912/pylogit/blob/f83b0fd6debaa7358d87c3828428f6d4ead71357/pylogit/base_multinomial_cm_v2.py#L1556-L1572", "method_name": "print_summaries", "code": "def print_summaries(self):\n         if hasattr(self, \"fit_summary\") and hasattr(self, \"summary\"):\n             print(\"\n\")\n             print(self.fit_summary)\n             print(\"=\" * 30)\n             print(self.summary)\n         else:\n             msg = \"This {} object has not yet been estimated so there \"\n             msg_2 = \"are no estimation summaries to print.\"\n             raise NotImplementedError(msg.format(self.model_type) + msg_2)\n         return None", "query": "print model summary"}
{"id": "https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/thread.py#L204-L226", "method_name": "get_pid", "code": "def get_pid(self):\n         if self.dwProcessId is None:\n             if self.__process is not None:\n                 self.dwProcessId = self.get_process().get_pid()\n             else:\n                 try:\n                     hThread = self.get_handle(\n                                         win32.THREAD_QUERY_LIMITED_INFORMATION)\n                     self.dwProcessId = win32.GetProcessIdOfThread(hThread)\n                 except AttributeError:\n                     self.dwProcessId = self.__get_pid_by_scanning()\n         return self.dwProcessId", "query": "get current process id"}
{"id": "https://github.com/LogicalDash/LiSE/blob/fe6fd4f0a7c1780e065f4c9babb9bc443af6bb84/ELiDE/ELiDE/board/pawn.py#L105-L108", "method_name": "on_priority", "code": "def on_priority(self, *args):\n         if self.proxy[\"_priority\"] != self.priority:\n             self.proxy[\"_priority\"] = self.priority\n         self.parent.restack()", "query": "priority queue"}
{"id": "https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/cli/html_content_extractor.py#L20-L32", "method_name": "run", "code": "def run(args):\n     html_content_extractor = HTMLContentExtractor()\n     with warnings.catch_warnings():\n         warnings.simplefilter(\"ignore\")\n         extractions = html_content_extractor.extract(html_text=args.input_file)\n         for e in extractions:\n             print(e.value)", "query": "extract data from html content"}
{"id": "https://github.com/Rapptz/discord.py/blob/05d4f7f9620ef33635d6ac965b26528e09cdaf5b/discord/ext/commands/core.py#L94-L101", "method_name": "_convert_to_bool", "code": "def _convert_to_bool(argument):\n     lowered = argument.lower()\n     if lowered in (\"yes\", \"y\", \"true\", \"t\", \"1\", \"enable\", \"on\"):\n         return True\n     elif lowered in (\"no\", \"n\", \"false\", \"f\", \"0\", \"disable\", \"off\"):\n         return False\n     else:\n         raise BadArgument(lowered + \" is not a recognised boolean option\")", "query": "convert int to bool"}
{"id": "https://github.com/pedroburon/tbk/blob/ecd6741e0bae06269eb4ac885c3ffcb7902ee40e/tbk/webpay/encryption.py#L36-L42", "method_name": "encrypt_message", "code": "def encrypt_message(self, signed_message, message, key, iv):\n         raw = signed_message + message\n         block_size = AES.block_size\n         pad = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size).encode(\"utf-8\")\n         message_to_encrypt = pad(raw)\n         cipher = AES.new(key, AES.MODE_CBC, iv)\n         return cipher.encrypt(message_to_encrypt)", "query": "aes encryption"}
{"id": "https://github.com/zblz/naima/blob/d6a6781d73bf58fd8269e8b0e3b70be22723cd5b/naima/extern/minimize.py#L66-L67", "method_name": "minimize", "code": "def minimize(func, x0, args=(), options={}, method=None):\n    return _minimize_neldermead(func, x0, args=args, **options)", "query": "nelder mead optimize"}
{"id": "https://github.com/RPi-Distro/python-gpiozero/blob/7b67374fd0c8c4fde5586d9bad9531f076db9c0c/gpiozero/mixins.py#L539-L546", "method_name": "value", "code": "def value(self):\n         if not self.partial:\n             self.full.wait()\n         try:\n             return self.average(self.queue)\n         except (ZeroDivisionError, ValueError):\n             return 0.0", "query": "get current observable value"}
{"id": "https://github.com/myusuf3/delorean/blob/3e8a7b8cfd4c26546f62bde2f34002893adfa08a/delorean/dates.py#L492-L512", "method_name": "epoch", "code": "def epoch(self):\n         epoch_sec = pytz.utc.localize(datetime.utcfromtimestamp(0))\n         now_sec = pytz.utc.normalize(self._dt)\n         delta_sec = now_sec - epoch_sec\n         return get_total_second(delta_sec)", "query": "convert a utc time to epoch"}
{"id": "https://github.com/amperser/proselint/blob/cb619ee4023cc7856f5fb96aec2a33a2c9f1a2e2/proselint/tools.py#L99-L147", "method_name": "memoize", "code": "def memoize(f):\n     cache_dirname = os.path.join(_get_xdg_cache_home(), \"proselint\")\n     legacy_cache_dirname = os.path.join(os.path.expanduser(\"~\"), \".proselint\")\n     if not os.path.isdir(cache_dirname):\n         if os.path.isdir(legacy_cache_dirname):\n             os.rename(legacy_cache_dirname, cache_dirname)\n         else:\n             os.makedirs(cache_dirname)\n     cache_filename = f.__module__ + \".\" + f.__name__\n     cachepath = os.path.join(cache_dirname, cache_filename)\n     @functools.wraps(f)\n     def wrapped(*args, **kwargs):\n         if hasattr(f, \"__self__\"):\n             args = args[1:]\n         signature = (f.__module__ + \".\" + f.__name__).encode(\"utf-8\")\n         tempargdict = inspect.getcallargs(f, *args, **kwargs)\n         for item in list(tempargdict.items()):\n             signature += item[1].encode(\"utf-8\")\n         key = hashlib.sha256(signature).hexdigest()\n         try:\n             cache = _get_cache(cachepath)\n             return cache[key]\n         except KeyError:\n             value = f(*args, **kwargs)\n             cache[key] = value\n             cache.sync()\n             return value\n         except TypeError:\n             call_to = f.__module__ + \".\" + f.__name__\n             print(\"Warning: could not disk cache call to %s;\"\n                   \"it probably has unhashable args. Error: %s\" %\n                   (call_to, traceback.format_exc()))\n             return f(*args, **kwargs)\n     return wrapped", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/alpacahq/alpaca-trade-api-python/blob/9c9dea3b4a37c909f88391b202e86ff356a8b4d7/alpaca_trade_api/rest.py#L40-L43", "method_name": "status_code", "code": "def status_code(self):\n         http_error = self._http_error\n         if http_error is not None and hasattr(http_error, \"response\"):\n             return http_error.response.status_code", "query": "custom http error response"}
{"id": "https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/doc/figures.py#L43-L45", "method_name": "heatmap2", "code": "def heatmap2():\n    x = np.arange(5)\n    pw.heatmap(z=np.arange(25), x=np.tile(x, 5), y=x.repeat(5)).save(\"fig_heatmap2.html\", **options)", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/geex-arts/django-jet/blob/64d5379f8a2278408694ce7913bf25f26035b855/jet/dashboard/dashboard_modules/google_analytics.py#L267-L279", "method_name": "format_grouped_date", "code": "def format_grouped_date(self, data, group):\n         date = self.get_grouped_date(data, group)\n         if group == \"week\":\n             date = u\"%s 鈥?%s\" % (\n                 (date - datetime.timedelta(days=6)).strftime(\"%d.%m\"),\n                 date.strftime(\"%d.%m\")\n             )\n         elif group == \"month\":\n             date = date.strftime(\"%b, %Y\")\n         else:\n             date = formats.date_format(date, \"DATE_FORMAT\")\n         return date", "query": "format date"}
{"id": "https://github.com/binux/pyspider/blob/3fccfabe2b057b7a56d4a4c79dc0dd6cd2239fe9/pyspider/libs/utils.py#L72-L130", "method_name": "format_date", "code": "def format_date(date, gmt_offset=0, relative=True, shorter=False, full_format=False):\n     if not date:\n         return \"-\"\n     if isinstance(date, float) or isinstance(date, int):\n         date = datetime.datetime.utcfromtimestamp(date)\n     now = datetime.datetime.utcnow()\n     if date > now:\n         if relative and (date - now).seconds < 60:\n             date = now\n         else:\n             full_format = True\n     local_date = date - datetime.timedelta(minutes=gmt_offset)\n     local_now = now - datetime.timedelta(minutes=gmt_offset)\n     local_yesterday = local_now - datetime.timedelta(hours=24)\n     difference = now - date\n     seconds = difference.seconds\n     days = difference.days\n     format = None\n     if not full_format:\n         ret_, fff_format = fix_full_format(days, seconds, relative, shorter, local_date, local_yesterday)\n         format = fff_format\n         if ret_:\n             return format\n         else:\n             format = format\n     if format is None:\n         format = \"%(month_name)s %(day)s, %(year)s\" if shorter else \\\n             \"%(month_name)s %(day)s, %(year)s at %(time)s\"\n     str_time = \"%d:%02d\" % (local_date.hour, local_date.minute)\n     return format % {\n         \"month_name\": local_date.strftime(\"%b\"),\n         \"weekday\": local_date.strftime(\"%A\"),\n         \"day\": str(local_date.day),\n         \"year\": str(local_date.year),\n         \"month\": local_date.month,\n         \"time\": str_time\n     }", "query": "format date"}
{"id": "https://github.com/ChrisCummins/labm8/blob/dd10d67a757aefb180cb508f86696f99440c94f5/text.py#L40-L52", "method_name": "get_substring_idxs", "code": "def get_substring_idxs(substr, string):\n     return [match.start() for match in re.finditer(substr, string)]", "query": "positions of substrings in string"}
{"id": "https://github.com/nkmathew/yasi-sexp-indenter/blob/6ec2a4675e79606c555bcb67494a0ba994b05805/yasi.py#L551-L568", "method_name": "parse_rc_json", "code": "def parse_rc_json():\n     fname = \".yasirc.json\"\n     path = os.path.expanduser(\"~/\" + fname)\n     if os.path.exists(fname):\n         path = os.path.abspath(fname)\n     elif not os.path.exists(path):\n         path = \"\"\n     content = \"\"\n     if path:\n         with open(path) as f:\n             content = f.read()\n     ret = {}\n     if content:\n         ret = json.loads(content)\n     return collections.defaultdict(dict, ret)", "query": "parse json file"}
{"id": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/regions/knn_classifier_region.py#L684-L688", "method_name": "_getAccuracy", "code": "def _getAccuracy(self):\n     n = self.confusion.shape[0]\n     assert n == self.confusion.shape[1], \"Confusion matrix is non-square.\"\n     return self.confusion[range(n), range(n)].sum(), self.confusion.sum()", "query": "confusion matrix"}
{"id": "https://github.com/jobec/rfc5424-logging-handler/blob/9c4f669c5e54cf382936cd950e2204caeb6d05f0/rfc5424logging/handler.py#L277-L281", "method_name": "get_procid", "code": "def get_procid(self, record):\n         procid = getattr(record, \"procid\", self.procid)\n         if procid is None or procid == \"\":\n             procid = getattr(record, \"process\", NILVALUE)\n         return self.filter_printusascii(str(procid))", "query": "get current process id"}
{"id": "https://github.com/QInfer/python-qinfer/blob/8170c84a0be1723f8c6b09e0d3c7a40a886f1fe3/src/qinfer/distributions.py#L1078-L1082", "method_name": "sample", "code": "def sample(self, n=1):\n         p_vals = self._p_dist.rvs(size=n)[:, np.newaxis]\n         return np.random.binomial(self.n, p_vals)", "query": "binomial distribution"}
{"id": "https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/widgets/image_cleaner.py#L209-L218", "method_name": "write_csv", "code": "def write_csv(self):\n         csv_path = self._path/\"cleaned.csv\"\n         with open(csv_path, \"w\") as f:\n             csv_writer = csv.writer(f)\n             csv_writer.writerow([\"name\",\"label\"])\n             for pair in self._csv_dict.items():\n                 pair = [os.path.relpath(pair[0], self._path), pair[1]]\n                 csv_writer.writerow(pair)\n         return csv_path", "query": "write csv"}
{"id": "https://github.com/jdp/urp/blob/778c16d9a5eae75316ce20aad742af7122be558c/urp.py#L17-L22", "method_name": "parse", "code": "def parse(args, data):\n     url = urlparse(data)\n     query = url.query\n     if not args.no_query_params:\n         query = parse_qsl(url.query)\n     return url, query", "query": "parse query string in url"}
{"id": "https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/__init__.py#L232-L258", "method_name": "levenshtein", "code": "def levenshtein(s1, s2, allow_substring=False):\n     len1, len2 = len(s1), len(s2)\n     lev = []\n     for i in range(len1 + 1):\n         lev.append([0] * (len2 + 1))\n     for i in range(len1 + 1):\n         lev[i][0] = i\n     for j in range(len2 + 1):\n         lev[0][j] = 0 if allow_substring else j\n     for i in range(len1):\n         for j in range(len2):\n             lev[i + 1][j + 1] = min(lev[i][j + 1] + 1, lev[i + 1][j] + 1, lev[i][j] + (s1[i] != s2[j]))\n     return min(lev[len1]) if allow_substring else lev[len1][len2]", "query": "string similarity levenshtein"}
{"id": "https://github.com/indygreg/python-zstandard/blob/74fa5904c3e7df67a4260344bf919356a181487e/bench.py#L32-L71", "method_name": "timer", "code": "def timer(fn, miniter=3, minwall=3.0):\n     results = []\n     count = 0\n     wall_begin = time.time()\n     while True:\n         wstart = time.time()\n         start = os.times()\n         fn()\n         end = os.times()\n         wend = time.time()\n         count += 1\n         user = end[0] - start[0]\n         system = end[1] - start[1]\n         cpu = user + system\n         wall = wend - wstart\n         results.append((cpu, user, system, wall))\n         if count < miniter:\n             continue\n         elapsed = wend - wall_begin\n         if elapsed < minwall:\n             continue\n         break\n     return results", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/tisimst/mcerp/blob/2bb8260c9ad2d58a806847f1b627b6451e407de1/mcerp/__init__.py#L1150-L1167", "method_name": "Binomial", "code": "def Binomial(n, p, tag=None):\n     assert (\n         int(n) == n and n > 0\n     ), \"Binomial number of trials \"n\" must be an integer greater than zero\"\n     assert (\n         0 < p < 1\n     ), \"Binomial probability \"p\" must be between zero and one, non-inclusive\"\n     return uv(ss.binom(n, p), tag=tag)", "query": "binomial distribution"}
{"id": "https://github.com/lcharleux/argiope/blob/8170e431362dc760589f7d141090fd133dece259/argiope/abq/pypostproc.py#L23-L49", "method_name": "read_field_report", "code": "def read_field_report(path, data_flag = \"*DATA\", meta_data_flag = \"*METADATA\"):\n   text = open(path).read()\n   mdpos = text.find(meta_data_flag)\n   dpos = text.find(data_flag)\n   mdata = io.StringIO( \"\n\".join(text[mdpos:dpos].split(\"\n\")[1:]))\n   data = io.StringIO( \"\n\".join(text[dpos:].split(\"\n\")[1:]))\n   data = pd.read_csv(data, index_col = 0)\n   data = data.groupby(data.index).mean()\n   mdata = pd.read_csv(mdata, sep = \"=\", header = None, index_col = 0)[1]\n   mdata = mdata.to_dict()\n   out = {}\n   out[\"step_num\"] = int(mdata[\"step_num\"])\n   out[\"step_label\"] = mdata[\"step_label\"]\n   out[\"frame\"] = int(mdata[\"frame\"])\n   out[\"frame_value\"] = float(mdata[\"frame_value\"])\n   out[\"part\"] = mdata[\"instance\"]\n   position_map = {\"NODAL\": \"node\", \n                   \"ELEMENT_CENTROID\": \"element\", \n                   \"WHOLE_ELEMENT\": \"element\"}\n   out[\"position\"] = position_map[mdata[\"position\"]]\n   out[\"label\"] = mdata[\"label\"]  \n   out[\"data\"] = data\n   field_class = getattr(argiope.mesh, mdata[\"argiope_class\"])\n   return field_class(**out)", "query": "extracting data from a text file"}
{"id": "https://github.com/caktus/django-timepiece/blob/52515dec027664890efbc535429e1ba1ee152f40/timepiece/entries/views.py#L44-L55", "method_name": "get_dates", "code": "def get_dates(self):\n         today = datetime.date.today()\n         day = today\n         if \"week_start\" in self.request.GET:\n             param = self.request.GET.get(\"week_start\")\n             try:\n                 day = datetime.datetime.strptime(param, \"%Y-%m-%d\").date()\n             except:\n                 pass\n         week_start = utils.get_week_start(day)\n         week_end = week_start + relativedelta(days=6)\n         return today, week_start, week_end", "query": "get current date"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/ipythonconsole/plugin.py#L577-L583", "method_name": "set_working_directory", "code": "def set_working_directory(self, dirname): \n         if dirname: \n             self.main.workingdirectory.chdir(dirname, refresh_explorer=True, \n                                              refresh_console=False)", "query": "set working directory"}
{"id": "https://github.com/tutorcruncher/pydf/blob/53dd030f02f112593ed6e2655160a40b892a23c0/docker-entrypoint.py#L36-L53", "method_name": "generate", "code": "async def generate(request):\n     start = time()\n     config = {}\n     for k, v in request.headers.items():\n         if k.startswith(\"Pdf-\") or k.startswith(\"Pdf_\"):\n             config[k[4:].lower()] = v.lower()\n     data = await request.read()\n     if not data:\n         logger.info(\"Request with no body data\")\n         raise web.HTTPBadRequest(text=\"400: no HTML data to convert to PDF in request body\n\")\n     try:\n         pdf_content = await app[\"apydf\"].generate_pdf(data.decode(), **config)\n     except RuntimeError as e:\n         logger.info(\"Error generating PDF, time %0.2fs, config: %s\", time() - start, config)\n         return web.Response(text=str(e) + \"\n\", status=418)\n     else:\n         logger.info(\"PDF generated in %0.2fs, html-len %d, pdf-len %d\", time() - start, len(data), len(pdf_content))\n         return web.Response(body=pdf_content, content_type=\"application/pdf\")", "query": "convert html to pdf"}
{"id": "https://github.com/Koed00/django-q/blob/c84fd11a67c9a47d821786dfcdc189bb258c6f54/django_q/models.py#L53-L55", "method_name": "group_count", "code": "def group_count(self, failures=False):\n        if self.group:\n            return self.get_group_count(self.group, failures)", "query": "group by count"}
{"id": "https://github.com/teitei-tk/Simple-AES-Cipher/blob/cb4bc69d762397f3a26f6b009e2b7fa5d706ed5f/simple_aes_cipher/cipher.py#L15-L19", "method_name": "encrypt", "code": "def encrypt(self, raw, mode=AES.MODE_CBC):\n         raw = self._pad(raw, AES.block_size)\n         iv = Random.new().read(AES.block_size)\n         cipher = AES.new(self.key, mode, iv)\n         return base64.b64encode(iv + cipher.encrypt(raw)).decode(\"utf-8\")", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/mbakker7/timml/blob/91e99ad573cb8a9ad8ac1fa041c3ca44520c2390/timml/linesink.py#L567-L595", "method_name": "initialize", "code": "def initialize(self):\n         for ls in self.lslist:\n             ls.initialize()\n         self.ncp = self.nls * self.lslist[0].ncp\n         self.nparam = self.nls * self.lslist[0].nparam\n         self.nunknowns = self.nparam\n         self.xls = np.empty((self.nls, 2))\n         self.yls = np.empty((self.nls, 2))\n         for i, ls in enumerate(self.lslist):\n             self.xls[i, :] = [ls.x1, ls.x2]\n             self.yls[i, :] = [ls.y1, ls.y2]\n         if self.aq is None:\n             self.aq = self.model.aq.find_aquifer_data(self.lslist[0].xc,\n                                                       self.lslist[0].yc)\n         self.parameters = np.zeros((self.nparam, 1))\n         self.xc = np.array([ls.xc for ls in self.lslist]).flatten()\n         self.yc = np.array([ls.yc for ls in self.lslist]).flatten()\n         self.xcin = np.array([ls.xcin for ls in self.lslist]).flatten()\n         self.ycin = np.array([ls.ycin for ls in self.lslist]).flatten()\n         self.xcout = np.array([ls.xcout for ls in self.lslist]).flatten()\n         self.ycout = np.array([ls.ycout for ls in self.lslist]).flatten()\n         self.cosnorm = np.array([ls.cosnorm for ls in self.lslist]).flatten()\n         self.sinnorm = np.array([ls.sinnorm for ls in self.lslist]).flatten()\n         self.aqin = self.model.aq.find_aquifer_data(self.xcin[0], self.ycin[0])\n         self.aqout = self.model.aq.find_aquifer_data(self.xcout[0],\n                                                      self.ycout[0])", "query": "initializing array"}
{"id": "https://github.com/hcpl/xkbgroup/blob/fcf4709a3c8221e0cdf62c09e5cccda232b0104c/xkbgroup/core.py#L242-L258", "method_name": "groups_count", "code": "def groups_count(self):\n         if self._keyboard_description.contents.ctrls is not None:\n             return self._keyboard_description.contents.ctrls.contents.num_groups\n         else:\n             groups_source = self._groups_source\n             groups_count = 0\n             while (groups_count < XkbNumKbdGroups and\n                    groups_source[groups_count] != None_):\n                 groups_count += 1\n             return groups_count", "query": "group by count"}
{"id": "https://github.com/ibelie/typy/blob/3616845fb91459aacd8df6bf82c5d91f4542bee7/typy/Proto.py#L266-L274", "method_name": "Enum", "code": "def Enum(name, *fields):\n \tobj = TypeObject()\n \tobj.isEnum = True\n \tobj.__name__ = name\n \tobj.__enum__ = {}\n \tfor a, p in fields:\n \t\tobj.__enum__[p] = TypeObject()\n \t\tobj.__enum__[p].name = a\n \treturn obj", "query": "get name of enumerated value"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/check_species.py#L14-L19", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Check species consistency between a VASP POSCAR file and a POTCAR file.\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\", nargs=\"?\", default=\"POSCAR\" )\n     parser.add_argument( \"potcar\", help=\"filename of the VASP POTCAR to be processed\", nargs=\"?\", default=\"POTCAR\" )\n     parser.add_argument( \"-p\", \"--ppset\", help=\"check whether the POTCAR pseudopotentials belong to a specific pseudopotential set\", choices=potcar_sets )\n     return parser.parse_args()", "query": "parse command line argument"}
{"id": "https://github.com/ga4gh/ga4gh-common/blob/ea1b562dce5bf088ac4577b838cfac7745f08346/ga4gh/common/utils.py#L30-L40", "method_name": "getPathOfExecutable", "code": "def getPathOfExecutable(executable):\n     exe_paths = os.environ[\"PATH\"].split(\":\")\n     for exe_path in exe_paths:\n         exe_file = os.path.join(exe_path, executable)\n         if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK):\n             return exe_file\n     return None", "query": "get executable path"}
{"id": "https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L20-L25", "method_name": "encrypt", "code": "def encrypt(self, data):\n         if self.iv is None:\n             cipher = pyAES.new(self.key, self.mode)\n         else:\n             cipher = pyAES.new(self.key, self.mode, self.iv)\n         return cipher.encrypt(pad_data(AES.str_to_bytes(data)))", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/thebjorn/pydeps/blob/1e6715b7bea47a40e8042821b57937deaaa0fdc3/pydeps/arguments.py#L20-L31", "method_name": "boolval", "code": "def boolval(v): \n     if isinstance(v, bool): \n         return v \n     if isinstance(v, int): \n         return bool(v) \n     if is_string(v): \n         v = v.lower() \n         if v in {\"j\", \"y\", \"ja\", \"yes\", \"1\", \"true\"}: \n             return True \n         if v in {\"n\", \"nei\", \"no\", \"0\", \"false\"}: \n             return False \n     raise ValueError(\"Don\"t know how to convert %r to bool\" % v)", "query": "convert int to bool"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/components/rom.py#L101-L134", "method_name": "extract_zip", "code": "def extract_zip(self):\n         assert self.FILE_COUNT>0\n         try:\n             with zipfile.ZipFile(self.archive_path, \"r\") as zip:\n                 namelist = zip.namelist()\n                 print(\"namelist():\", namelist)\n                 if len(namelist) != self.FILE_COUNT:\n                     msg = (\n                         \"Wrong archive content?!?\"\n                         \" There exists %i files, but it should exist %i.\"\n                         \"Existing names are: %r\"\n                     ) % (len(namelist), self.FILE_COUNT, namelist)\n                     log.error(msg)\n                     raise RuntimeError(msg)\n                 for filename in namelist:\n                     content = zip.read(filename)\n                     dst = self.file_rename(filename)\n                     out_filename=os.path.join(self.ROM_PATH, dst)\n                     with open(out_filename, \"wb\") as f:\n                         f.write(content)\n                     if dst == filename:\n                         print(\"%r extracted\" % out_filename)\n                     else:\n                         print(\"%r extracted to %r\" % (filename, out_filename))\n                     self.post_processing(out_filename)\n         except BadZipFile as err:\n             msg = \"Error extracting archive %r: %s\" % (self.archive_path, err)\n             log.error(msg)\n             raise BadZipFile(msg)", "query": "extract zip file recursively"}
{"id": "https://github.com/mar10/wsgidav/blob/cec0d84222fc24bea01be1cea91729001963f172/wsgidav/dav_error.py#L264-L274", "method_name": "get_http_status_string", "code": "def get_http_status_string(v):\n     code = get_http_status_code(v)\n     try:\n         return ERROR_DESCRIPTIONS[code]\n     except KeyError:\n         return \"{} Status\".format(code)", "query": "get the description of a http status code"}
{"id": "https://github.com/guilhermechapiewski/simple-db-migrate/blob/7ea6ffd0c58f70079cc344eae348430c7bdaaab3/simple_db_migrate/mysql.py#L30-L40", "method_name": "__mysql_connect", "code": "def __mysql_connect(self, connect_using_database_name=True):\n         try:\n             conn = self.__mysql_driver.connect(host=self.__mysql_host, port=self.__mysql_port, user=self.__mysql_user, passwd=self.__mysql_passwd)\n             conn.set_character_set(self.__mysql_encoding)\n             if connect_using_database_name:\n                 conn.select_db(self.__mysql_db)\n             return conn\n         except Exception as e:\n             raise Exception(\"could not connect to database: %s\" % e)", "query": "connect to sql"}
{"id": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/http_code.py#L157-L204", "method_name": "get", "code": "def get(self):\n         if PyFunceble.HTTP_CODE[\"active\"]:\n             http_code = self._access()\n             list_of_valid_http_code = []\n             for codes in [\n                 PyFunceble.HTTP_CODE[\"list\"][\"up\"],\n                 PyFunceble.HTTP_CODE[\"list\"][\"potentially_down\"],\n                 PyFunceble.HTTP_CODE[\"list\"][\"potentially_up\"],\n             ]:\n                 list_of_valid_http_code.extend(codes)\n             if http_code not in list_of_valid_http_code or http_code is None:\n                 return \"*\" * 3\n             return http_code\n         return None", "query": "get the description of a http status code"}
{"id": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/converter/o5json.py#L9-L63", "method_name": "xml_to_json", "code": "def xml_to_json(root):\n     j = {}\n     if len(root) == 0:  \n         return _maybe_intify(root.text)\n     if len(root) == 1 and root[0].tag.startswith(\"{\" + NS_GML):  \n         return gml_to_geojson(root[0])\n     if root.tag == \"open511\":\n         j[\"meta\"] = {\"version\": root.get(\"version\")}\n     for elem in root:\n         name = elem.tag\n         if name == \"link\" and elem.get(\"rel\"):\n             name = elem.get(\"rel\") + \"_url\"\n             if name == \"self_url\":\n                 name = \"url\"\n             if root.tag == \"open511\":\n                 j[\"meta\"][name] = elem.get(\"href\")\n                 continue\n         elif name.startswith(\"{\" + NS_PROTECTED):\n             name = \"!\" + name[name.index(\"}\") + 1:] \n         elif name[0] == \"{\":\n             name = \"+\" + name[name.index(\"}\") + 1:]\n         if name in j:\n             continue  \n         elif elem.tag == \"link\" and not elem.text:\n             j[name] = elem.get(\"href\")\n         elif len(elem):\n             if name == \"grouped_events\":\n                 j[name] = [xml_link_to_json(child, to_dict=False) for child in elem]\n             elif name in (\"attachments\", \"media_files\"):\n                 j[name] = [xml_link_to_json(child, to_dict=True) for child in elem]\n             elif all((name == pluralize(child.tag) for child in elem)):\n                 j[name] = [xml_to_json(child) for child in elem]\n             else:\n                 j[name] = xml_to_json(elem)\n         else:\n             if root.tag == \"open511\" and name.endswith(\"s\") and not elem.text:\n                 j[name] = []\n             else:\n                 j[name] = _maybe_intify(elem.text)\n     return j", "query": "json to xml conversion"}
{"id": "https://github.com/santoshphilip/eppy/blob/55410ff7c11722f35bc4331ff5e00a0b86f787e1/eppy/EPlusInterfaceFunctions/parse_idd.py#L142-L385", "method_name": "extractidddata", "code": "def extractidddata(fname, debug=False):\n     try:\n         if isinstance(fname, (file, StringIO)):\n             astr = fname.read()\n             try:\n                 astr = astr.decode(\"ISO-8859-2\")\n             except AttributeError:\n                 pass \n         else:\n             astr = mylib2.readfile(fname)\n     except NameError:\n         if isinstance(fname, (FileIO, StringIO)):\n             astr = fname.read()\n             try:\n                 astr = astr.decode(\"ISO-8859-2\")\n             except AttributeError:\n                 pass\n         else:\n             astr = mylib2.readfile(fname)\n     (nocom, nocom1, blocklst) = get_nocom_vars(astr)\n     astr = nocom\n     st1 = removeblanklines(astr)\n     if debug:\n         mylib1.write_str2file(\"nocom2.txt\", st1.encode(\"latin-1\"))\n     groupls = []\n     alist = st1.splitlines()\n     for element in alist:\n         lss = element.split()\n         if lss[0].upper() == \"\\\\group\".upper():\n             groupls.append(element)\n     groupstart = []\n     for i in range(len(groupls)):\n         iindex = alist.index(groupls[i])\n         groupstart.append([alist[iindex], alist[iindex+1]])\n     for element in groupls:\n         alist.remove(element)\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom3.txt\", st1.encode(\"latin-1\"))\n     for i in range(len(alist)):\n         alist[i] = alist[i].strip()\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom4.txt\", st1.encode(\"latin-1\"))\n     lss = []\n     for i in range(len(alist)):\n         if alist[i][0] != \"\\\\\":\n             pnt = alist[i].find(\"\\\\\")\n             if pnt != -1:\n                 lss.append(alist[i][:pnt].strip())\n                 lss.append(alist[i][pnt:].strip())\n             else:\n                 lss.append(alist[i])\n         else:\n             lss.append(alist[i])\n     alist = lss[:]\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom5.txt\", st1.encode(\"latin-1\"))\n     lss = []\n     for element in alist:\n         if element[0] != \"\\\\\":\n             llist = element.split(\",\")\n             if llist[-1] == \"\":\n                 tmp = llist.pop()\n             for elm in llist:\n                 if elm[-1] == \";\":\n                     lss.append(elm.strip())\n                 else:\n                     lss.append((elm+\",\").strip())\n         else:\n             lss.append(element)\n     ls_debug = alist[:] \n     alist = lss[:]\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom6.txt\", st1.encode(\"latin-1\"))\n     if debug:\n         lss_debug = []\n         for element in ls_debug:\n             if element[0] != \"\\\\\":\n                 llist = element.split(\",\")\n                 if llist[-1] == \"\":\n                     tmp = llist.pop()\n                 for elm in llist:\n                     if elm[-1] == \";\":\n                         lss_debug.append(elm[:-1].strip())\n                     else:\n                         lss_debug.append((elm).strip())\n             else:\n                 lss_debug.append(element)\n         ls_debug = lss_debug[:]\n         st1 = \"\n\".join(ls_debug)\n         mylib1.write_str2file(\"nocom7.txt\", st1.encode(\"latin-1\"))\n     for i in range(len(lss)):\n         if lss[i][0] != \"\\\\\":\n             lss[i] = \"=====var=====\"\n     st2 = \"\n\".join(lss)\n     lss = st2.split(\"=====var=====\n\")\n     lss.pop(0) \n     if debug:\n         fname = \"nocom8.txt\"\n         fhandle = open(fname, \"wb\")\n         k = 0\n         for i in range(len(blocklst)):\n             for j in range(len(blocklst[i])):\n                 atxt = blocklst[i][j]+\"\n\"\n                 fhandle.write(atxt)\n                 atxt = lss[k]\n                 fhandle.write(atxt.encode(\"latin-1\"))\n                 k = k+1\n         fhandle.close()\n     k = 0\n     lst = []\n     for i in range(len(blocklst)):\n         lst.append([])\n         for j in range(len(blocklst[i])):\n             lst[i].append(lss[k])\n             k = k+1\n     if debug:\n         fname = \"nocom9.txt\"\n         fhandle = open(fname, \"wb\")\n         k = 0\n         for i in range(len(blocklst)):\n             for j in range(len(blocklst[i])):\n                 atxt = blocklst[i][j]+\"\n\"\n                 fhandle.write(atxt)\n                 fhandle.write(lst[i][j].encode(\"latin-1\"))\n                 k = k+1\n         fhandle.close()\n     for i in range(len(lst)):\n         for j in range(len(lst[i])):\n             lst[i][j] = lst[i][j].splitlines()\n             for k in range(len(lst[i][j])):\n                 lst[i][j][k] = lst[i][j][k][1:]\n     commlst = lst\n     clist = lst\n     lss = []\n     for i in range(0, len(clist)):\n         alist = []\n         for j in range(0, len(clist[i])):\n             itt = clist[i][j]\n             ddtt = {}\n             for element in itt:\n                 if len(element.split()) == 0:\n                     break\n                 ddtt[element.split()[0].lower()] = []\n             for element in itt:\n                 if len(element.split()) == 0:\n                     break\n                 ddtt[element.split()[0].lower()].append(\" \".join(element.split()[1:]))\n             alist.append(ddtt)\n         lss.append(alist)\n     commdct = lss\n     return blocklst, commlst, commdct", "query": "extracting data from a text file"}
{"id": "https://github.com/uw-it-aca/uw-restclients-iasystem/blob/f65f169d54b0d39e2d732cba529ccd8b6cb49f8a/uw_iasystem/evaluation.py#L192-L199", "method_name": "_datetime_from_string", "code": "def _datetime_from_string(date_string):\n     if date_string:\n         date_format = \"%Y-%m-%dT%H:%M:%S\"\n         date_string = date_string.replace(\"Z\", \"\")\n         date = datetime.strptime(date_string, date_format)\n         return pytz.utc.localize(date)\n     return \"\"", "query": "string to date"}
{"id": "https://github.com/Turbo87/utm/blob/efdd46ab0a341ce2aa45f8144d8b05a4fa0fd592/utm/conversion.py#L261-L283", "method_name": "latlon_to_zone_number", "code": "def latlon_to_zone_number(latitude, longitude):\n     if use_numpy:\n         if isinstance(latitude, mathlib.ndarray):\n             latitude = latitude.flat[0]\n         if isinstance(longitude, mathlib.ndarray):\n             longitude = longitude.flat[0]\n     if 56 <= latitude < 64 and 3 <= longitude < 12:\n         return 32\n     if 72 <= latitude <= 84 and longitude >= 0:\n         if longitude < 9:\n             return 31\n         elif longitude < 21:\n             return 33\n         elif longitude < 33:\n             return 35\n         elif longitude < 42:\n             return 37\n     return int((longitude + 180) / 6) + 1", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/geopy/geopy/blob/02c838d965e76497f3c3d61f53808c86b5c58224/geopy/geocoders/ignfrance.py#L444-L527", "method_name": "_xml_to_json_places", "code": "def _xml_to_json_places(tree, is_reverse=False):\n         select_multi = (\n             \"GeocodedAddress\"\n             if not is_reverse\n             else \"ReverseGeocodedLocation\"\n         )\n         adresses = tree.findall(\".//\" + select_multi)\n         places = []\n         sel_pl = \".//Address/Place[@type=\"{}\"]\"\n         for adr in adresses:\n             el = {}\n             el[\"pos\"] = adr.find(\"./Point/pos\")\n             el[\"street\"] = adr.find(\".//Address/StreetAddress/Street\")\n             el[\"freeformaddress\"] = adr.find(\".//Address/freeFormAddress\")\n             el[\"municipality\"] = adr.find(sel_pl.format(\"Municipality\"))\n             el[\"numero\"] = adr.find(sel_pl.format(\"Numero\"))\n             el[\"feuille\"] = adr.find(sel_pl.format(\"Feuille\"))\n             el[\"section\"] = adr.find(sel_pl.format(\"Section\"))\n             el[\"departement\"] = adr.find(sel_pl.format(\"Departement\"))\n             el[\"commune_absorbee\"] = adr.find(sel_pl.format(\"CommuneAbsorbee\"))\n             el[\"commune\"] = adr.find(sel_pl.format(\"Commune\"))\n             el[\"insee\"] = adr.find(sel_pl.format(\"INSEE\"))\n             el[\"qualite\"] = adr.find(sel_pl.format(\"Qualite\"))\n             el[\"territoire\"] = adr.find(sel_pl.format(\"Territoire\"))\n             el[\"id\"] = adr.find(sel_pl.format(\"ID\"))\n             el[\"id_tr\"] = adr.find(sel_pl.format(\"ID_TR\"))\n             el[\"bbox\"] = adr.find(sel_pl.format(\"Bbox\"))\n             el[\"nature\"] = adr.find(sel_pl.format(\"Nature\"))\n             el[\"postal_code\"] = adr.find(\".//Address/PostalCode\")\n             el[\"extended_geocode_match_code\"] = adr.find(\n                 \".//ExtendedGeocodeMatchCode\"\n             )\n             place = {}\n             def testContentAttrib(selector, key):\n                 return selector.attrib.get(\n                     key,\n                     None\n                 ) if selector is not None else None\n             place[\"accuracy\"] = testContentAttrib(\n                 adr.find(\".//GeocodeMatchCode\"), \"accuracy\")\n             place[\"match_type\"] = testContentAttrib(\n                 adr.find(\".//GeocodeMatchCode\"), \"matchType\")\n             place[\"building\"] = testContentAttrib(\n                 adr.find(\".//Address/StreetAddress/Building\"), \"number\")\n             place[\"search_centre_distance\"] = testContentAttrib(\n                 adr.find(\".//SearchCentreDistance\"), \"value\")\n             for key, value in iteritems(el):\n                 if value is not None:\n                     place[key] = value.text\n                     if value.text is None:\n                         place[key] = None\n                 else:\n                     place[key] = None\n             if place[\"pos\"]:\n                 lat, lng = place[\"pos\"].split(\" \")\n                 place[\"lat\"] = lat.strip()\n                 place[\"lng\"] = lng.strip()\n             else:\n                 place[\"lat\"] = place[\"lng\"] = None\n             place.pop(\"pos\", None)\n             places.append(place)\n         return places", "query": "json to xml conversion"}
{"id": "https://github.com/ask/ghettoq/blob/22a0fcd865b618cbbbfd102efd88a7983507c24e/ghettoq/backends/beanstalk.py#L32-L34", "method_name": "put", "code": "def put(self, queue, message, priority=0, **kwargs):\n        self.client.use(queue)\n        self.client.put(message, priority=priority)", "query": "priority queue"}
{"id": "https://github.com/kyper-data/python-highcharts/blob/a4c488ae5c2e125616efad5a722f3dfd8a9bc450/highcharts/highmaps/highmaps.py#L292-L315", "method_name": "set_map_source", "code": "def set_map_source(self, map_src, jsonp_map = False):\n         if not map_src:\n             raise OptionTypeError(\"No map source input, please refer to: https://code.highcharts.com/mapdata/\")\n         if  jsonp_map:\n             self.jsonp_map_flag = True\n             self.map = \"geojson\"\n             self.jsonp_map_url = json.dumps(map_src)\n         else:\n             self.add_JSsource(map_src)\n             map_name = self._get_jsmap_name(map_src)\n             self.map = \"geojson\"\n             self.jsmap = self.map + \" = Highcharts.geojson(\" + map_name + \");\"\n             self.add_JSscript(\"var \" + self.jsmap, \"head\")\n         if self.data_temp:\n             self.data_temp[0].__options__().update({\"mapData\": MapObject(self.map)})", "query": "map to json"}
{"id": "https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/api.py#L1695-L1829", "method_name": "cluster_kmeans", "code": "def cluster_kmeans(data=None, k=None, max_iter=10, tolerance=1e-5, stride=1,\n                    metric=\"euclidean\", init_strategy=\"kmeans++\", fixed_seed=False,\n                    n_jobs=None, chunksize=None, skip=0, keep_data=False, clustercenters=None, **kwargs):\n     r\n     from pyemma.coordinates.clustering.kmeans import KmeansClustering\n     res = KmeansClustering(n_clusters=k, max_iter=max_iter, metric=metric, tolerance=tolerance,\n                            init_strategy=init_strategy, fixed_seed=fixed_seed, n_jobs=n_jobs, skip=skip,\n                            keep_data=keep_data, clustercenters=clustercenters, stride=stride)\n     from pyemma.util.reflection import get_default_args\n     cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_kmeans)[\"chunksize\"], **kwargs)\n     if data is not None:\n         res.estimate(data, chunksize=cs)\n     else:\n         res.chunksize = cs\n     return res", "query": "k means clustering"}
{"id": "https://github.com/honzajavorek/tipi/blob/cbe51192725608b6fba1244a48610ae231b13e08/tipi/repl.py#L65-L74", "method_name": "replace", "code": "def replace(html, replacements=None):\n     if not replacements:\n         return html  \n     html = HTMLFragment(html)\n     for r in replacements:\n         r.replace(html)\n     return unicode(html)", "query": "html entities replace"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L72-L76", "method_name": "aes_cbc_encrypt", "code": "def aes_cbc_encrypt(plain_text: bytes, key: bytes, iv: bytes = b\"\"):\n         if len(iv) == 0:\n             iv = AESHandler.generate_iv()\n         cipher = AES.new(key=key, mode=AES.MODE_CBC, iv=iv)\n         return cipher.IV, cipher.encrypt(pad(plain_text, AES.block_size))", "query": "aes encryption"}
{"id": "https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/plotting/matplot_dep/plot_definitions.py#L99-L102", "method_name": "scatter", "code": "def scatter(self, ax, X, Y, Z=None, color=Tango.colorsHex[\"mediumBlue\"], label=None, marker=\"o\", **kwargs):\n         if Z is not None:\n             return ax.scatter(X, Y, c=color, zs=Z, label=label, marker=marker, **kwargs)\n         return ax.scatter(X, Y, c=color, label=label, marker=marker, **kwargs)", "query": "scatter plot"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/ipythonconsole/plugin.py#L577-L583", "method_name": "set_working_directory", "code": "def set_working_directory(self, dirname): \n         if dirname: \n             self.main.workingdirectory.chdir(dirname, refresh_explorer=True, \n                                              refresh_console=False)", "query": "set working directory"}
{"id": "https://github.com/alexhayes/django-pdfkit/blob/02774ae2cb67d05dd5e4cb50661c56464ebb2413/django_pdfkit/views.py#L55-L74", "method_name": "render_pdf", "code": "def render_pdf(self, *args, **kwargs):\n         html = self.render_html(*args, **kwargs)\n         options = self.get_pdfkit_options()\n         if \"debug\" in self.request.GET and settings.DEBUG:\n             options[\"debug-javascript\"] = 1\n         kwargs = {}\n         wkhtmltopdf_bin = os.environ.get(\"WKHTMLTOPDF_BIN\")\n         if wkhtmltopdf_bin:\n             kwargs[\"configuration\"] = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_bin)\n         pdf = pdfkit.from_string(html, False, options, **kwargs)\n         return pdf", "query": "convert html to pdf"}
{"id": "https://github.com/nestauk/gtr/blob/49e1b8db1f4376612ea1ec8585a79375fa15a899/gtr/services/base.py#L25-L33", "method_name": "handle_http_error", "code": "def handle_http_error(self, response, custom_messages=None,\n                           raise_for_status=False):\n         if not custom_messages:\n             custom_messages = {}\n         if response.status_code in custom_messages.keys():\n             raise requests.exceptions.HTTPError(\n                 custom_messages[response.status_code])\n         if raise_for_status:\n             response.raise_for_status()", "query": "custom http error response"}
{"id": "https://github.com/kibitzr/kibitzr/blob/749da312488f1dda1ed1093cf4c95aaac0a604f7/kibitzr/transformer/jinja_transform.py#L81-L88", "method_name": "text_filter", "code": "def text_filter(html):\n     if isinstance(html, list):\n         html = \"\".join(html)\n     ok, content = SoupOps.extract_text(html)\n     if ok:\n         return content\n     else:\n         raise RuntimeError(\"Extract text failed\")", "query": "extract data from html content"}
{"id": "https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L471-L484", "method_name": "file_unzipper", "code": "def file_unzipper(directory):\n    debug.log(\"Unzipping directory (%s)...\"%directory)\n    for root, dirs, files in os.walk(directory, topdown=False):\n       if root != \"\":\n          orig_dir = os.getcwd()\n          os.chdir(directory)\n          Popen(\"gunzip -q -f *.gz > /dev/null 2>&1\", shell=True).wait()\n          Popen(\"unzip -qq -o \"*.zip\" > /dev/null 2>&1\", shell=True).wait()\n          Popen(\"rm -f *.zip > /dev/null 2>&1\", shell=True).wait()\n          os.chdir(orig_dir)", "query": "unzipping large files"}
{"id": "https://github.com/theislab/scanpy/blob/9e4e5ee02e04cf618872d9b098e24f0542e8b227/scanpy/plotting/_anndata.py#L30-L179", "method_name": "scatter", "code": "def scatter(\n         adata,\n         x=None,\n         y=None,\n         color=None,\n         use_raw=None,\n         layers=\"X\",\n         sort_order=True,\n         alpha=None,\n         basis=None,\n         groups=None,\n         components=None,\n         projection=\"2d\",\n         legend_loc=\"right margin\",\n         legend_fontsize=None,\n         legend_fontweight=None,\n         color_map=None,\n         palette=None,\n         frameon=None,\n         right_margin=None,\n         left_margin=None,\n         size=None,\n         title=None,\n         show=None,\n         save=None,\n         ax=None):\n     if basis is not None:\n         axs = _scatter_obs(\n             adata=adata,\n             x=x,\n             y=y,\n             color=color,\n             use_raw=use_raw,\n             layers=layers,\n             sort_order=sort_order,\n             alpha=alpha,\n             basis=basis,\n             groups=groups,\n             components=components,\n             projection=projection,\n             legend_loc=legend_loc,\n             legend_fontsize=legend_fontsize,\n             legend_fontweight=legend_fontweight,\n             color_map=color_map,\n             palette=palette,\n             frameon=frameon,\n             right_margin=right_margin,\n             left_margin=left_margin,\n             size=size,\n             title=title,\n             show=show,\n             save=save,\n             ax=ax)\n     elif x is not None and y is not None:\n         if ((x in adata.obs.keys() or x in adata.var.index)\n             and (y in adata.obs.keys() or y in adata.var.index)\n             and (color is None or color in adata.obs.keys() or color in adata.var.index)):\n             axs = _scatter_obs(\n                 adata=adata,\n                 x=x,\n                 y=y,\n                 color=color,\n                 use_raw=use_raw,\n                 layers=layers,\n                 sort_order=sort_order,\n                 alpha=alpha,\n                 basis=basis,\n                 groups=groups,\n                 components=components,\n                 projection=projection,\n                 legend_loc=legend_loc,\n                 legend_fontsize=legend_fontsize,\n                 legend_fontweight=legend_fontweight,\n                 color_map=color_map,\n                 palette=palette,\n                 frameon=frameon,\n                 right_margin=right_margin,\n                 left_margin=left_margin,\n                 size=size,\n                 title=title,\n                 show=show,\n                 save=save,\n                 ax=ax)\n         elif ((x in adata.var.keys() or x in adata.obs.index)\n                 and (y in adata.var.keys() or y in adata.obs.index)\n                 and (color is None or color in adata.var.keys() or color in adata.obs.index)):\n             axs = _scatter_var(\n                 adata=adata,\n                 x=x,\n                 y=y,\n                 color=color,\n                 use_raw=use_raw,\n                 layers=layers,\n                 sort_order=sort_order,\n                 alpha=alpha,\n                 basis=basis,\n                 groups=groups,\n                 components=components,\n                 projection=projection,\n                 legend_loc=legend_loc,\n                 legend_fontsize=legend_fontsize,\n                 legend_fontweight=legend_fontweight,\n                 color_map=color_map,\n                 palette=palette,\n                 frameon=frameon,\n                 right_margin=right_margin,\n                 left_margin=left_margin,\n                 size=size,\n                 title=title,\n                 show=show,\n                 save=save,\n                 ax=ax)\n         else:\n             raise ValueError(\n                 \"`x`, `y`, and potential `color` inputs must all come from either `.obs` or `.var`\")\n     else:\n         raise ValueError(\"Either provide a `basis` or `x` and `y`.\")\n     return axs", "query": "scatter plot"}
{"id": "https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/api.py#L1645-L1692", "method_name": "cluster_mini_batch_kmeans", "code": "def cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric=\"euclidean\",\n                               init_strategy=\"kmeans++\", n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs):\n     r\n     from pyemma.coordinates.clustering.kmeans import MiniBatchKmeansClustering\n     res = MiniBatchKmeansClustering(n_clusters=k, max_iter=max_iter, metric=metric, init_strategy=init_strategy,\n                                     batch_size=batch_size, n_jobs=n_jobs, skip=skip, clustercenters=clustercenters)\n     from pyemma.util.reflection import get_default_args\n     cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_mini_batch_kmeans)[\"chunksize\"], **kwargs)\n     if data is not None:\n         res.estimate(data, chunksize=cs)\n     else:\n         res.chunksize = chunksize\n     return res", "query": "k means clustering"}
{"id": "https://github.com/DataBiosphere/dsub/blob/443ce31daa6023dc2fd65ef2051796e19d18d5a7/dsub/providers/google.py#L466-L473", "method_name": "_datetime_to_utc_int", "code": "def _datetime_to_utc_int(date):\n     if date is None:\n       return None\n     epoch = dsub_util.replace_timezone(datetime.utcfromtimestamp(0), pytz.utc)\n     return (date - epoch).total_seconds()", "query": "convert a utc time to epoch"}
{"id": "https://github.com/widdowquinn/pyani/blob/2b24ec971401e04024bba896e4011984fe3f53f0/pyani/pyani_graphics.py#L219-L228", "method_name": "get_mpl_heatmap_axes", "code": "def get_mpl_heatmap_axes(dfr, fig, heatmap_gs):\n     heatmap_axes = fig.add_subplot(heatmap_gs[1, 1])\n     heatmap_axes.set_xticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0]))\n     heatmap_axes.set_yticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0]))\n     heatmap_axes.grid(False)\n     heatmap_axes.xaxis.tick_bottom()\n     heatmap_axes.yaxis.tick_right()\n     return heatmap_axes", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/pedroburon/tbk/blob/ecd6741e0bae06269eb4ac885c3ffcb7902ee40e/tbk/webpay/encryption.py#L36-L42", "method_name": "encrypt_message", "code": "def encrypt_message(self, signed_message, message, key, iv):\n         raw = signed_message + message\n         block_size = AES.block_size\n         pad = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size).encode(\"utf-8\")\n         message_to_encrypt = pad(raw)\n         cipher = AES.new(key, AES.MODE_CBC, iv)\n         return cipher.encrypt(message_to_encrypt)", "query": "aes encryption"}
{"id": "https://github.com/shazow/unstdlib.py/blob/e0632fe165cfbfdb5a7e4bc7b412c9d6f2ebad83/unstdlib/standard/list_.py#L16-L36", "method_name": "groupby_count", "code": "def groupby_count(i, key=None, force_keys=None):\n     counter = defaultdict(lambda: 0)\n     if not key:\n         key = lambda o: o\n     for k in i:\n         counter[key(k)] += 1\n     if force_keys:\n         for k in force_keys:\n             counter[k] += 0\n     return counter.items()", "query": "group by count"}
{"id": "https://github.com/tintinweb/pyetherchain/blob/cfeee3944b84fd12842ec3031d1d08ec7d63d33c/pyetherchain/pyetherchain.py#L185-L193", "method_name": "get_hardforks", "code": "def get_hardforks(self):\n         rows = self._parse_tbodies(self.session.get(\"/hardForks\").text)[0]  \n         result = []\n         for col in rows:\n             result.append({\"name\": self._extract_text_from_html( col[0]),\n                       \"on_roadmap\": True if \"yes\" in col[1].lower() else False,\n                       \"date\": self._extract_text_from_html(col[2]),\n                       \"block\": int(self._extract_text_from_html(col[3]))})\n         return result", "query": "extract data from html content"}
{"id": "https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/util/url.py#L72-L103", "method_name": "_urlparse_qs", "code": "def _urlparse_qs(url):\n     querystring = urlparse(url)[4]\n     pairs = [s2 for s1 in querystring.split(\"&\") for s2 in s1.split(\";\")]\n     result = OrderedDefaultDict(list)\n     for name_value in pairs:\n         pair = name_value.split(\"=\", 1)\n         if len(pair) != 2:\n             continue\n         if len(pair[1]) > 0:\n             name = _unquote(pair[0].replace(\"+\", \" \"))\n             value = _unquote(pair[1].replace(\"+\", \" \"))\n             result[name].append(value)\n     return result", "query": "parse query string in url"}
{"id": "https://github.com/rigetti/pyquil/blob/ec98e453084b0037d69d8c3245f6822a5422593d/pyquil/api/_base_connection.py#L52-L59", "method_name": "post_json", "code": "def post_json(session, url, json):\n     res = session.post(url, json=json)\n     if res.status_code >= 400:\n         raise parse_error(res)\n     return res", "query": "httpclient post json"}
{"id": "https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/regression.py#L10-L223", "method_name": "linear_regression", "code": "def linear_regression(X, y, add_intercept=True, coef_only=False, alpha=0.05,\n                       as_dataframe=True, remove_na=False):\n     if isinstance(X, pd.DataFrame):\n         names = X.keys().tolist()\n     elif isinstance(X, pd.Series):\n         names = [X.name]\n     else:\n         names = []\n     assert 0 < alpha < 1\n     assert y.ndim == 1, \"y must be one-dimensional.\"\n     X = np.asarray(X)\n     y = np.asarray(y)\n     if X.ndim == 1:\n         X = X[..., np.newaxis]\n     if remove_na:\n         X, y = rm_na(X, y[..., np.newaxis], paired=True, axis=\"rows\")\n         y = np.squeeze(y)\n     y_gd = np.isfinite(y).all()\n     X_gd = np.isfinite(X).all()\n     assert y_gd, \"Target (y) contains NaN or Inf. Please remove them.\"\n     assert X_gd, \"Predictors (X) contain NaN or Inf. Please remove them.\"\n     assert y.shape[0] == X.shape[0], \"X and y must have same number of samples\"\n     if not names:\n         names = [\"x\" + str(i + 1) for i in range(X.shape[1])]\n     if add_intercept:\n         X = np.column_stack((np.ones(X.shape[0]), X))\n         names.insert(0, \"Intercept\")\n     coef = np.linalg.lstsq(X, y, rcond=None)[0]\n     if coef_only:\n         return coef\n     pred = np.dot(X, coef)\n     resid = np.square(y - pred)\n     ss_res = resid.sum()\n     n, p = X.shape[0], X.shape[1]\n     dof = n - p if add_intercept else n - p - 1\n     MSE = ss_res / dof\n     beta_var = MSE * (np.linalg.pinv(np.dot(X.T, X)).diagonal())\n     beta_se = np.sqrt(beta_var)\n     ss_tot = np.square(y - y.mean()).sum()\n     r2 = 1 - (ss_res / ss_tot)\n     adj_r2 = 1 - (1 - r2) * (n - 1) / dof\n     T = coef / beta_se\n     pval = np.array([2 * t.sf(np.abs(i), dof) for i in T])\n     crit = t.ppf(1 - alpha / 2, dof)\n     marg_error = crit * beta_se\n     ll = coef - marg_error\n     ul = coef + marg_error\n     ll_name = \"CI[%.1f%%]\" % (100 * alpha / 2)\n     ul_name = \"CI[%.1f%%]\" % (100 * (1 - alpha / 2))\n     stats = {\"names\": names, \"coef\": coef, \"se\": beta_se, \"T\": T,\n              \"pval\": pval, \"r2\": r2, \"adj_r2\": adj_r2, ll_name: ll,\n              ul_name: ul}\n     if as_dataframe:\n         return pd.DataFrame.from_dict(stats)\n     else:\n         return stats", "query": "linear regression"}
{"id": "https://github.com/AndrewAnnex/SpiceyPy/blob/fc20a9b9de68b58eed5b332f0c051fb343a6e335/spiceypy/spiceypy.py#L8663-L8689", "method_name": "mxmg", "code": "def mxmg(m1, m2, nrow1, ncol1, ncol2):\n     m1 = stypes.toDoubleMatrix(m1)\n     m2 = stypes.toDoubleMatrix(m2)\n     mout = stypes.emptyDoubleMatrix(x=ncol2, y=nrow1)\n     nrow1 = ctypes.c_int(nrow1)\n     ncol1 = ctypes.c_int(ncol1)\n     ncol2 = ctypes.c_int(ncol2)\n     libspice.mxmg_c(m1, m2, nrow1, ncol1, ncol2, mout)\n     return stypes.cMatrixToNumpy(mout)", "query": "matrix multiply"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_rdf.py#L9-L17", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser()\n     parser.add_argument( \"xdatcar\" )\n     parser.add_argument( \"label\", nargs = 2 )\n     parser.add_argument( \"max_r\", type = float )\n     parser.add_argument( \"n_bins\", type = int )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/zblz/naima/blob/d6a6781d73bf58fd8269e8b0e3b70be22723cd5b/naima/extern/minimize.py#L66-L67", "method_name": "minimize", "code": "def minimize(func, x0, args=(), options={}, method=None):\n    return _minimize_neldermead(func, x0, args=args, **options)", "query": "nelder mead optimize"}
{"id": "https://github.com/Jaymon/prom/blob/b7ad2c259eca198da03e1e4bc7d95014c168c361/prom/query.py#L1458-L1464", "method_name": "process_id", "code": "def process_id(self):\n         ret = \"\"\n         if thread:\n             f = getattr(os, \"getpid\", None)\n             if f:\n                 ret = str(f())\n         return ret", "query": "get current process id"}
{"id": "https://github.com/docker/docker-py/blob/613d6aad83acc9931ff2ecfd6a6c7bd8061dc125/docker/api/client.py#L275-L289", "method_name": "_post_json", "code": "def _post_json(self, url, data, **kwargs):\n         data2 = {}\n         if data is not None and isinstance(data, dict):\n             for k, v in six.iteritems(data):\n                 if v is not None:\n                     data2[k] = v\n         elif data is not None:\n             data2 = data\n         if \"headers\" not in kwargs:\n             kwargs[\"headers\"] = {}\n         kwargs[\"headers\"][\"Content-Type\"] = \"application/json\"\n         return self._post(url, data=json.dumps(data2), **kwargs)", "query": "httpclient post json"}
{"id": "https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/flows.py#L1718-L1752", "method_name": "register_work", "code": "def register_work(self, work, deps=None, manager=None, workdir=None):\n         if getattr(self, \"workdir\", None) is not None:\n             work_workdir = None\n             if workdir is None:\n                 work_workdir = os.path.join(self.workdir, \"w\" + str(len(self)))\n             else:\n                 work_workdir = os.path.join(self.workdir, os.path.basename(workdir))\n             work.set_workdir(work_workdir)\n         if manager is not None:\n             work.set_manager(manager)\n         self.works.append(work)\n         if deps:\n             deps = [Dependency(node, exts) for node, exts in deps.items()]\n             work.add_deps(deps)\n         return work", "query": "set working directory"}
{"id": "https://github.com/radujica/baloo/blob/f6e05e35b73a75e8a300754c6bdc575e5f2d53b9/baloo/weld/weld_ops.py#L464-L491", "method_name": "_weld_sort", "code": "def _weld_sort(arrays, weld_types, ascending=True):\n     obj_id, index_obj = create_weld_object(arrays[0])\n     index_obj.weld_code = \"len({})\".format(obj_id)\n     index_column = weld_range(0, index_obj, 1)\n     arrays.insert(0, index_column)\n     weld_types.insert(0, WeldLong())\n     weld_obj_vec_of_struct = weld_arrays_to_vec_of_struct(arrays, weld_types)\n     weld_obj = create_empty_weld_object()\n     weld_obj_vec_of_struct_id = get_weld_obj_id(weld_obj, weld_obj_vec_of_struct)\n     types = struct_of(\"{e}\", weld_types)\n     ascending_sort_func = \"{}\".format(\", \".join((\"e.${}\".format(i) for i in range(1, len(arrays)))))\n     zero_literals = dict(enumerate([to_weld_literal(0, weld_type) for weld_type in weld_types]))\n     descending_sort_func = \"{}\".format(\", \".join((\"{} - e.${}\".format(zero_literals[i], i)\n                                                   for i in range(1, len(arrays)))))\n     sort_func = ascending_sort_func if ascending else descending_sort_func\n     weld_template = \"sort({struct},  e: {types}  {sort_func})\"\n     weld_obj.weld_code = weld_template.format(struct=weld_obj_vec_of_struct_id,\n                                               types=types,\n                                               sort_func=sort_func)\n     return weld_obj", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/JamesGardiner/chwrapper/blob/50f9cb2f5264c59505e8cc4e45ee6dc5d5669134/chwrapper/services/base.py#L97-L109", "method_name": "handle_http_error", "code": "def handle_http_error(\n         self, response, ignore=None, custom_messages=None, raise_for_status=True\n     ):\n         status = response.status_code\n         ignore = ignore or []\n         custom_messages = custom_messages or {}\n         if status in ignore or status in self._ignore_codes:\n             return None\n         elif response.status_code in custom_messages.keys():\n             raise requests.exceptions.HTTPError(custom_messages[response.status_code])\n         elif raise_for_status:\n             response.raise_for_status()", "query": "custom http error response"}
{"id": "https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/playhouse/mysql_ext.py#L14-L17", "method_name": "_connect", "code": "def _connect(self):\n         if mysql_connector is None:\n             raise ImproperlyConfigured(\"MySQL connector not installed!\")\n         return mysql_connector.connect(db=self.database, **self.connect_params)", "query": "connect to sql"}
{"id": "https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/filetypes.py#L264-L267", "method_name": "save", "code": "def save(self, filename, metadata={}, **data):\n         super(NumpyFile, self).save(filename, metadata, **data)\n         savefn = numpy.savez_compressed if self.compress else numpy.savez\n         savefn(self._savepath(filename), metadata=metadata, **data)", "query": "save list to file"}
{"id": "https://github.com/hackedd/gw2api/blob/5543a78e6e3ed0573b7e84c142c44004b4779eac/gw2api/map.py#L50-L105", "method_name": "maps", "code": "def maps(map_id=None, lang=\"en\"):\n     if map_id:\n         cache_name = \"maps.%s.%s.json\" % (map_id, lang)\n         params = {\"map_id\": map_id, \"lang\": lang}\n     else:\n         cache_name = \"maps.%s.json\" % lang\n         params = {\"lang\": lang}\n     data = get_cached(\"maps.json\", cache_name, params=params).get(\"maps\")\n     return data.get(str(map_id)) if map_id else data", "query": "map to json"}
{"id": "https://github.com/danielhrisca/asammdf/blob/3c7a1fd19c957ceebe4dcdbb2abf00806c2bdb66/asammdf/mdf.py#L1821-L2190", "method_name": "concatenate", "code": "def concatenate(files, version=\"4.10\", sync=True, add_samples_origin=False, **kwargs):\n         if not files:\n             raise MdfException(\"No files given for merge\")\n         callback = kwargs.get(\"callback\", None)\n         if callback:\n             callback(0, 100)\n         mdf_nr = len(files)\n         versions = []\n         if sync:\n             timestamps = []\n             for file in files:\n                 if isinstance(file, MDF):\n                     timestamps.append(file.header.start_time)\n                     versions.append(file.version)\n                 else:\n                     with open(file, \"rb\") as mdf:\n                         mdf.seek(64)\n                         blk_id = mdf.read(2)\n                         if blk_id == b\"HD\":\n                             header = HeaderV3\n                             versions.append(\"3.00\")\n                         else:\n                             versions.append(\"4.00\")\n                             blk_id += mdf.read(2)\n                             if blk_id == b\"\n                                 header = HeaderV4\n                             else:\n                                 raise MdfException(f\"\"{file}\" is not a valid MDF file\")\n                         header = header(address=64, stream=mdf)\n                         timestamps.append(header.start_time)\n             try:\n                 oldest = min(timestamps)\n             except TypeError:\n                 timestamps = [\n                     timestamp.astimezone(timezone.utc)\n                     for timestamp in timestamps\n                 ]\n                 oldest = min(timestamps)\n             offsets = [(timestamp - oldest).total_seconds() for timestamp in timestamps]\n             offsets = [offset if offset > 0 else 0 for offset in offsets]\n         else:\n             file = files[0]\n             if isinstance(file, MDF):\n                 oldest = file.header.start_time\n                 versions.append(file.version)\n             else:\n                 with open(file, \"rb\") as mdf:\n                     mdf.seek(64)\n                     blk_id = mdf.read(2)\n                     if blk_id == b\"HD\":\n                         versions.append(\"3.00\")\n                         header = HeaderV3\n                     else:\n                         versions.append(\"4.00\")\n                         blk_id += mdf.read(2)\n                         if blk_id == b\"\n                             header = HeaderV4\n                         else:\n                             raise MdfException(f\"\"{file}\" is not a valid MDF file\")\n                     header = header(address=64, stream=mdf)\n                     oldest = header.start_time\n             offsets = [0 for _ in files]\n         version = validate_version_argument(version)\n         merged = MDF(version=version, callback=callback)\n         merged.header.start_time = oldest\n         encodings = []\n         included_channel_names = []\n         if add_samples_origin:\n             origin_conversion = {}\n             for i, mdf in enumerate(files):\n                 origin_conversion[f\"val_{i}\"] = i\n                 if isinstance(mdf, MDF):\n                     origin_conversion[f\"text_{i}\"] = str(mdf.name)\n                 else:\n                     origin_conversion[f\"text_{i}\"] = str(mdf)\n             origin_conversion = from_dict(origin_conversion)\n         for mdf_index, (offset, mdf) in enumerate(zip(offsets, files)):\n             if not isinstance(mdf, MDF):\n                 mdf = MDF(mdf)\n             try:\n                 for can_id, info in mdf.can_logging_db.items():\n                     if can_id not in merged.can_logging_db:\n                         merged.can_logging_db[can_id] = {}\n                     merged.can_logging_db[can_id].update(info)\n             except AttributeError:\n                 pass\n             if mdf_index == 0:\n                 last_timestamps = [None for gp in mdf.groups]\n                 groups_nr = len(mdf.groups)\n             cg_nr = -1\n             for i, group in enumerate(mdf.groups):\n                 included_channels = mdf._included_channels(i)\n                 if mdf_index == 0:\n                     included_channel_names.append(\n                         [group.channels[k].name for k in included_channels]\n                     )\n                 else:\n                     names = [group.channels[k].name for k in included_channels]\n                     if names != included_channel_names[i]:\n                         if sorted(names) != sorted(included_channel_names[i]):\n                             raise MdfException(f\"internal structure of file {mdf_index} is different\")\n                         else:\n                             logger.warning(\n                                 f\"Different channel order in channel group {i} of file {mdf_index}.\"\n                                 \" Data can be corrupted if the there are channels with the same \"\n                                 \"name in this channel group\"\n                             )\n                             included_channels = [\n                                 mdf._validate_channel_selection(\n                                     name=name_,\n                                     group=i,\n                                 )[1]\n                                 for name_ in included_channel_names[i]\n                             ]\n                 if included_channels:\n                     cg_nr += 1\n                 else:\n                     continue\n                 channels_nr = len(group.channels)\n                 y_axis = MERGE\n                 idx = np.searchsorted(CHANNEL_COUNT, channels_nr, side=\"right\") - 1\n                 if idx < 0:\n                     idx = 0\n                 read_size = y_axis[idx]\n                 idx = 0\n                 last_timestamp = last_timestamps[i]\n                 first_timestamp = None\n                 original_first_timestamp = None\n                 if read_size:\n                     mdf.configure(read_fragment_size=int(read_size))\n                 parents, dtypes = mdf._prepare_record(group)\n                 data = mdf._load_data(group)\n                 for fragment in data:\n                     if dtypes.itemsize:\n                         group.record = np.core.records.fromstring(\n                             fragment[0], dtype=dtypes\n                         )\n                     else:\n                         group.record = None\n                     if mdf_index == 0 and idx == 0:\n                         encodings_ = []\n                         encodings.append(encodings_)\n                         signals = []\n                         for j in included_channels:\n                             sig = mdf.get(\n                                 group=i,\n                                 index=j,\n                                 data=fragment,\n                                 raw=True,\n                                 ignore_invalidation_bits=True,\n                                 copy_master=False,\n                             )\n                             if version < \"4.00\":\n                                 if sig.samples.dtype.kind == \"S\":\n                                     encodings_.append(sig.encoding)\n                                     strsig = mdf.get(\n                                         group=i,\n                                         index=j,\n                                         samples_only=True,\n                                         ignore_invalidation_bits=True,\n                                     )[0]\n                                     sig.samples = sig.samples.astype(strsig.dtype)\n                                     del strsig\n                                     if sig.encoding != \"latin-1\":\n                                         if sig.encoding == \"utf-16-le\":\n                                             sig.samples = (\n                                                 sig.samples.view(np.uint16)\n                                                 .byteswap()\n                                                 .view(sig.samples.dtype)\n                                             )\n                                             sig.samples = encode(\n                                                 decode(sig.samples, \"utf-16-be\"),\n                                                 \"latin-1\",\n                                             )\n                                         else:\n                                             sig.samples = encode(\n                                                 decode(sig.samples, sig.encoding),\n                                                 \"latin-1\",\n                                             )\n                                 else:\n                                     encodings_.append(None)\n                             if not sig.samples.flags.writeable:\n                                 sig.samples = sig.samples.copy()\n                             signals.append(sig)\n                         if signals and len(signals[0]):\n                             if offset > 0:\n                                 timestamps = sig[0].timestamps + offset\n                                 for sig in signals:\n                                     sig.timestamps = timestamps\n                             last_timestamp = signals[0].timestamps[-1]\n                             first_timestamp = signals[0].timestamps[0]\n                             original_first_timestamp = first_timestamp\n                         if add_samples_origin:\n                             if signals:\n                                 _s = signals[-1]\n                                 signals.append(\n                                     Signal(\n                                         samples=np.ones(len(_s), dtype=\"<u2\")*mdf_index,\n                                         timestamps=_s.timestamps,\n                                         conversion=origin_conversion,\n                                         name=\"__samples_origin\",\n                                     )\n                                 )\n                                 _s = None\n                         if signals:\n                             merged.append(signals, common_timebase=True)\n                             try:\n                                 if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                                     merged.groups[-1].channel_group.flags = group.channel_group.flags\n                                     merged.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                                     merged.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                                     merged.groups[-1].channel_group.comment = group.channel_group.comment\n                             except AttributeError:\n                                 pass\n                         else:\n                             break\n                         idx += 1\n                     else:\n                         master = mdf.get_master(i, fragment, copy_master=False)\n                         _copied = False\n                         if len(master):\n                             if original_first_timestamp is None:\n                                 original_first_timestamp = master[0]\n                             if offset > 0:\n                                 master = master + offset\n                                 _copied = True\n                             if last_timestamp is None:\n                                 last_timestamp = master[-1]\n                             else:\n                                 if last_timestamp >= master[0]:\n                                     if len(master) >= 2:\n                                         delta = master[1] - master[0]\n                                     else:\n                                         delta = 0.001\n                                     if _copied:\n                                         master -= master[0]\n                                     else:\n                                         master = master - master[0]\n                                         _copied = True\n                                     master += last_timestamp + delta\n                                 last_timestamp = master[-1]\n                             signals = [(master, None)]\n                             for k, j in enumerate(included_channels):\n                                 sig = mdf.get(\n                                     group=i,\n                                     index=j,\n                                     data=fragment,\n                                     raw=True,\n                                     samples_only=True,\n                                     ignore_invalidation_bits=True,\n                                 )\n                                 signals.append(sig)\n                                 if version < \"4.00\":\n                                     encoding = encodings[i][k]\n                                     samples = sig[0]\n                                     if encoding:\n                                         if encoding != \"latin-1\":\n                                             if encoding == \"utf-16-le\":\n                                                 samples = (\n                                                     samples.view(np.uint16)\n                                                     .byteswap()\n                                                     .view(samples.dtype)\n                                                 )\n                                                 samples = encode(\n                                                     decode(samples, \"utf-16-be\"),\n                                                     \"latin-1\",\n                                                 )\n                                             else:\n                                                 samples = encode(\n                                                     decode(samples, encoding), \"latin-1\"\n                                                 )\n                                             sig.samples = samples\n                             if signals:\n                                 if add_samples_origin:\n                                     _s = signals[-1][0]\n                                     signals.append(\n                                         (np.ones(len(_s), dtype=\"<u2\")*mdf_index, None)\n                                     )\n                                     _s = None\n                                 merged.extend(cg_nr, signals)\n                             if first_timestamp is None:\n                                 first_timestamp = master[0]\n                         idx += 1\n                     group.record = None\n                 last_timestamps[i] = last_timestamp\n             if callback:\n                 callback(i + 1 + mdf_index * groups_nr, groups_nr * mdf_nr)\n             if MDF._terminate:\n                 return\n             merged._transfer_events(mdf)\n         return merged", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/QualiSystems/vCenterShell/blob/e2e24cd938a92a68f4a8e6a860810d3ef72aae6d/package/cloudshell/cp/vcenter/commands/save_sandbox.py#L38-L91", "method_name": "save_app", "code": "def save_app(self, si, logger, vcenter_data_model, reservation_id, save_app_actions, cancellation_context):\n         results = []\n         logger.info(\"Save Sandbox command starting on \" + vcenter_data_model.default_datacenter)\n         if not save_app_actions:\n             raise Exception(\"Failed to save app, missing data in request.\")\n         actions_grouped_by_save_types = groupby(save_app_actions, lambda x: x.actionParams.saveDeploymentModel)\n         artifactSaversToActions = {ArtifactHandler.factory(k,\n                                                            self.pyvmomi_service,\n                                                            vcenter_data_model,\n                                                            si,\n                                                            logger,\n                                                            self.deployer,\n                                                            reservation_id,\n                                                            self.resource_model_parser,\n                                                            self.snapshot_saver,\n                                                            self.task_waiter,\n                                                            self.folder_manager,\n                                                            self.port_group_configurer,\n                                                            self.cs)\n                                    : list(g)\n                                    for k, g in actions_grouped_by_save_types}\n         self.validate_requested_save_types_supported(artifactSaversToActions,\n                                                      logger,\n                                                      results)\n         error_results = [r for r in results if not r.success]\n         if not error_results:\n             logger.info(\"Handling Save App requests\")\n             results = self._execute_save_actions_using_pool(artifactSaversToActions,\n                                                             cancellation_context,\n                                                             logger,\n                                                             results)\n             logger.info(\"Completed Save Sandbox command\")\n         else:\n             logger.error(\"Some save app requests were not valid, Save Sandbox command failed.\")\n         return results", "query": "save list to file"}
{"id": "https://github.com/yunpian/yunpian-python-sdk/blob/405a1196ec83fdf29ff454f74ef036974be11970/yunpian_python_sdk/ypclient.py#L195-L208", "method_name": "urlEncodeAndJoin", "code": "def urlEncodeAndJoin(self, seq, sepr=\",\"):\n         try:\n             from urllib.parse import quote_plus as encode\n             return sepr.join([encode(x, encoding=CHARSET_UTF8) for x in seq])\n         except ImportError:\n             from urllib import quote as encode\n             return sepr.join([i for i in map(lambda x: encode(x), seq)])", "query": "encode url"}
{"id": "https://github.com/andrewramsay/sk8-drivers/blob/67347a71762fb421f5ae65a595def5c7879e8b0c/pysk8/calibration/sk8_calibration_gui.py#L49-L57", "method_name": "update", "code": "def update(self):\n         elapsed = int(100 * ((time.time() - self.started_at) / self.GYRO_BIAS_TIME))\n         self.progressBar.setValue(elapsed)\n         self.samples.append(self.imu.gyro)\n         if time.time() - self.started_at > self.GYRO_BIAS_TIME:\n             self.accept()\n             QtWidgets.QMessageBox.information(self, \"Gyro calibration\", \"Calibration finished\")", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/googleapis/google-auth-library-python/blob/2c6ad78917e936f38f87c946209c8031166dc96e/google/auth/_helpers.py#L130-L173", "method_name": "update_query", "code": "def update_query(url, params, remove=None):\n     if remove is None:\n         remove = []\n     parts = urllib.parse.urlparse(url)\n     query_params = urllib.parse.parse_qs(parts.query)\n     query_params.update(params)\n     query_params = {\n         key: value for key, value\n         in six.iteritems(query_params)\n         if key not in remove}\n     new_query = urllib.parse.urlencode(query_params, doseq=True)\n     new_parts = parts._replace(query=new_query)\n     return urllib.parse.urlunparse(new_parts)", "query": "parse query string in url"}
{"id": "https://github.com/airbus-cert/mispy/blob/6d523d6f134d2bd38ec8264be74e73b68403da65/mispy/misp.py#L595-L605", "method_name": "date", "code": "def date(self):\n         if self._date:\n             return self._date\n         return datetime.datetime.now().strftime(\"%Y-%m-%d\")", "query": "get current date"}
{"id": "https://github.com/defensio/defensio-python/blob/c1d2b64be941acb63c452a6d9a5526c59cb37007/defensio/__init__.py#L121-L125", "method_name": "_urlencode", "code": "def _urlencode(self, url):\n       if is_python3():\n         return urllib.parse.urlencode(url)\n       else:\n         return urllib.urlencode(url)", "query": "encode url"}
{"id": "https://github.com/fogleman/pg/blob/124ea3803c788b2c98c4f3a428e5d26842a67b58/pg/core.py#L93-L97", "method_name": "multiply", "code": "def multiply(self, matrix):\n         positions = [matrix * x for x in self.positions]\n         normals = list(self.normals)\n         uvs = list(self.uvs)\n         return Mesh(positions, normals, uvs)", "query": "matrix multiply"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L120-L137", "method_name": "get_table", "code": "def get_table(self, database_name, table_name):\n         result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)\n         return result[\"Table\"]", "query": "get database table name"}
{"id": "https://github.com/Unity-Technologies/ml-agents/blob/37d139af636e4a2351751fbf0f2fca5a9ed7457f/ml-agents/mlagents/trainers/barracuda.py#L228-L258", "method_name": "summary", "code": "def summary(model, print_layer_links, print_barracuda_json, print_tensors):\n     def array_without_brackets(arr):\n         return str(arr)[1:-1] \n     if print_layer_links:\n         for l in model.layers:\n             print(l.name, \" <= \", l.inputs)\n     if print_barracuda_json:\n         print(to_json(model))\n     if model.globals:\n         if isinstance(model.globals, dict):\n             model.globals = {x.name:x.shape for x in model.globals}\n         print(\"GLOBALS:\", array_without_brackets(model.globals))\n     for l in model.layers:\n         if isinstance(model.inputs, dict):\n             ins = {i:model.inputs[i] for i in l.inputs if i in model.inputs}\n         else:\n             ins = [i for i in l.inputs if i in model.inputs]\n         if ins:\n             print(\"IN: %s => \"%s\"\" % (array_without_brackets(ins), l.name))\n     for mem_in, mem_out in zip(model.memories[1::3], model.memories[2::3]):\n         print(\"MEM: \"%s\" => \"%s\"\" % (mem_in, mem_out))\n     print(\"OUT:\", array_without_brackets(model.outputs))\n     if (print_tensors):\n         for l in model.layers:\n             for x in l.tensors:\n                 print(x.name, x.shape, x.data.dtype, x.data)", "query": "print model summary"}
{"id": "https://github.com/jbm950/pygame_toolbox/blob/3fe32145fc149e4dd0963c30a2b6a4dddd4fac0e/pygame_toolbox/graphics/widgets.py#L141-L162", "method_name": "__call__", "code": "def __call__(self, *arg):\n         if self.status:\n             self.status = 0\n             self.image.fill((255, 255, 255))\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)\n         else:\n             self.status = 1\n             if self.checktype == \"r\":\n                 self.draw_rect_check()\n             elif self.checktype == \"c\":\n                 self.draw_circle_check()\n             ptg.Button.set_position(self, self.position, self.midpoint,\n                                     self.surface)", "query": "check if a checkbox is checked"}
{"id": "https://github.com/incf-nidash/nidmresults/blob/438f7cce6abc4a4379b629bd76f4d427891e033f/nidmresults/owl/owl_reader.py#L85-L93", "method_name": "get_nidm_parent", "code": "def get_nidm_parent(self, term):\n         parents = self.get_direct_parents(term)\n         for parent in parents:\n             if not self.is_external_namespace(parent):\n                 return parent\n         return None", "query": "get all parents of xml node"}
{"id": "https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/playhouse/mysql_ext.py#L14-L17", "method_name": "_connect", "code": "def _connect(self):\n         if mysql_connector is None:\n             raise ImproperlyConfigured(\"MySQL connector not installed!\")\n         return mysql_connector.connect(db=self.database, **self.connect_params)", "query": "connect to sql"}
{"id": "https://github.com/tensorflow/lucid/blob/d1a1e2e4fd4be61b89b8cba20dc425a5ae34576e/lucid/scratch/web/svelte.py#L43-L68", "method_name": "SvelteComponent", "code": "def SvelteComponent(name, path):\n   if path[-3:] == \".js\":\n     js_path = path\n   elif path[-5:] == \".html\":\n     print(\"Trying to build svelte component from html...\")\n     js_path = build_svelte(path)\n   js_content = read(js_path, mode=\"r\")\n   def inner(data):\n     id_str = js_id(name)\n     html = _template \\\n         .replace(\"$js\", js_content) \\\n         .replace(\"$name\", name) \\\n         .replace(\"$data\", json.dumps(data)) \\\n         .replace(\"$id\", id_str)\n     _display_html(html)\n   return inner", "query": "get inner html"}
{"id": "https://github.com/coleifer/huey/blob/416e8da1ca18442c08431a91bce373de7d2d200f/huey/storage.py#L401-L405", "method_name": "enqueue", "code": "def enqueue(self, data, priority=None):\n         if priority:\n             raise NotImplementedError(\"Task priorities are not supported by \"\n                                       \"this storage.\")\n         self.conn.lpush(self.queue_key, data)", "query": "priority queue"}
{"id": "https://github.com/TheRobotCarlson/DocxMerge/blob/df8d2add5ff4f33e31d36a6bd3d4495140334538/DocxMerge/DocxMerge.py#L274-L293", "method_name": "replace_doc_text", "code": "def replace_doc_text(file, replacements):\n     document = Document(file)\n     paragraphs = document.paragraphs\n     changes = False\n     for paragraph in paragraphs:\n         text = paragraph.text\n         for original, replace in replacements.items():\n             if original in replace and replace in text:\n                 continue\n             if original in text:\n                 changes = True\n                 text = text.replace(original, replace)\n                 paragraph.text = text\n     if changes:\n         print(\"changing {}\".format(file))\n     document.save(file)", "query": "replace in file"}
{"id": "https://github.com/diffeo/yakonfig/blob/412e195da29b4f4fc7b72967c192714a6f5eaeb5/yakonfig/cmd.py#L132-L148", "method_name": "parseline", "code": "def parseline(self, line):\n         cmd, arg, line = Cmd.parseline(self, line)\n         if cmd and cmd.strip() != \"\":\n             dof = getattr(self, \"do_\" + cmd, None)\n             argf = getattr(self, \"args_\" + cmd, None)\n         else:\n             argf = None\n         if argf:\n             parser = argparse.ArgumentParser(\n                 prog=cmd,\n                 description=getattr(dof, \"__doc__\", None))\n             argf(parser)\n             try:\n                 arg = parser.parse_args(shlex.split(arg))\n             except SystemExit, e:\n                 return \"\", \"\", \"\"\n         return cmd, arg, line", "query": "parse command line argument"}
{"id": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/data.py#L22-L42", "method_name": "uniq_stable", "code": "def uniq_stable(elems):\n     unique = []\n     unique_dict = {}\n     for nn in elems:\n         if nn not in unique_dict:\n             unique.append(nn)\n             unique_dict[nn] = None\n     return unique", "query": "unique elements"}
{"id": "https://github.com/polyaxon/polyaxon/blob/e1724f0756b1a42f9e7aa08a976584a84ef7f016/polyaxon/libs/json_utils.py#L75-L79", "method_name": "dumps", "code": "def dumps(value, escape=False, **kwargs):\n     if escape:\n         return _default_escaped_encoder.encode(value)\n     return _default_encoder.encode(value)", "query": "html encode string"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40", "method_name": "binomial", "code": "def binomial(n,k):\n     if n==k: return 1\n     assert n>k, \"Attempting to call binomial(%d,%d)\" % (n,k)\n     return factorial(n)//(factorial(k)*factorial(n-k))", "query": "binomial distribution"}
{"id": "https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/regression.py#L10-L223", "method_name": "linear_regression", "code": "def linear_regression(X, y, add_intercept=True, coef_only=False, alpha=0.05,\n                       as_dataframe=True, remove_na=False):\n     if isinstance(X, pd.DataFrame):\n         names = X.keys().tolist()\n     elif isinstance(X, pd.Series):\n         names = [X.name]\n     else:\n         names = []\n     assert 0 < alpha < 1\n     assert y.ndim == 1, \"y must be one-dimensional.\"\n     X = np.asarray(X)\n     y = np.asarray(y)\n     if X.ndim == 1:\n         X = X[..., np.newaxis]\n     if remove_na:\n         X, y = rm_na(X, y[..., np.newaxis], paired=True, axis=\"rows\")\n         y = np.squeeze(y)\n     y_gd = np.isfinite(y).all()\n     X_gd = np.isfinite(X).all()\n     assert y_gd, \"Target (y) contains NaN or Inf. Please remove them.\"\n     assert X_gd, \"Predictors (X) contain NaN or Inf. Please remove them.\"\n     assert y.shape[0] == X.shape[0], \"X and y must have same number of samples\"\n     if not names:\n         names = [\"x\" + str(i + 1) for i in range(X.shape[1])]\n     if add_intercept:\n         X = np.column_stack((np.ones(X.shape[0]), X))\n         names.insert(0, \"Intercept\")\n     coef = np.linalg.lstsq(X, y, rcond=None)[0]\n     if coef_only:\n         return coef\n     pred = np.dot(X, coef)\n     resid = np.square(y - pred)\n     ss_res = resid.sum()\n     n, p = X.shape[0], X.shape[1]\n     dof = n - p if add_intercept else n - p - 1\n     MSE = ss_res / dof\n     beta_var = MSE * (np.linalg.pinv(np.dot(X.T, X)).diagonal())\n     beta_se = np.sqrt(beta_var)\n     ss_tot = np.square(y - y.mean()).sum()\n     r2 = 1 - (ss_res / ss_tot)\n     adj_r2 = 1 - (1 - r2) * (n - 1) / dof\n     T = coef / beta_se\n     pval = np.array([2 * t.sf(np.abs(i), dof) for i in T])\n     crit = t.ppf(1 - alpha / 2, dof)\n     marg_error = crit * beta_se\n     ll = coef - marg_error\n     ul = coef + marg_error\n     ll_name = \"CI[%.1f%%]\" % (100 * alpha / 2)\n     ul_name = \"CI[%.1f%%]\" % (100 * (1 - alpha / 2))\n     stats = {\"names\": names, \"coef\": coef, \"se\": beta_se, \"T\": T,\n              \"pval\": pval, \"r2\": r2, \"adj_r2\": adj_r2, ll_name: ll,\n              ul_name: ul}\n     if as_dataframe:\n         return pd.DataFrame.from_dict(stats)\n     else:\n         return stats", "query": "linear regression"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/ipythonconsole/plugin.py#L577-L583", "method_name": "set_working_directory", "code": "def set_working_directory(self, dirname): \n         if dirname: \n             self.main.workingdirectory.chdir(dirname, refresh_explorer=True, \n                                              refresh_console=False)", "query": "set working directory"}
{"id": "https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/thread.py#L204-L226", "method_name": "get_pid", "code": "def get_pid(self):\n         if self.dwProcessId is None:\n             if self.__process is not None:\n                 self.dwProcessId = self.get_process().get_pid()\n             else:\n                 try:\n                     hThread = self.get_handle(\n                                         win32.THREAD_QUERY_LIMITED_INFORMATION)\n                     self.dwProcessId = win32.GetProcessIdOfThread(hThread)\n                 except AttributeError:\n                     self.dwProcessId = self.__get_pid_by_scanning()\n         return self.dwProcessId", "query": "get current process id"}
{"id": "https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/viz.py#L47-L65", "method_name": "plot_confusion_reports", "code": "def plot_confusion_reports(y, y_hat, class_names=None):\n     if class_names is None:\n         class_names = list(set(y).union(set(y_hat)))\n     cnf_matrix = confusion_matrix(y, y_hat)\n     np.set_printoptions(precision=2)\n     plt.figure()\n     plot_confusion_matrix(cnf_matrix, classes=class_names,\n                           title=\"Confusion matrix, without normalization\")\n     plt.figure()\n     plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                           title=\"Normalized confusion matrix\")\n     plt.show()", "query": "confusion matrix"}
{"id": "https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2510-L2520", "method_name": "distinct_counts", "code": "def distinct_counts(self):\n         k = [hash(self.values[:, i].tobytes()) for i in range(self.shape[1])]\n         counts = sorted(collections.Counter(k).values(), reverse=True)\n         return np.asarray(counts)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/samuelcolvin/grablib/blob/2fca8a3950f29fb2a97a7bd75c0839060a91cedf/grablib/download.py#L125-L152", "method_name": "_extract_zip", "code": "def _extract_zip(self, url, content, value):\n         zipinmemory = IO(content)\n         zcopied = 0\n         with zipfile.ZipFile(zipinmemory) as zipf:\n             progress_logger.debug(\"%d files in zip archive\", len(zipf.namelist()))\n             for filepath in zipf.namelist():\n                 if filepath.endswith(\"/\"):\n                     continue\n                 regex_pattern, targets = None, None\n                 for r, t in value.items():\n                     if re.match(r, filepath):\n                         regex_pattern, targets = r, t\n                         break\n                 if regex_pattern is None:\n                     progress_logger.debug(\"\"%s\" no target found\", filepath)\n                 elif targets is None:\n                     progress_logger.debug(\"\"%s\" skipping (regex: \"%s\")\", filepath, regex_pattern)\n                 else:\n                     if isinstance(targets, str):\n                         targets = [targets]\n                     for target in targets:\n                         new_path = self._file_path(filepath, target, regex=regex_pattern)\n                         progress_logger.debug(\"\"%s\" 鉃?\"%s\" (regex: \"%s\")\",\n                                               filepath, new_path.relative_to(self.download_root), regex_pattern)\n                         self._write(new_path, zipf.read(filepath), url)\n                         zcopied += 1\n         return zcopied", "query": "extract zip file recursively"}
{"id": "https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/snep/client.py#L51-L74", "method_name": "recv_response", "code": "def recv_response(socket, acceptable_length, timeout):\n     if socket.poll(\"recv\", timeout):\n         snep_response = socket.recv()\n         if len(snep_response) < 6:\n             log.debug(\"snep response initial fragment too short\")\n             return None\n         version, status, length = struct.unpack(\">BBL\", snep_response[:6])\n         if length > acceptable_length:\n             log.debug(\"snep response exceeds acceptable length\")\n             return None\n         if len(snep_response) - 6 < length:\n             socket.send(b\"\\x10\\x00\\x00\\x00\\x00\\x00\")\n             while len(snep_response) - 6 < length:\n                 if socket.poll(\"recv\", timeout):\n                     snep_response += socket.recv()\n                 else:\n                     return None\n         return bytearray(snep_response)", "query": "socket recv timeout"}
{"id": "https://github.com/why2pac/dp-tornado/blob/a5948f5693f6ee2d9bab31f611fedc074e1caa96/dp_tornado/helper/datetime/date.py#L74-L76", "method_name": "yyyymmdd", "code": "def yyyymmdd(self, auto=None, datetime=None, timezone=None, timestamp=None, ms=False, concat=\"\"):\n        datetime = self.convert(auto=auto, datetime=datetime, timezone=timezone, timestamp=timestamp, ms=ms)\n        return \"%04d%s%02d%s%02d\" % (datetime.year, concat, datetime.month, concat, datetime.day)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/api.py#L1645-L1692", "method_name": "cluster_mini_batch_kmeans", "code": "def cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric=\"euclidean\",\n                               init_strategy=\"kmeans++\", n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs):\n     r\n     from pyemma.coordinates.clustering.kmeans import MiniBatchKmeansClustering\n     res = MiniBatchKmeansClustering(n_clusters=k, max_iter=max_iter, metric=metric, init_strategy=init_strategy,\n                                     batch_size=batch_size, n_jobs=n_jobs, skip=skip, clustercenters=clustercenters)\n     from pyemma.util.reflection import get_default_args\n     cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_mini_batch_kmeans)[\"chunksize\"], **kwargs)\n     if data is not None:\n         res.estimate(data, chunksize=cs)\n     else:\n         res.chunksize = chunksize\n     return res", "query": "k means clustering"}
{"id": "https://github.com/LiftoffSoftware/htmltag/blob/f6989f9a3301e7c96ee613e5dbbe43b2bde615c7/htmltag.py#L442-L448", "method_name": "escape", "code": "def escape(self, string):\n         html_entities = {\"&\": \"&amp;\", \"<\": \"&lt;\", \">\": \"&gt;\"}\n         return HTML(\"\".join(html_entities.get(c, c) for c in string))", "query": "html entities replace"}
{"id": "https://github.com/kennethreitz/requests-html/blob/b59a9f2fb9333d7d467154a0fd82978efdb9d23b/requests_html.py#L110-L111", "method_name": "html", "code": "def html(self, html: str) -> None:\n        self._html = html.encode(self.encoding)", "query": "html encode string"}
{"id": "https://github.com/SiLab-Bonn/pixel_clusterizer/blob/d2c8c3072fb03ebb7c6a3e8c57350fbbe38efd4d/pixel_clusterizer/clusterizer.py#L120-L126", "method_name": "_init_arrays", "code": "def _init_arrays(self, size=0):\n         if self.initialized:\n             self._cluster_hits = np.zeros(shape=(size, ), dtype=np.dtype(self._cluster_hits_descr))\n             self._clusters = np.zeros(shape=(size, ), dtype=np.dtype(self._cluster_descr))\n             self._assigned_hit_array = np.zeros(shape=(size, ), dtype=np.bool)\n             self._cluster_hit_indices = np.empty(shape=(size, ), dtype=np_int_type_chooser(size))\n             self._cluster_hit_indices.fill(-1)", "query": "initializing array"}
{"id": "https://github.com/neuropsychology/NeuroKit.py/blob/c9589348fbbde0fa7e986048c48f38e6b488adfe/examples/UnderDev/eeg/eeg_microstates.py#L201-L253", "method_name": "eeg_microstates_clustering", "code": "def eeg_microstates_clustering(data, n_microstates=4, clustering_method=\"kmeans\", n_jobs=1, n_init=25, occurence_rejection_treshold=0.05, max_refitting=5, verbose=True):\n     training_set = data.copy()\n     if verbose is True:\n         print(\"- Initializing the clustering algorithm...\")\n     if clustering_method == \"kmeans\":\n         algorithm = sklearn.cluster.KMeans(init=\"k-means++\", n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n     elif clustering_method == \"spectral\":\n         algorithm = sklearn.cluster.SpectralClustering(n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n     elif clustering_method == \"agglom\":\n         algorithm = sklearn.cluster.AgglomerativeClustering(n_clusters=n_microstates, linkage=\"complete\")\n     elif clustering_method == \"dbscan\":\n         algorithm = sklearn.cluster.DBSCAN(min_samples=100)\n     elif clustering_method == \"affinity\":\n         algorithm = sklearn.cluster.AffinityPropagation(damping=0.5)\n     else:\n         print(\"NeuroKit Error: eeg_microstates(): clustering_method must be \"kmeans\", \"spectral\", \"dbscan\", \"affinity\" or \"agglom\"\")\n     refitting = 0  \n     good_fit_achieved = False\n     while good_fit_achieved is False:\n         good_fit_achieved = True\n         if verbose is True:\n             print(\"- Fitting the classifier...\")\n         algorithm.fit(training_set)\n         if verbose is True:\n             print(\"- Clustering back the initial data...\")\n         predicted = algorithm.fit_predict(training_set)\n         if verbose is True:\n             print(\"- Check for abnormalities...\")\n         occurences = dict(collections.Counter(predicted))\n         masks = [np.array([True]*len(training_set))]\n         for microstate in occurences:\n             if occurences[microstate] < len(data)*occurence_rejection_treshold:\n                 good_fit_achieved = False\n                 refitting += 1  \n                 print(\"NeuroKit Warning: eeg_microstates(): detected some outliers: refitting the classifier (n=\" + str(refitting) + \").\")\n                 masks.append(predicted!=microstate)\n         mask = np.all(masks, axis=0)\n         training_set = training_set[mask]\n     return(algorithm)", "query": "k means clustering"}
{"id": "https://github.com/ewels/MultiQC/blob/2037d6322b2554146a74efbf869156ad20d4c4ec/multiqc/modules/bcl2fastq/bcl2fastq.py#L142-L268", "method_name": "parse_file_as_json", "code": "def parse_file_as_json(self, myfile):\n         try:\n             content = json.loads(myfile[\"f\"])\n         except ValueError:\n             log.warn(\"Could not parse file as json: {}\".format(myfile[\"fn\"]))\n             return\n         runId = content[\"RunId\"]\n         if runId not in self.bcl2fastq_data:\n             self.bcl2fastq_data[runId] = dict()\n         run_data = self.bcl2fastq_data[runId]\n         for conversionResult in content.get(\"ConversionResults\", []):\n             l = conversionResult[\"LaneNumber\"]\n             lane = \"L{}\".format(conversionResult[\"LaneNumber\"])\n             if lane in run_data:\n                 log.debug(\"Duplicate runId/lane combination found! Overwriting: {}\".format(self.prepend_runid(runId, lane)))\n             run_data[lane] = {\n                 \"total\": 0,\n                 \"total_yield\": 0,\n                 \"perfectIndex\": 0,\n                 \"samples\": dict(),\n                 \"yieldQ30\": 0,\n                 \"qscore_sum\": 0\n             }\n             rlane = run_data[lane]\n             try:\n                 unknown_barcode = content[\"UnknownBarcodes\"][l - 1][\"Barcodes\"]\n             except IndexError:\n                 unknown_barcode = next(\n                     (item[\"Barcodes\"] for item in content[\"UnknownBarcodes\"] if item[\"Lane\"] == 8),\n                     None\n                 )\n             run_data[lane][\"unknown_barcodes\"] = unknown_barcode\n             for demuxResult in conversionResult.get(\"DemuxResults\", []):\n                 if demuxResult[\"SampleName\"] == demuxResult[\"SampleName\"]:\n                     sample = demuxResult[\"SampleName\"]\n                 else:\n                     sample = \"{}-{}\".format(demuxResult[\"SampleId\"], demuxResult[\"SampleName\"])\n                 if sample in run_data[lane][\"samples\"]:\n                     log.debug(\"Duplicate runId/lane/sample combination found! Overwriting: {}, {}\".format(self.prepend_runid(runId, lane), sample))\n                 run_data[lane][\"samples\"][sample] = {\n                     \"total\": 0,\n                     \"total_yield\": 0,\n                     \"perfectIndex\": 0,\n                     \"filename\": os.path.join(myfile[\"root\"], myfile[\"fn\"]),\n                     \"yieldQ30\": 0,\n                     \"qscore_sum\": 0\n                 }\n                 lsample = run_data[lane][\"samples\"][sample]\n                 for r in range(1,5):\n                         lsample[\"R{}_yield\".format(r)] = 0\n                         lsample[\"R{}_Q30\".format(r)] = 0\n                         lsample[\"R{}_trimmed_bases\".format(r)] = 0\n                 rlane[\"total\"] += demuxResult[\"NumberReads\"]\n                 rlane[\"total_yield\"] += demuxResult[\"Yield\"]\n                 lsample[\"total\"] += demuxResult[\"NumberReads\"]\n                 lsample[\"total_yield\"] += demuxResult[\"Yield\"]\n                 for indexMetric in demuxResult.get(\"IndexMetrics\", []):\n                     rlane[\"perfectIndex\"] += indexMetric[\"MismatchCounts\"][\"0\"]\n                     lsample[\"perfectIndex\"] += indexMetric[\"MismatchCounts\"][\"0\"]\n                 for readMetric in demuxResult.get(\"ReadMetrics\", []):\n                     r = readMetric[\"ReadNumber\"]\n                     rlane[\"yieldQ30\"] += readMetric[\"YieldQ30\"]\n                     rlane[\"qscore_sum\"] += readMetric[\"QualityScoreSum\"]\n                     lsample[\"yieldQ30\"] += readMetric[\"YieldQ30\"]\n                     lsample[\"qscore_sum\"] += readMetric[\"QualityScoreSum\"]\n                     lsample[\"R{}_yield\".format(r)] += readMetric[\"Yield\"]\n                     lsample[\"R{}_Q30\".format(r)] += readMetric[\"YieldQ30\"]\n                     lsample[\"R{}_trimmed_bases\".format(r)] += readMetric[\"TrimmedBases\"]\n                 for r in range(1,5):\n                     if not lsample[\"R{}_yield\".format(r)] and not lsample[\"R{}_Q30\".format(r)] and not lsample[\"R{}_trimmed_bases\".format(r)]:\n                         lsample.pop(\"R{}_yield\".format(r))\n                         lsample.pop(\"R{}_Q30\".format(r))\n                         lsample.pop(\"R{}_trimmed_bases\".format(r))\n             undeterminedYieldQ30 = 0\n             undeterminedQscoreSum = 0\n             undeterminedTrimmedBases = 0\n             if \"Undetermined\" in conversionResult:\n                 for readMetric in conversionResult[\"Undetermined\"][\"ReadMetrics\"]:\n                     undeterminedYieldQ30 += readMetric[\"YieldQ30\"]\n                     undeterminedQscoreSum += readMetric[\"QualityScoreSum\"]\n                     undeterminedTrimmedBases += readMetric[\"TrimmedBases\"]\n                 run_data[lane][\"samples\"][\"undetermined\"] = {\n                     \"total\": conversionResult[\"Undetermined\"][\"NumberReads\"],\n                     \"total_yield\": conversionResult[\"Undetermined\"][\"Yield\"],\n                     \"perfectIndex\": 0,\n                     \"yieldQ30\": undeterminedYieldQ30,\n                     \"qscore_sum\": undeterminedQscoreSum,\n                     \"trimmed_bases\": undeterminedTrimmedBases\n                 }\n         for lane_id, lane in run_data.items():\n             try:\n                 lane[\"percent_Q30\"] = (float(lane[\"yieldQ30\"])\n                     / float(lane[\"total_yield\"])) * 100.0\n             except ZeroDivisionError:\n                 lane[\"percent_Q30\"] = \"NA\"\n             try:\n                 lane[\"percent_perfectIndex\"] = (float(lane[\"perfectIndex\"])\n                     / float(lane[\"total\"])) * 100.0\n             except ZeroDivisionError:\n                 lane[\"percent_perfectIndex\"] = \"NA\"\n             try:\n                 lane[\"mean_qscore\"] = float(lane[\"qscore_sum\"]) / float(lane[\"total_yield\"])\n             except ZeroDivisionError:\n                 lane[\"mean_qscore\"] = \"NA\"\n             for sample_id, sample in lane[\"samples\"].items():\n                 try:\n                     sample[\"percent_Q30\"] = (float(sample[\"yieldQ30\"])\n                         / float(sample[\"total_yield\"])) * 100.0\n                 except ZeroDivisionError:\n                     sample[\"percent_Q30\"] = \"NA\"\n                 try:\n                     sample[\"percent_perfectIndex\"] = (float(sample[\"perfectIndex\"])\n                         / float(sample[\"total\"])) * 100.0\n                 except ZeroDivisionError:\n                     sample[\"percent_perfectIndex\"] = \"NA\"\n                 try:\n                     sample[\"mean_qscore\"] = float(sample[\"qscore_sum\"]) / float(sample[\"total_yield\"])\n                 except ZeroDivisionError:\n                     sample[\"mean_qscore\"] = \"NA\"", "query": "parse json file"}
{"id": "https://github.com/openstates/billy/blob/5fc795347f12a949e410a8cfad0c911ea6bced67/billy/web/api/handlers.py#L27-L48", "method_name": "_build_mongo_filter", "code": "def _build_mongo_filter(request, keys, icase=True):\n     _filter = {}\n     keys = set(keys) - set([\"fields\"])\n     for key in keys:\n         value = request.GET.get(key)\n         if value:\n             if key in _lower_fields:\n                 _filter[key] = value.lower()\n             elif key.endswith(\"__in\"):\n                 values = value.split(\" \")\n                 _filter[key[:-4]] = values\n             elif key == \"bill_id\":\n                 _filter[key] = fix_bill_id(value.upper())\n             else:\n                 _filter[key] = re.compile(\"^%s$\" % value, re.IGNORECASE)\n     return _filter", "query": "regex case insensitive"}
{"id": "https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/utils/parse.py#L21-L80", "method_name": "thread", "code": "def thread(data, default=u\"Untitled.\", id=None):\n     html = html5lib.parse(data, treebuilder=\"dom\")\n     assert html.lastChild.nodeName == \"html\"\n     html = html.lastChild\n     el = list(filter(lambda i: i.attributes[\"id\"].value == \"isso-thread\",\n                      filter(lambda i: \"id\" in i.attributes,\n                             chain(*map(html.getElementsByTagName, (\"div\", \"section\"))))))\n     if not el:\n         return id, default\n     el = el[0]\n     visited = []\n     def recurse(node):\n         for child in node.childNodes:\n             if child.nodeType != child.ELEMENT_NODE:\n                 continue\n             if child.nodeName.upper() == \"H1\":\n                 return child\n             if child not in visited:\n                 return recurse(child)\n     def gettext(rv):\n         for child in rv.childNodes:\n             if child.nodeType == child.TEXT_NODE:\n                 yield child.nodeValue\n             if child.nodeType == child.ELEMENT_NODE:\n                 for item in gettext(child):\n                     yield item\n     try:\n         id = unquote(el.attributes[\"data-isso-id\"].value)\n     except (KeyError, AttributeError):\n         pass\n     try:\n         return id, unquote(el.attributes[\"data-title\"].value)\n     except (KeyError, AttributeError):\n         pass\n     while el is not None:  \n         visited.append(el)\n         rv = recurse(el)\n         if rv:\n             return id, \"\".join(gettext(rv)).strip()\n         el = el.parentNode\n     return id, default", "query": "reading element from html - <td>"}
{"id": "https://github.com/nefarioustim/parker/blob/ccc1de1ac6bfb5e0a8cfa4fdebb2f38f2ee027d6/parker/parsedpage.py#L65-L75", "method_name": "_filter_by_regex", "code": "def _filter_by_regex(self, regex, text, group=1):\n         match = re.search(\n             regex,\n             text,\n             re.MULTILINE\n         )\n         return match.group(group).strip() if (\n             match and match.groups()\n         ) else text", "query": "regex case insensitive"}
{"id": "https://github.com/cs50/lib50/blob/941767f6c0a3b81af0cdea48c25c8d5a761086eb/lib50/_api.py#L570-L611", "method_name": "_lfs_add", "code": "def _lfs_add(files, git):\n     larges, huges = [], []\n     for file in files:\n         size = os.path.getsize(file)\n         if size > (100 * 1024 * 1024):\n             larges.append(file)\n         elif size > (2 * 1024 * 1024 * 1024):\n             huges.append(file)\n     if huges:\n         raise Error(_(\"These files are too large to be submitted:\n{}\n\"\n                       \"Remove these files from your directory \"\n                       \"and then re-run {}!\").format(\"\n\".join(huges), org))\n     if larges:\n         if not shutil.which(\"git-lfs\"):\n             raise Error(_(\"These files are too large to be submitted:\n{}\n\"\n                           \"Install git-lfs (or remove these files from your directory) \"\n                           \"and then re-run!\").format(\"\n\".join(larges)))\n         _run(git(\"lfs install --local\"))\n         _run(git(\"config credential.helper cache\"))\n         for large in larges:\n             _run(git(\"rm --cached {}\".format(shlex.quote(large))))\n             _run(git(\"lfs track {}\".format(shlex.quote(large))))\n             _run(git(\"add {}\".format(shlex.quote(large))))\n         _run(git(\"add --force .gitattributes\"))", "query": "unzipping large files"}
{"id": "https://github.com/gunthercox/ChatterBot/blob/1a03dcb45cba7bdc24d3db5e750582e0cb1518e2/chatterbot/parsing.py#L506-L517", "method_name": "convert_string_to_number", "code": "def convert_string_to_number(value):\n     if value is None:\n         return 1\n     if isinstance(value, int):\n         return value\n     if value.isdigit():\n         return int(value)\n     num_list = map(lambda s: NUMBERS[s], re.findall(numbers + \"+\", value.lower()))\n     return sum(num_list)", "query": "convert string to number"}
{"id": "https://github.com/adrienverge/yamllint/blob/fec2c2fba736cabf6bee6b5eeb905cab0dc820f6/yamllint/rules/new_line_at_end_of_file.py#L34-L37", "method_name": "check", "code": "def check(conf, line):\n     if line.end == len(line.buffer) and line.end > line.start:\n         yield LintProblem(line.line_no, line.end - line.start + 1,\n                           \"no new line character at the end of file\")", "query": "read text file line by line"}
{"id": "https://github.com/realitix/vulkan/blob/07285387092aaa61d2d71fa2913d60a73f022cbe/vulkan/_vulkan.py#L6130-L6135", "method_name": "vkCmdDispatch", "code": "def vkCmdDispatch(commandBuffer\n         ,groupCountX\n         ,groupCountY\n         ,groupCountZ\n         ,):\n     result = _callApi(lib.vkCmdDispatch, commandBuffer,groupCountX,groupCountY,groupCountZ)", "query": "group by count"}
{"id": "https://github.com/lotrekagency/djlotrek/blob/10a304103768c3d59bdb8859eba86ef3327c9598/djlotrek/templatetags/djlotrek_filters.py#L50-L53", "method_name": "regex_match", "code": "def regex_match(value, regex):\n     pattern = re.compile(regex)\n     if pattern.match(value):\n         return True", "query": "regex case insensitive"}
{"id": "https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_pretty_print.py#L8-L14", "method_name": "json_pretty_print", "code": "def json_pretty_print(s):\n     s = json.loads(s)\n     return json.dumps(s,\n                       sort_keys=True,\n                       indent=4,\n                       separators=(\",\", \": \"))", "query": "pretty print json"}
{"id": "https://github.com/infobip/infobip-api-python-client/blob/2da11962f877294c53bd545715e62d6ce5c139f0/infobip/util/http.py#L14-L16", "method_name": "deserialize", "code": "def deserialize(self, s, cls):\n        vals = json.JSONDecoder().decode(s)\n        return self.deserialize_map(vals, cls)", "query": "deserialize json"}
{"id": "https://github.com/jborean93/smbprotocol/blob/d8eb00fbc824f97d0f4946e3f768c5e6c723499a/smbprotocol/structure.py#L775-L785", "method_name": "_to_string", "code": "def _to_string(self):\n         enum_name = None\n         value = self._get_calculated_value(self.value)\n         for enum, enum_value in vars(self.enum_type).items():\n             if value == enum_value:\n                 enum_name = enum\n                 break\n         if enum_name is None:\n             return \"(%d) UNKNOWN_ENUM\" % value\n         else:\n             return \"(%d) %s\" % (value, enum_name)", "query": "get name of enumerated value"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/pdf.py#L161-L186", "method_name": "add_to_writer", "code": "def add_to_writer(self,\n                       writer: PdfFileWriter,\n                       start_recto: bool = True) -> None:\n         if self.is_html:\n             pdf = get_pdf_from_html(\n                 html=self.html,\n                 header_html=self.header_html,\n                 footer_html=self.footer_html,\n                 wkhtmltopdf_filename=self.wkhtmltopdf_filename,\n                 wkhtmltopdf_options=self.wkhtmltopdf_options)\n             append_memory_pdf_to_writer(pdf, writer, start_recto=start_recto)\n         elif self.is_filename:\n             if start_recto and writer.getNumPages() % 2 != 0:\n                 writer.addBlankPage()\n             writer.appendPagesFromReader(PdfFileReader(\n                 open(self.filename, \"rb\")))\n         else:\n             raise AssertionError(\"PdfPlan: shouldn\"t get here!\")", "query": "convert html to pdf"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547", "method_name": "check_checkbox", "code": "def check_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     if not check_box.is_selected():\n         check_box.click()", "query": "check if a checkbox is checked"}
{"id": "https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/im.py#L202-L230", "method_name": "_do_post", "code": "def _do_post(self, url, **kwargs):\n         scaleioapi_post_headers = {\"Content-type\":\"application/json\",\"Version\":\"1.0\"}\n         self.logger.debug(\"_do_post()\")\n         if kwargs:\n             for key, value in kwargs.iteritems():\n                 if key == \"headers\":\n                     scaleio_post_headers = value\n                     print \"Adding custom POST headers\"\n                 if key == \"files\":\n                     upl_files = value\n                     print \"Adding files to upload\"\n         try:\n             response = self._session.post(url, headers=scaleioapi_post_headers, verify_ssl=self._im_verify_ssl, files=upl_files)\n             self.logger.debug(\"_do_post() - Response: \" + \"{}\".format(response.text))\n             if response.status_code == requests.codes.ok:\n                 return response\n             else:\n                 self.logger.error(\"_do_post() - Response Code: \" + \"{}\".format(response.status_code))\n                 raise RuntimeError(\"_do_post() - HTTP response error\" + response.status_code)\n         except:\n             raise RuntimeError(\"_do_post() - Communication error with ScaleIO gateway\")\n         return response", "query": "custom http error response"}
{"id": "https://github.com/jeffa/HTML-Auto-python/blob/e0d24335d86c21b6278f776f3d3416c90e438ab9/t/05-encode.py#L6-L70", "method_name": "test_lower", "code": "def test_lower(self):\n         encoder = Encoder()\n         self.assertEqual(\n             \"&amp;&lt;&gt;&quot;&apos;\",\n             encoder.encode( \"&<>\"\\\"\" ),\n             \"default chars encoded when chars is nil\"\n         )\n         self.assertEqual(\n             \"&amp;&lt;&gt;&quot;&apos;\",\n             encoder.encode( \"&<>\"\\\"\", \"\" ),\n             \"encodes when chars is empty\"\n         )\n         self.assertEqual(\n             \"h&\n             encoder.encode( \"hello\", \"e\" ),\n             \"requested chars encoded correctly\"\n         )\n         self.assertEqual(\n             \"hell&\n             encoder.encode( \"hell0\", 0 ),\n             \"zero encodes correctly\"\n         )\n         self.assertEqual(\n             \"&amp;b&\n             encoder.encode( \"&bar\", \"a&\" ),\n             \"ampersand is not double encoded\"\n         )\n         self.assertEqual(\n             \"hello\",\n             encoder.encode( \"hello\" ),\n             \"no encodes when default chars is nil\"\n         )\n         self.assertEqual(\n             \"hello\",\n             encoder.encode( \"hello\", \"\" ),\n             \"no encodes when default chars is empty\"\n         )\n         deadbeef = chr(222) + chr(173) + chr(190) + chr(239)\n         self.assertEqual(\n             \"&THORN;&shy;&frac34;&iuml;\",\n             encoder.encode( deadbeef, deadbeef ),\n             \"hex codes encoded correctly\"\n         )\n         self.assertEqual(\n             \"&THORN;&shy;&frac34;&iuml;\",\n             encoder.encode( deadbeef ),\n             \"hex codes encoded correctly when chars is nil\"\n         )\n         self.assertEqual(\n             \"&THORN;&shy;&frac34;&iuml;\",\n             encoder.encode( deadbeef, \"\" ),\n             \"hex codes encoded correctly when chars is empty\"\n         )", "query": "html encode string"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/stickers.py#L281-L291", "method_name": "pdf_from_post", "code": "def pdf_from_post(self):\n         html = self.request.form.get(\"html\")\n         style = self.request.form.get(\"style\")\n         reporthtml = \"<html><head>{0}</head><body>{1}</body></html>\"\n         reporthtml = reporthtml.format(style, html)\n         reporthtml = safe_unicode(reporthtml).encode(\"utf-8\")\n         pdf_fn = tempfile.mktemp(suffix=\".pdf\")\n         pdf_file = createPdf(htmlreport=reporthtml, outfile=pdf_fn)\n         return pdf_file", "query": "convert html to pdf"}
{"id": "https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/pipeline/disambiguate/run.py#L35-L39", "method_name": "nat_cmp", "code": "def nat_cmp(a, b):\n     convert = lambda text: int(text) if text.isdigit() else text \n     alphanum_key = lambda key: [ convert(c) for c in re.split(\"([0-9]+)\", key) ] \n     return (alphanum_key(a) > alphanum_key(b))-(alphanum_key(a) < alphanum_key(b))", "query": "convert string to number"}
{"id": "https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/tasks.py#L4149-L4153", "method_name": "set_workdir", "code": "def set_workdir(self, workdir, chroot=False):\n         super().set_workdir(workdir, chroot=chroot)\n         self.output_file = self.log_file", "query": "set working directory"}
{"id": "https://github.com/mclarkk/lifxlan/blob/ead0e3114d6aa2e5e77dab1191c13c16066c32b0/lifxlan/message.py#L126-L130", "method_name": "convert_MAC_to_int", "code": "def convert_MAC_to_int(addr):\n     reverse_bytes_str = addr.split(\":\")\n     reverse_bytes_str.reverse()\n     addr_str = \"\".join(reverse_bytes_str)\n     return int(addr_str, 16)", "query": "reverse a string"}
{"id": "https://github.com/lpantano/seqcluster/blob/774e23add8cd4fdc83d626cea3bd1f458e7d060d/seqcluster/libs/thinkbayes.py#L1530-L1535", "method_name": "EvalBinomialPmf", "code": "def EvalBinomialPmf(k, n, p):\n     return scipy.stats.binom.pmf(k, n, p)", "query": "binomial distribution"}
{"id": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/heatmaps.py#L110-L158", "method_name": "draw", "code": "def draw(self, size=None, cmap=\"jet\"):\n         heatmaps_uint8 = self.to_uint8()\n         heatmaps_drawn = []\n         for c in sm.xrange(heatmaps_uint8.shape[2]):\n             heatmap_c = heatmaps_uint8[..., c:c+1]\n             if size is not None:\n                 heatmap_c_rs = ia.imresize_single_image(heatmap_c, size, interpolation=\"nearest\")\n             else:\n                 heatmap_c_rs = heatmap_c\n             heatmap_c_rs = np.squeeze(heatmap_c_rs).astype(np.float32) / 255.0\n             if cmap is not None:\n                 import matplotlib.pyplot as plt\n                 cmap_func = plt.get_cmap(cmap)\n                 heatmap_cmapped = cmap_func(heatmap_c_rs)\n                 heatmap_cmapped = np.delete(heatmap_cmapped, 3, 2)\n             else:\n                 heatmap_cmapped = np.tile(heatmap_c_rs[..., np.newaxis], (1, 1, 3))\n             heatmap_cmapped = np.clip(heatmap_cmapped * 255, 0, 255).astype(np.uint8)\n             heatmaps_drawn.append(heatmap_cmapped)\n         return heatmaps_drawn", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/HearthSim/python-hsreplay/blob/ec03a18a75ae4c1e0facc583a7213a4c5b7f99ff/hsreplay/elements.py#L47-L66", "method_name": "xml", "code": "def xml(self):\n \t\telement = ElementTree.Element(self.tagname)\n \t\tfor node in self.nodes:\n \t\t\telement.append(node.xml())\n \t\tfor attr in self.attributes:\n \t\t\tattrib = getattr(self, attr, None)\n \t\t\tif attrib is not None:\n \t\t\t\tif isinstance(attrib, bool):\n \t\t\t\t\tattrib = str(attrib).lower()\n \t\t\t\telif isinstance(attrib, int):\n \t\t\t\t\tattrib = str(int(attrib))\n \t\t\t\telement.attrib[attr] = attrib\n \t\tif self.timestamp and self.ts:\n \t\t\telement.attrib[\"ts\"] = self.ts.isoformat()\n \t\tfor k, v in self._attributes.items():\n \t\t\telement.attrib[k] = v\n \t\treturn element", "query": "set file attrib hidden"}
{"id": "https://github.com/ofek/pypinfo/blob/48d56e690d7667ae5854752c3a2dc07e321d5637/pypinfo/core.py#L65-L70", "method_name": "format_date", "code": "def format_date(date, timestamp_format):\n     try:\n         date = DATE_ADD.format(int(date))\n     except ValueError:\n         date = timestamp_format.format(date)\n     return date", "query": "format date"}
{"id": "https://github.com/JamesGardiner/chwrapper/blob/50f9cb2f5264c59505e8cc4e45ee6dc5d5669134/chwrapper/services/base.py#L97-L109", "method_name": "handle_http_error", "code": "def handle_http_error(\n         self, response, ignore=None, custom_messages=None, raise_for_status=True\n     ):\n         status = response.status_code\n         ignore = ignore or []\n         custom_messages = custom_messages or {}\n         if status in ignore or status in self._ignore_codes:\n             return None\n         elif response.status_code in custom_messages.keys():\n             raise requests.exceptions.HTTPError(custom_messages[response.status_code])\n         elif raise_for_status:\n             response.raise_for_status()", "query": "custom http error response"}
{"id": "https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/mit_stats.py#L48-L88", "method_name": "runningMedian", "code": "def runningMedian(seq, M):\n     seq = iter(seq)\n     s = []\n     m = M // 2 \n     s = [item for item in islice(seq,M)]\n     d = deque(s)\n     median = lambda : s[m] if bool(M&1) else (s[m-1]+s[m])*0.5\n     s.sort()\n     medians = [median()]\n     for item in seq:\n         old = d.popleft()          \n         d.append(item)             \n         del s[bisect_left(s, old)] \n         insort(s, item)            \n         medians.append(median())\n     return medians", "query": "deducting the median from each column"}
{"id": "https://github.com/abilian/abilian-core/blob/0a71275bf108c3d51e13ca9e093c0249235351e3/abilian/web/admin/panels/audit.py#L29-L33", "method_name": "format_date_for_input", "code": "def format_date_for_input(date):\n     date_fmt = get_locale().date_formats[\"short\"].pattern\n     date_fmt = date_fmt.replace(\"MMMM\", \"MM\").replace(\"MMM\", \"MM\")\n     return format_date(date, date_fmt)", "query": "format date"}
{"id": "https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/imgIO.py#L39-L73", "method_name": "imread", "code": "def imread(img, color=None, dtype=None): \n     COLOR2CV = {\"gray\": cv2.IMREAD_GRAYSCALE, \n                 \"all\": cv2.IMREAD_COLOR, \n                 None: cv2.IMREAD_ANYCOLOR \n                 } \n     c = COLOR2CV[color] \n     if callable(img): \n         img = img() \n     elif isinstance(img, string_types): \n         if dtype in (None, \"noUint\") or np.dtype(dtype) != np.uint8: \n             c  = cv2.IMREAD_ANYDEPTH \n         img2 = cv2.imread(img, c) \n         if img2 is None: \n             raise IOError(\"image \"%s\" is not existing\" % img) \n         img = img2 \n     elif color == \"gray\" and img.ndim == 3:  \n         img = toGray(img) \n     if dtype is not None: \n         if isinstance(img, np.ndarray): \n             img = _changeArrayDType(img, dtype, cutHigh=False) \n     return img", "query": "converting uint8 array to image"}
{"id": "https://github.com/django-extensions/django-extensions/blob/7e0bef97ea6cb7f9eea5e2528e3a985a83a7b9b8/django_extensions/management/commands/sqldsn.py#L100-L141", "method_name": "_postgresql", "code": "def _postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):  \n         dsn = []\n         if dsn_style is None or dsn_style == \"all\" or dsn_style == \"keyvalue\":\n             dsnstr = \"host=\"{0}\" dbname=\"{2}\" user=\"{3}\" password=\"{4}\"\"\n             if dbport is not None:\n                 dsnstr += \" port=\"{1}\"\"\n             dsn.append(dsnstr.format(dbhost,\n                                      dbport,\n                                      dbname,\n                                      dbuser,\n                                      dbpass,))\n         if dsn_style == \"all\" or dsn_style == \"kwargs\":\n             dsnstr = \"host=\"{0}\", database=\"{2}\", user=\"{3}\", password=\"{4}\"\"\n             if dbport is not None:\n                 dsnstr += \", port=\"{1}\"\"\n             dsn.append(dsnstr.format(dbhost,\n                                      dbport,\n                                      dbname,\n                                      dbuser,\n                                      dbpass))\n         if dsn_style == \"all\" or dsn_style == \"uri\":\n             dsnstr = \"postgresql://{user}:{password}@{host}/{name}\"\n             dsn.append(dsnstr.format(\n                 host=\"{host}:{port}\".format(host=dbhost, port=dbport) if dbport else dbhost,  \n                 name=dbname, user=dbuser, password=dbpass))\n         if dsn_style == \"all\" or dsn_style == \"pgpass\":\n             dsn.append(\":\".join(map(str, filter(\n                 None, [dbhost, dbport, dbname, dbuser, dbpass]))))\n         return dsn", "query": "postgresql connection"}
{"id": "https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/filter.py#L459-L463", "method_name": "params", "code": "def params(self):\n         if not any(filter.params for filter in self):\n             return None\n         else:\n             return Array(filter.params or Null() for filter in self)", "query": "filter array"}
{"id": "https://github.com/mar10/wsgidav/blob/cec0d84222fc24bea01be1cea91729001963f172/wsgidav/dav_error.py#L264-L274", "method_name": "get_http_status_string", "code": "def get_http_status_string(v):\n     code = get_http_status_code(v)\n     try:\n         return ERROR_DESCRIPTIONS[code]\n     except KeyError:\n         return \"{} Status\".format(code)", "query": "get the description of a http status code"}
{"id": "https://github.com/rjrequina/Cebuano-POS-Tagger/blob/fb1b46448c40b23ac8e8e373f073f1f8934b6dc6/eval/evaluator.py#L72-L92", "method_name": "confusion_matrix", "code": "def confusion_matrix(actual=[], pred=[]):\n     idx = {\n         \"ADJ\" : 0,\n         \"ADV\" : 1,\n         \"CONJ\": 2,\n         \"DET\" : 3,\n         \"NOUN\": 4,\n         \"NUM\" : 5,\n         \"OTH\" : 6,\n         \"PART\": 7,\n         \"PRON\": 8,\n         \"SYM\" : 9,\n         \"VERB\": 10\n     }\n     matrix = [[0 for i in range(11)] for j in range(11)]\n     for i in range(0, len(actual)):\n        matrix[idx[actual[i]]][idx[pred[i]]] += 1\n     return matrix", "query": "confusion matrix"}
{"id": "https://github.com/Unity-Technologies/ml-agents/blob/37d139af636e4a2351751fbf0f2fca5a9ed7457f/ml-agents/mlagents/trainers/barracuda.py#L228-L258", "method_name": "summary", "code": "def summary(model, print_layer_links, print_barracuda_json, print_tensors):\n     def array_without_brackets(arr):\n         return str(arr)[1:-1] \n     if print_layer_links:\n         for l in model.layers:\n             print(l.name, \" <= \", l.inputs)\n     if print_barracuda_json:\n         print(to_json(model))\n     if model.globals:\n         if isinstance(model.globals, dict):\n             model.globals = {x.name:x.shape for x in model.globals}\n         print(\"GLOBALS:\", array_without_brackets(model.globals))\n     for l in model.layers:\n         if isinstance(model.inputs, dict):\n             ins = {i:model.inputs[i] for i in l.inputs if i in model.inputs}\n         else:\n             ins = [i for i in l.inputs if i in model.inputs]\n         if ins:\n             print(\"IN: %s => \"%s\"\" % (array_without_brackets(ins), l.name))\n     for mem_in, mem_out in zip(model.memories[1::3], model.memories[2::3]):\n         print(\"MEM: \"%s\" => \"%s\"\" % (mem_in, mem_out))\n     print(\"OUT:\", array_without_brackets(model.outputs))\n     if (print_tensors):\n         for l in model.layers:\n             for x in l.tensors:\n                 print(x.name, x.shape, x.data.dtype, x.data)", "query": "print model summary"}
{"id": "https://github.com/ewdurbin/community/blob/f6b80e215a88508e3b07c2f4996ede6edea2c8e3/community/app.py#L36-L42", "method_name": "compute_status", "code": "def compute_status():\n     status_codes = []\n     for name, health_url in DEPENDENCIES.items():\n         status_codes.append(get_status(health_url)[0])\n     if max(status_codes) == 500:\n         return (\"UNHEALTHY\", max(status_codes))\n     return (\"HEALTHY\", max(status_codes))", "query": "get the description of a http status code"}
{"id": "https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/wstools/TimeoutSocket.py#L90-L93", "method_name": "recv", "code": "def recv(self, amt, flags=0):\n         if select.select([self.sock], [], [], self.timeout)[0]:\n             return self.sock.recv(amt, flags)\n         raise TimeoutError(\"socket recv() timeout.\")", "query": "socket recv timeout"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/adapters/acquirefielddefaults.py#L44-L57", "method_name": "__call__", "code": "def __call__(self, field):\n         acquire = getattr(field, \"acquire\", True)\n         if acquire:\n             fieldname = getattr(field, \"acquire_fieldname\", field.getName())\n             current = self.context\n             while hasattr(current, \"aq_parent\"):\n                 current = current.aq_parent\n                 if IPloneSiteRoot.providedBy(current):\n                     break\n                 if fieldname in current.Schema()._names:\n                     value = current.Schema()[fieldname].get(current)\n                     if value is not None:\n                         return value", "query": "get current observable value"}
{"id": "https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/url.py#L250-L272", "method_name": "url_parse_query", "code": "def url_parse_query(query, encoding=None):\n     if isinstance(query, unicode):\n         if encoding is None:\n             encoding = url_encoding\n         query = query.encode(encoding, \"ignore\")\n     append = \"\"\n     while \"?\" in query:\n         query, rest = query.rsplit(\"?\", 1)\n         append = \"?\"+url_parse_query(rest)+append\n     l = []\n     for k, v, sep in parse_qsl(query, keep_blank_values=True):\n         k = url_quote_part(k, \"/-:,;\")\n         if v:\n             v = url_quote_part(v, \"/-:,;\")\n             l.append(\"%s=%s%s\" % (k, v, sep))\n         elif v is None:\n             l.append(\"%s%s\" % (k, sep))\n         else:\n             l.append(\"%s=%s\" % (k, sep))\n     return \"\".join(l) + append", "query": "parse query string in url"}
{"id": "https://github.com/diffeo/yakonfig/blob/412e195da29b4f4fc7b72967c192714a6f5eaeb5/yakonfig/cmd.py#L132-L148", "method_name": "parseline", "code": "def parseline(self, line):\n         cmd, arg, line = Cmd.parseline(self, line)\n         if cmd and cmd.strip() != \"\":\n             dof = getattr(self, \"do_\" + cmd, None)\n             argf = getattr(self, \"args_\" + cmd, None)\n         else:\n             argf = None\n         if argf:\n             parser = argparse.ArgumentParser(\n                 prog=cmd,\n                 description=getattr(dof, \"__doc__\", None))\n             argf(parser)\n             try:\n                 arg = parser.parse_args(shlex.split(arg))\n             except SystemExit, e:\n                 return \"\", \"\", \"\"\n         return cmd, arg, line", "query": "parse command line argument"}
{"id": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfobjects/pdftext.py#L165-L172", "method_name": "_underline", "code": "def _underline(self): \n         up = self.font.underline_position \n         ut = self.font.underline_thickness \n         w = self.font._string_width(self.text) \n         s = \"%.2f %.2f %.2f %.2f re f\" % (self.cursor.x, \n             self.cursor.y_prime - up, w, ut) \n         return s", "query": "underline text in label widget"}
{"id": "https://github.com/caktus/django-timepiece/blob/52515dec027664890efbc535429e1ba1ee152f40/timepiece/entries/views.py#L44-L55", "method_name": "get_dates", "code": "def get_dates(self):\n         today = datetime.date.today()\n         day = today\n         if \"week_start\" in self.request.GET:\n             param = self.request.GET.get(\"week_start\")\n             try:\n                 day = datetime.datetime.strptime(param, \"%Y-%m-%d\").date()\n             except:\n                 pass\n         week_start = utils.get_week_start(day)\n         week_end = week_start + relativedelta(days=6)\n         return today, week_start, week_end", "query": "get current date"}
{"id": "https://github.com/LIVVkit/LIVVkit/blob/680120cd437e408673e62e535fc0a246c7fc17db/livvkit/components/numerics.py#L82-L87", "method_name": "_print_summary", "code": "def _print_summary(module, case, summary):\n     try:\n         module.print_summary(case, summary)\n     except (NotImplementedError, AttributeError):\n         print(\"    Ran \" + case + \"!\")\n         print(\"\")", "query": "print model summary"}
{"id": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/analyzation/make_observable.py#L13-L37", "method_name": "make_dict_observable", "code": "def make_dict_observable(matrix_observable):\n     dict_observable = {}\n     observable = np.array(matrix_observable)\n     observable_size = len(observable)\n     observable_bits = int(np.ceil(np.log2(observable_size)))\n     binary_formater = \"0{}b\".format(observable_bits)\n     if observable.ndim == 2:\n         observable = observable.diagonal()\n     for state_no in range(observable_size):\n         state_str = format(state_no, binary_formater)\n         dict_observable[state_str] = observable[state_no]\n     return dict_observable", "query": "get current observable value"}
{"id": "https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/distribution.py#L274-L319", "method_name": "anderson", "code": "def anderson(*args, dist=\"norm\"):\n     from scipy.stats import anderson as ads\n     k = len(args)\n     from_dist = np.zeros(k, \"bool\")\n     sig_level = np.zeros(k)\n     for j in range(k):\n         st, cr, sig = ads(args[j], dist=dist)\n         from_dist[j] = True if (st > cr).any() else False\n         sig_level[j] = sig[np.argmin(np.abs(st - cr))]\n     if k == 1:\n         from_dist = bool(from_dist)\n         sig_level = float(sig_level)\n     return from_dist, sig_level", "query": "binomial distribution"}
{"id": "https://github.com/monarch-initiative/dipper/blob/24cc80db355bbe15776edc5c7b41e0886959ba41/scripts/scrape-impc.py#L14-L40", "method_name": "parse_item", "code": "def parse_item(self, response):\n         key = None\n         if re.search(r\"parameterontologies\", response.url) and \\\n             response.css(\"html body div\n                          \".dark::text\").extract() is not None and \\\n             len(response.css(\"html body div\n                              \".dark::text\").extract()) > 0:\n             key = response.css(\"html body div\n                                \".dark::text\").extract()[0]\n         elif (re.search(r\"parameters\", response.url) or re.search(r\"protocol\", response.url)) and \\\n             response.css(\"html body div\n                          \".dark::text\").extract() is not None and \\\n             len(response.css(\"html body div\n                              \".dark::text\").extract()) > 0:\n             key = response.css(\"html body div\n                                \".dark::text\").extract()[0]\n         elif re.search(r\"procedures\", response.url) and \\\n             response.css(\"html body div\n                          \"::text\").extract() is not None and \\\n             len(response.css(\"html body div\n                              \"::text\").extract()) > 0:\n             key = response.css(\"html body div\n                                \"::text\").extract()[0]\n         if key is not None:\n             yield {\n                 key: response.url\n             }", "query": "extract data from html content"}
{"id": "https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/utils/dialog.py#L233-L234", "method_name": "get_checkbox_state_by_name", "code": "def get_checkbox_state_by_name(self, checkbox_text):\n        return [checkbox.get_active() for checkbox in self.checkboxes if checkbox.get_label() == checkbox_text]", "query": "check if a checkbox is checked"}
{"id": "https://github.com/ctuning/ck/blob/7e009814e975f8742790d3106340088a46223714/ck/kernel.py#L4351-L4377", "method_name": "get_current_date_time", "code": "def get_current_date_time(i):\n     import datetime\n     a={}\n     now1=datetime.datetime.now()\n     now=now1.timetuple()\n     a[\"date_year\"]=now[0]\n     a[\"date_month\"]=now[1]\n     a[\"date_day\"]=now[2]\n     a[\"time_hour\"]=now[3]\n     a[\"time_minute\"]=now[4]\n     a[\"time_second\"]=now[5]\n     return {\"return\":0, \"array\":a, \"iso_datetime\":now1.isoformat()}", "query": "get current date"}
{"id": "https://github.com/go-macaroon-bakery/py-macaroon-bakery/blob/63ce1ef1dabe816eb8aaec48fbb46761c34ddf77/macaroonbakery/bakery/_macaroon.py#L242-L248", "method_name": "deserialize_json", "code": "def deserialize_json(cls, serialized_json):\n         serialized = json.loads(serialized_json)\n         return Macaroon.from_dict(serialized)", "query": "deserialize json"}
{"id": "https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/system/time_helper.py#L78-L82", "method_name": "day_to_month", "code": "def day_to_month(timeperiod):\n     t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN)\n     return t.strftime(SYNERGY_MONTHLY_PATTERN)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L534-L574", "method_name": "copy_file", "code": "def copy_file(src, dst, ignore=None):\n    src = re.sub(\"[^\\w/\\-\\.\\*]\", \"\", src)\n    dst = re.sub(\"[^\\w/\\-\\.\\*]\", \"\", dst)\n    if len(re.sub(\"[\\W]\", \"\", src)) < 5 or len(re.sub(\"[\\W]\", \"\", dst)) < 5:\n       debug.log(\"Error: Copying file failed. Provided paths are invalid! src=\"%s\" dst=\"%s\"\"%(src, dst))\n    else:\n       check = False\n       if dst[-1] == \"/\":\n          if os.path.exists(dst):\n             check = True \n          else:\n             debug.log(\"Error: Copying file failed. Destination directory does not exist (%s)\"%(dst)) \n       elif os.path.exists(dst):\n          if os.path.isdir(dst):\n             check = True \n             dst += \"/\" \n          else:\n             debug.log(\"Error: Copying file failed. %s exists!\"%dst)\n       elif os.path.exists(os.path.dirname(dst)):\n          check = True \n       else:\n          debug.log(\"Error: Copying file failed. %s is an invalid distination!\"%dst)\n       if check:\n          files = glob.glob(src)\n          if ignore is not None: files = [fil for fil in files if not ignore in fil]\n          if len(files) != 0:\n             debug.log(\"Copying File(s)...\", \"Copy from %s\"%src, \"to %s\"%dst) \n             for file_ in files:\n                if os.path.isfile(file_):\n                   debug.log(\"Copying file: %s\"%file_) \n                   shutil.copy(file_, dst)\n                else:\n                   debug.log(\"Error: Copying file failed. %s is not a regular file!\"%file_) \n          else: debug.log(\"Error: Copying file failed. No files were found! (%s)\"%src) ", "query": "copying a file to a path"}
{"id": "https://github.com/jdp/urp/blob/778c16d9a5eae75316ce20aad742af7122be558c/urp.py#L17-L22", "method_name": "parse", "code": "def parse(args, data):\n     url = urlparse(data)\n     query = url.query\n     if not args.no_query_params:\n         query = parse_qsl(url.query)\n     return url, query", "query": "parse query string in url"}
{"id": "https://github.com/kennethreitz/requests-html/blob/b59a9f2fb9333d7d467154a0fd82978efdb9d23b/requests_html.py#L110-L111", "method_name": "html", "code": "def html(self, html: str) -> None:\n        self._html = html.encode(self.encoding)", "query": "html encode string"}
{"id": "https://github.com/facelessuser/backrefs/blob/3b3d60f5d57b02044f880aa29c9c5add0e31a34f/tools/unidatadownload.py#L32-L44", "method_name": "unzip_unicode", "code": "def unzip_unicode(output, version):\n     unzipper = zipfile.ZipFile(os.path.join(output, \"unicodedata\", \"%s.zip\" % version))\n     target = os.path.join(output, \"unicodedata\", version)\n     print(\"Unzipping %s.zip...\" % version)\n     os.makedirs(target)\n     for f in unzipper.namelist():\n         unzipper.extract(f, target)", "query": "unzipping large files"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/rnc_web.py#L408-L416", "method_name": "get_png_img_html", "code": "def get_png_img_html(blob: Union[bytes, memoryview],\n                      extra_html_class: str = None) -> str:\n     return .format(\n         \"class=\"{}\" \".format(extra_html_class) if extra_html_class else \"\",\n         get_png_data_url(blob)\n     )", "query": "get inner html"}
{"id": "https://github.com/mapbox/mapboxgl-jupyter/blob/f6e403c13eaa910e70659c7d179e8e32ce95ae34/mapboxgl/viz.py#L20-L37", "method_name": "generate_vector_color_map", "code": "def generate_vector_color_map(self):\n         vector_stops = []\n         if type(self.data) == str:\n             self.data = geojson_to_dict_list(self.data)\n         for row in self.data:\n             color = color_map(row[self.color_property], self.color_stops, self.color_default)\n             vector_stops.append([row[self.data_join_property], color])\n         return vector_stops", "query": "map to json"}
{"id": "https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/viz.py#L12-L44", "method_name": "plot_confusion_matrix", "code": "def plot_confusion_matrix(cm, classes,\n                           normalize=False,\n                           title=\"Confusion matrix\",\n                           cmap=plt.cm.Blues):\n     if normalize:\n         cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n         print(\"Normalized confusion matrix\")\n     else:\n         print(\"Confusion matrix, without normalization\")\n     print(cm)\n     plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n     plt.title(title)\n     plt.colorbar()\n     tick_marks = np.arange(len(classes))\n     plt.xticks(tick_marks, classes, rotation=45)\n     plt.yticks(tick_marks, classes)\n     fmt = \".2f\" if normalize else \"d\"\n     thresh = cm.max() / 2.\n     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n         plt.text(j, i, format(cm[i, j], fmt),\n                  horizontalalignment=\"center\",\n                  color=\"white\" if cm[i, j] > thresh else \"black\")\n     plt.tight_layout()\n     plt.ylabel(\"True label\")\n     plt.xlabel(\"Predicted label\")", "query": "confusion matrix"}
{"id": "https://github.com/phoopy/phoopy-console/blob/d38ec0eb952e79239699a0f855c07437a34024b0/phoopy/console/helper/string_helper.py#L41-L43", "method_name": "get_most_used_words", "code": "def get_most_used_words(words, stopwords):\n        valid_words = [word.lower() for word in words if StringHelper.is_valid_word(word, stopwords)]\n        return dict(Counter(valid_words).most_common(5))", "query": "determine a string is a valid word"}
{"id": "https://github.com/openfisca/openfisca-core/blob/92ce9396e29ae5d9bac5ea604cfce88517c6b35c/openfisca_core/variables.py#L427-L433", "method_name": "default_array", "code": "def default_array(self, array_size):\n         array = np.empty(array_size, dtype = self.dtype)\n         if self.value_type == Enum:\n             array.fill(self.default_value.index)\n             return EnumArray(array, self.possible_values)\n         array.fill(self.default_value)\n         return array", "query": "empty array"}
{"id": "https://github.com/dlancer/django-crispy-contact-form/blob/3d422556add5aea3607344a034779c214f84da04/contact_form/helpers.py#L8-L19", "method_name": "get_user_ip", "code": "def get_user_ip(request):\n     ip = get_real_ip(request)\n     if ip is None:\n         ip = get_ip(request)\n         if ip is None:\n             ip = \"127.0.0.1\"\n     return ip", "query": "get current ip address"}
{"id": "https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/stats.py#L114-L117", "method_name": "mad", "code": "def mad(v):\n     return np.median(np.abs(v - np.median(v)))", "query": "deducting the median from each column"}
{"id": "https://github.com/AshleySetter/optoanalysis/blob/9d390acc834d70024d47b574aea14189a5a5714e/optoanalysis/optoanalysis/optoanalysis.py#L3311-L3329", "method_name": "_GetRealImagArray", "code": "def _GetRealImagArray(Array):\n     ImagArray = _np.array([num.imag for num in Array])\n     RealArray = _np.array([num.real for num in Array])\n     return RealArray, ImagArray", "query": "initializing array"}
{"id": "https://github.com/wecatch/app-turbo/blob/75faf97371a9a138c53f92168d0a486636cb8a9c/turbo/session.py#L185-L193", "method_name": "_set_cookie", "code": "def _set_cookie(self, name, value):\n         cookie_domain = self._config.cookie_domain\n         cookie_path = self._config.cookie_path\n         cookie_expires = self._config.cookie_expires\n         if self._config.secure:\n             return self.handler.set_secure_cookie(\n                 name, value, expires_days=cookie_expires / (3600 * 24), domain=cookie_domain, path=cookie_path)\n         else:\n             return self.handler.set_cookie(name, value, expires=cookie_expires, domain=cookie_domain, path=cookie_path)", "query": "create cookie"}
{"id": "https://github.com/oplatek/csv2json/blob/f2f95db71ba2ce683fd6d0d3e2f13c9d0a77ceb6/csv2json/__init__.py#L20-L51", "method_name": "convert", "code": "def convert(csv, json, **kwargs):\n     csv_local, json_local = None, None\n     try:\n         if csv == \"-\" or csv is None:\n             csv = sys.stdin\n         elif isinstance(csv, str):\n             csv = csv_local = open(csv, \"r\")\n         if json == \"-\" or json is None:\n             json = sys.stdout\n         elif isinstance(json, str):\n             json = json_local = open(json, \"w\")\n         data = load_csv(csv, **kwargs)\n         save_json(data, json, **kwargs)\n     finally:\n         if csv_local is not None:\n             csv_local.close()\n         if json_local is not None:\n             json_local.close()", "query": "convert json to csv"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/rotate_poscar.py#L7-L13", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Rotates the cell lattice in VASP POSCAR files\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\" )\n     parser.add_argument( \"-a\", \"--axis\", nargs=3, type=float, help=\"vector for rotation axis\", required=True )\n     parser.add_argument( \"-d\", \"--degrees\", type=int, help=\"rotation angle in degrees\", required=True )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/googleapis/google-auth-library-python/blob/2c6ad78917e936f38f87c946209c8031166dc96e/google/auth/_helpers.py#L130-L173", "method_name": "update_query", "code": "def update_query(url, params, remove=None):\n     if remove is None:\n         remove = []\n     parts = urllib.parse.urlparse(url)\n     query_params = urllib.parse.parse_qs(parts.query)\n     query_params.update(params)\n     query_params = {\n         key: value for key, value\n         in six.iteritems(query_params)\n         if key not in remove}\n     new_query = urllib.parse.urlencode(query_params, doseq=True)\n     new_parts = parts._replace(query=new_query)\n     return urllib.parse.urlunparse(new_parts)", "query": "parse query string in url"}
{"id": "https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/util/epmgp.py#L122-L208", "method_name": "min_faktor", "code": "def min_faktor(Mu, Sigma, k, gamma=1):\n     D = Mu.shape[0]\n     logS = np.zeros((D - 1,))\n     MP = np.zeros((D - 1,))\n     P = np.zeros((D - 1,))\n     M = np.copy(Mu)\n     V = np.copy(Sigma)\n     b = False\n     d = np.NaN\n     for count in range(50):\n         diff = 0\n         for i in range(D - 1):\n             l = i if  i < k else i + 1\n             try:\n                 M, V, P[i], MP[i], logS[i], d = lt_factor(k, l, M, V,\n                                                         MP[i], P[i], gamma)\n             except Exception as e:\n                 raise\n             if np.isnan(d):\n                 break\n             diff += np.abs(d)\n         if np.isnan(d):\n             break\n         if np.abs(diff) < 0.001:\n             b = True\n             break\n     if np.isnan(d):\n         logZ = -np.Infinity\n         yield logZ\n         dlogZdMu = np.zeros((D, 1))\n         yield dlogZdMu\n         dlogZdMudMu = np.zeros((D, D))\n         yield dlogZdMudMu\n         dlogZdSigma = np.zeros((int(0.5 * (D * (D + 1))), 1))\n         yield dlogZdSigma\n         mvmin = [Mu[k], Sigma[k, k]]\n         yield mvmin\n     else:\n         C = np.eye(D) / sq2\n         C[k, :] = -1 / sq2\n         C = np.delete(C, k, 1)\n         R = np.sqrt(P.T) * C\n         r = np.sum(MP.T * C, 1)\n         mp_not_zero = np.where(MP != 0)\n         mpm = MP[mp_not_zero] * MP[mp_not_zero] / P[mp_not_zero]\n         mpm = sum(mpm)\n         s = sum(logS)\n         IRSR = (np.eye(D - 1) + np.dot(np.dot(R.T, Sigma), R))\n         rSr = np.dot(np.dot(r.T, Sigma), r)\n         A = np.dot(R, np.linalg.solve(IRSR, R.T))\n         A = 0.5 * (A.T + A)  \n         b = (Mu + np.dot(Sigma, r))\n         Ab = np.dot(A, b)\n         try:\n             cIRSR = np.linalg.cholesky(IRSR)\n         except np.linalg.LinAlgError:\n             try:\n                 cIRSR = np.linalg.cholesky(IRSR + 1e-10 * np.eye(IRSR.shape[0]))\n             except np.linalg.LinAlgError:\n                 cIRSR = np.linalg.cholesky(IRSR + 1e-6 * np.eye(IRSR.shape[0]))\n         dts = 2 * np.sum(np.log(np.diagonal(cIRSR)))\n         logZ = 0.5 * (rSr - np.dot(b.T, Ab) - dts) + np.dot(Mu.T, r) + s - 0.5 * mpm\n         yield logZ\n         btA = np.dot(b.T, A)\n         dlogZdMu = r - Ab\n         yield dlogZdMu\n         dlogZdMudMu = -A\n         yield dlogZdMudMu\n         dlogZdSigma = -A - 2 * np.outer(r, Ab.T) + np.outer(r, r.T)\\\n                     + np.outer(btA.T, Ab.T)\n         dlogZdSigma2 = np.zeros_like(dlogZdSigma)\n         np.fill_diagonal(dlogZdSigma2, np.diagonal(dlogZdSigma))\n         dlogZdSigma = 0.5 * (dlogZdSigma + dlogZdSigma.T - dlogZdSigma2)\n         dlogZdSigma = np.rot90(dlogZdSigma, k=2)[np.triu_indices(D)][::-1]\n         yield dlogZdSigma", "query": "nelder mead optimize"}
{"id": "https://github.com/VIVelev/PyDojoML/blob/773fdce6866aa6decd306a5a85f94129fed816eb/dojo/cluster/hierarchical.py#L43-L62", "method_name": "cluster", "code": "def cluster(self, X):\n         X = super().cluster(X)\n         self._distances = linkage(X, method=self.linkage)\n         if self.mode == \"n_clusters\":\n             return fcluster(\n                 self._distances,\n                 self.n_clusters,\n                 criterion=\"maxclust\"\n             )\n         elif self.mode == \"max_distance\":\n             return fcluster(\n                 self._distances,\n                 self.max_distance,\n                 criterion=\"distance\"\n             )\n         else:\n             raise ParameterError(f\"Unknown / unsupported clustering mode: \\\"{self.mode}\\\"\")", "query": "k means clustering"}
{"id": "https://github.com/ChrisCummins/labm8/blob/dd10d67a757aefb180cb508f86696f99440c94f5/text.py#L137-L167", "method_name": "diff", "code": "def diff(s1, s2):\n     return levenshtein(s1, s2) / max(len(s1), len(s2))", "query": "string similarity levenshtein"}
{"id": "https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/utils/hparam.py#L558-L572", "method_name": "parse_json", "code": "def parse_json(self, values_json):\n     values_map = json.loads(values_json)\n     return self.override_from_dict(values_map)", "query": "map to json"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/rotate_poscar.py#L7-L13", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Rotates the cell lattice in VASP POSCAR files\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\" )\n     parser.add_argument( \"-a\", \"--axis\", nargs=3, type=float, help=\"vector for rotation axis\", required=True )\n     parser.add_argument( \"-d\", \"--degrees\", type=int, help=\"rotation angle in degrees\", required=True )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/ndarray/sparse.py#L1507-L1543", "method_name": "zeros", "code": "def zeros(stype, shape, ctx=None, dtype=None, **kwargs):\n     if stype == \"default\":\n         return _zeros_ndarray(shape, ctx=ctx, dtype=dtype, **kwargs)\n     if ctx is None:\n         ctx = current_context()\n     dtype = mx_real_t if dtype is None else dtype\n     if stype in (\"row_sparse\", \"csr\"):\n         aux_types = _STORAGE_AUX_TYPES[stype]\n     else:\n         raise ValueError(\"unknown storage type\" + stype)\n     out = _ndarray_cls(_new_alloc_handle(stype, shape, ctx, True, dtype, aux_types))\n     return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, out=out, **kwargs)", "query": "initializing array"}
{"id": "https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/api/model.py#L37-L51", "method_name": "summary", "code": "def summary(self, input_size=None, hashsummary=False):\n         if input_size is None:\n             print(self)\n             print(\"-\" * 120)\n             number = sum(p.numel() for p in self.model.parameters())\n             print(\"Number of model parameters: {:,}\".format(number))\n             print(\"-\" * 120)\n         else:\n             summary(self, input_size)\n         if hashsummary:\n             for idx, hashvalue in enumerate(self.hashsummary()):\n                 print(f\"{idx}: {hashvalue}\")", "query": "print model summary"}
{"id": "https://github.com/ml31415/numpy-groupies/blob/0911e9c59b14e11319e82d0876056ad2a17e6568/numpy_groupies/aggregate_numpy.py#L181-L185", "method_name": "_sort", "code": "def _sort(group_idx, a, size=None, fill_value=None, dtype=None, reverse=False):\n     sortidx = np.lexsort((-a if reverse else a, group_idx))\n     revidx = np.argsort(np.argsort(group_idx, kind=\"mergesort\"), kind=\"mergesort\")\n     return a[sortidx][revidx]", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/UCL-INGI/INGInious/blob/cbda9a9c7f2b8e8eb1e6d7d51f0d18092086300c/inginious/frontend/cookieless_app.py#L231-L237", "method_name": "_setcookie", "code": "def _setcookie(self, session_id, expires=\"\", **kw):\n         cookie_name = self._config.cookie_name\n         cookie_domain = self._config.cookie_domain\n         cookie_path = self._config.cookie_path\n         httponly = self._config.httponly\n         secure = self._config.secure\n         web.setcookie(cookie_name, session_id, expires=expires, domain=cookie_domain, httponly=httponly, secure=secure, path=cookie_path)", "query": "create cookie"}
{"id": "https://github.com/tisimst/mcerp/blob/2bb8260c9ad2d58a806847f1b627b6451e407de1/mcerp/__init__.py#L1150-L1167", "method_name": "Binomial", "code": "def Binomial(n, p, tag=None):\n     assert (\n         int(n) == n and n > 0\n     ), \"Binomial number of trials \"n\" must be an integer greater than zero\"\n     assert (\n         0 < p < 1\n     ), \"Binomial probability \"p\" must be between zero and one, non-inclusive\"\n     return uv(ss.binom(n, p), tag=tag)", "query": "binomial distribution"}
{"id": "https://github.com/Neurita/boyle/blob/2dae7199849395a209c887d5f30506e1de8a9ad9/scripts/filetree.py#L20-L72", "method_name": "copy", "code": "def copy(configfile=\"\", destpath=\"\", overwrite=False, sub_node=\"\"):\n     log.info(\"Running {0} {1} {2}\".format(os.path.basename(__file__),\n                                           whoami(),\n                                           locals()))\n     assert(os.path.isfile(configfile))\n     if os.path.exists(destpath):\n         if os.listdir(destpath):\n             raise FolderAlreadyExists(\"Folder {0} already exists. Please clean \"\n                                       \"it or change destpath.\".format(destpath))\n     else:\n         log.info(\"Creating folder {0}\".format(destpath))\n         path(destpath).makedirs_p()\n     from boyle.files.file_tree_map import FileTreeMap\n     file_map = FileTreeMap()\n     try:\n         file_map.from_config_file(configfile)\n     except Exception as e:\n         raise FileTreeMapError(str(e))\n     if sub_node:\n         sub_map = file_map.get_node(sub_node)\n         if not sub_map:\n             raise FileTreeMapError(\"Could not find sub node \"\n                                    \"{0}\".format(sub_node))\n         file_map._filetree = {}\n         file_map._filetree[sub_node] = sub_map\n     try:\n         file_map.copy_to(destpath, overwrite=overwrite)\n     except Exception as e:\n         raise FileTreeMapError(str(e))", "query": "copying a file to a path"}
{"id": "https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/vendored/potion_client/links.py#L73-L90", "method_name": "raise_for_status", "code": "def raise_for_status(self, response):\n         http_error_msg = \"\"\n         if 400 <= response.status_code < 500:\n             try:\n                 http_error_msg = response.json()\n             except:\n                 http_error_msg = (\"{code} Client Error: {reason} for url: {url}\".format(\n                     code=response.status_code, reason=response.reason, url=response.url)\n                 )\n         elif 500 <= response.status_code < 600:\n             http_error_msg = (\"{code} Server Error: {reason} for url: {url}\".format(\n                 code=response.status_code, reason=response.reason, url=response.url)\n             )\n         if http_error_msg:\n             raise HTTPError(http_error_msg, response=response)", "query": "custom http error response"}
{"id": "https://github.com/cltk/cltk/blob/ed9c025b7ec43c949481173251b70e05e4dffd27/cltk/ir/query.py#L24-L37", "method_name": "_regex_span", "code": "def _regex_span(_regex, _str, case_insensitive=True):\n     if case_insensitive:\n         flags = regex.IGNORECASE   regex.FULLCASE   regex.VERSION1\n     else:\n         flags = regex.VERSION1\n     comp = regex.compile(_regex, flags=flags)\n     matches = comp.finditer(_str)\n     for match in matches:\n         yield match", "query": "regex case insensitive"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568", "method_name": "assert_checked_checkbox", "code": "def assert_checked_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     assert check_box.is_selected(), \"Check box should be selected.\"", "query": "make the checkbox checked"}
{"id": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/idsortingalgorithm.py#L79-L109", "method_name": "sort", "code": "def sort(self, ids):\n         def extract_int(string):\n             return int(re.sub(r\"[^0-9]\", \"\", string))\n         tmp = list(ids)\n         if self.algorithm == IDSortingAlgorithm.UNSORTED:\n             self.log(u\"Sorting using UNSORTED\")\n         elif self.algorithm == IDSortingAlgorithm.LEXICOGRAPHIC:\n             self.log(u\"Sorting using LEXICOGRAPHIC\")\n             tmp = sorted(ids)\n         elif self.algorithm == IDSortingAlgorithm.NUMERIC:\n             self.log(u\"Sorting using NUMERIC\")\n             tmp = ids\n             try:\n                 tmp = sorted(tmp, key=extract_int)\n             except (ValueError, TypeError) as exc:\n                 self.log_exc(u\"Not all id values contain a numeric part. Returning the id list unchanged.\", exc, False, None)\n         return tmp", "query": "sort string list"}
{"id": "https://github.com/chrisjsewell/jsonextended/blob/c3a7a880cc09789b3c61204265dcbb127be76c8a/jsonextended/mockpath.py#L44-L54", "method_name": "readline", "code": "def readline(self):\n         if self._current_line >= len(self._linelist):\n             line = \"\"\n         else:\n             line = self._linelist[self._current_line] + \"\n\"\n         self._current_line += 1\n         self._current_indx += len(\n             \"\n\".join(self._linelist[0:self._current_line]))\n         if self._encoding is not None:\n             line = line.encode(self._encoding)\n         return line", "query": "read text file line by line"}
{"id": "https://github.com/crytic/slither/blob/04c147f7e50223c6af458ca430befae747ccd259/slither/core/declarations/contract.py#L431-L439", "method_name": "get_enum_from_name", "code": "def get_enum_from_name(self, enum_name):\n         return next((e for e in self.enums if e.name == enum_name), None)", "query": "get name of enumerated value"}
{"id": "https://github.com/jepegit/cellpy/blob/9f4a84cdd11f72cfa02cda8c2d7b5174abbb7370/cellpy/readers/core.py#L261-L266", "method_name": "no_data", "code": "def no_data(self):\n         try:\n             empty = self.dfdata.empty\n         except AttributeError:\n             empty = True\n         return empty", "query": "empty array"}
{"id": "https://github.com/Lilykos/pyphonetics/blob/7f55cccc1135e6015520a895eb6859318a4b6111/pyphonetics/distance_metrics/levenshtein.py#L1-L29", "method_name": "levenshtein_distance", "code": "def levenshtein_distance(word1, word2):\n     if len(word1) < len(word2):\n         return levenshtein_distance(word2, word1)\n     if len(word2) == 0:\n         return len(word1)\n     previous_row = list(range(len(word2) + 1))\n     for i, char1 in enumerate(word1):\n         current_row = [i + 1]\n         for j, char2 in enumerate(word2):\n             insertions = previous_row[j + 1] + 1\n             deletions = current_row[j] + 1\n             substitutions = previous_row[j] + (char1 != char2)\n             current_row.append(min(insertions, deletions, substitutions))\n         previous_row = current_row\n     return previous_row[-1]", "query": "string similarity levenshtein"}
{"id": "https://github.com/RaRe-Technologies/smart_open/blob/2dc8d60f223fc7b00a2000c56362a7bd6cd0850e/smart_open/http.py#L104-L134", "method_name": "read", "code": "def read(self, size=-1):\n         logger.debug(\"reading with size: %d\", size)\n         if self.response is None:\n             return b\"\"\n         if size == 0:\n             return b\"\"\n         elif size < 0 and len(self._read_buffer) == 0:\n             retval = self.response.raw.read()\n         elif size < 0:\n             retval = self._read_buffer.read() + self.response.raw.read()\n         else:\n             while len(self._read_buffer) < size:\n                 logger.debug(\"http reading more content at current_pos: %d with size: %d\", self._current_pos, size)\n                 bytes_read = self._read_buffer.fill(self._read_iter)\n                 if bytes_read == 0:\n                     retval = self._read_buffer.read()\n                     self._current_pos += len(retval)\n                     return retval\n             retval = self._read_buffer.read(size)\n         self._current_pos += len(retval)\n         return retval", "query": "buffered file reader read text"}
{"id": "https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/mover.py#L203-L208", "method_name": "get_ip", "code": "def get_ip(self):\n         if self._ip is not None:\n             ret = self._ip\n         else:\n             ret = self.ip_addr\n         return ret", "query": "get current ip address"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/components/rom.py#L101-L134", "method_name": "extract_zip", "code": "def extract_zip(self):\n         assert self.FILE_COUNT>0\n         try:\n             with zipfile.ZipFile(self.archive_path, \"r\") as zip:\n                 namelist = zip.namelist()\n                 print(\"namelist():\", namelist)\n                 if len(namelist) != self.FILE_COUNT:\n                     msg = (\n                         \"Wrong archive content?!?\"\n                         \" There exists %i files, but it should exist %i.\"\n                         \"Existing names are: %r\"\n                     ) % (len(namelist), self.FILE_COUNT, namelist)\n                     log.error(msg)\n                     raise RuntimeError(msg)\n                 for filename in namelist:\n                     content = zip.read(filename)\n                     dst = self.file_rename(filename)\n                     out_filename=os.path.join(self.ROM_PATH, dst)\n                     with open(out_filename, \"wb\") as f:\n                         f.write(content)\n                     if dst == filename:\n                         print(\"%r extracted\" % out_filename)\n                     else:\n                         print(\"%r extracted to %r\" % (filename, out_filename))\n                     self.post_processing(out_filename)\n         except BadZipFile as err:\n             msg = \"Error extracting archive %r: %s\" % (self.archive_path, err)\n             log.error(msg)\n             raise BadZipFile(msg)", "query": "extract zip file recursively"}
{"id": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_decor.py#L601-L651", "method_name": "memoize", "code": "def memoize(func):\n     cache = func._util_decor_memoize_cache = {}\n     def memoizer(*args, **kwargs):\n         key = str(args) + str(kwargs)\n         if key not in cache:\n             cache[key] = func(*args, **kwargs)\n         return cache[key]\n     memoizer = preserve_sig(memoizer, func)\n     memoizer.cache = cache\n     return memoizer", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/wrappers/abstract_wrapper.py#L161-L176", "method_name": "_search_for_executable", "code": "def _search_for_executable(self, executable):\n         if os.path.isfile(executable):\n             return os.path.abspath(executable)\n         else:\n             envpath = os.getenv(\"PATH\")\n             if envpath is None:\n                 return\n             for path in envpath.split(os.pathsep):\n                 exe = os.path.join(path, executable)\n                 if os.path.isfile(exe):\n                     return os.path.abspath(exe)", "query": "get executable path"}
{"id": "https://github.com/edublancas/sklearn-evaluation/blob/79ee6e4dfe911b5a5a9b78a5caaed7c73eef6f39/sklearn_evaluation/evaluator.py#L85-L89", "method_name": "confusion_matrix", "code": "def confusion_matrix(self):\n         return plot.confusion_matrix(self.y_true, self.y_pred,\n                                      self.target_names, ax=_gen_ax())", "query": "confusion matrix"}
{"id": "https://github.com/thebjorn/pydeps/blob/1e6715b7bea47a40e8042821b57937deaaa0fdc3/pydeps/arguments.py#L20-L31", "method_name": "boolval", "code": "def boolval(v): \n     if isinstance(v, bool): \n         return v \n     if isinstance(v, int): \n         return bool(v) \n     if is_string(v): \n         v = v.lower() \n         if v in {\"j\", \"y\", \"ja\", \"yes\", \"1\", \"true\"}: \n             return True \n         if v in {\"n\", \"nei\", \"no\", \"0\", \"false\"}: \n             return False \n     raise ValueError(\"Don\"t know how to convert %r to bool\" % v)", "query": "convert int to bool"}
{"id": "https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/db/__init__.py#L56-L62", "method_name": "execute", "code": "def execute(self, sql, args=()):\n         if isinstance(sql, (list, tuple)):\n             sql = \" \".join(sql)\n         with sqlite3.connect(self.path) as con:\n             return con.execute(sql, args)", "query": "connect to sql"}
{"id": "https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/collection.py#L74-L79", "method_name": "extract_impression_from_file", "code": "def extract_impression_from_file(file_path):\n     with open(file_path, \"r\", encoding=\"utf8\") as f:\n         text = f.read()\n     return extract_impression_from_text(text)", "query": "extracting data from a text file"}
{"id": "https://github.com/aouyar/PyMunin/blob/4f58a64b6b37c85a84cc7e1e07aafaa0321b249d/pysysinfo/mysql.py#L64-L69", "method_name": "_connect", "code": "def _connect(self):\n         if self._connParams:\n             self._conn = MySQLdb.connect(**self._connParams)\n         else:\n             self._conn = MySQLdb.connect(\"\")", "query": "connect to sql"}
{"id": "https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/vendored/potion_client/links.py#L73-L90", "method_name": "raise_for_status", "code": "def raise_for_status(self, response):\n         http_error_msg = \"\"\n         if 400 <= response.status_code < 500:\n             try:\n                 http_error_msg = response.json()\n             except:\n                 http_error_msg = (\"{code} Client Error: {reason} for url: {url}\".format(\n                     code=response.status_code, reason=response.reason, url=response.url)\n                 )\n         elif 500 <= response.status_code < 600:\n             http_error_msg = (\"{code} Server Error: {reason} for url: {url}\".format(\n                 code=response.status_code, reason=response.reason, url=response.url)\n             )\n         if http_error_msg:\n             raise HTTPError(http_error_msg, response=response)", "query": "custom http error response"}
{"id": "https://github.com/simonw/datasette/blob/11b352b4d52fd02a422776edebb14f12e4994d3b/datasette/app.py#L521-L527", "method_name": "table_metadata", "code": "def table_metadata(self, database, table):\n         \"Fetch table-specific metadata.\"\n         return (self.metadata(\"databases\") or {}).get(database, {}).get(\n             \"tables\", {}\n         ).get(\n             table, {}\n         )", "query": "get database table name"}
{"id": "https://github.com/oplatek/csv2json/blob/f2f95db71ba2ce683fd6d0d3e2f13c9d0a77ceb6/csv2json/__init__.py#L20-L51", "method_name": "convert", "code": "def convert(csv, json, **kwargs):\n     csv_local, json_local = None, None\n     try:\n         if csv == \"-\" or csv is None:\n             csv = sys.stdin\n         elif isinstance(csv, str):\n             csv = csv_local = open(csv, \"r\")\n         if json == \"-\" or json is None:\n             json = sys.stdout\n         elif isinstance(json, str):\n             json = json_local = open(json, \"w\")\n         data = load_csv(csv, **kwargs)\n         save_json(data, json, **kwargs)\n     finally:\n         if csv_local is not None:\n             csv_local.close()\n         if json_local is not None:\n             json_local.close()", "query": "convert json to csv"}
{"id": "https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_to_csv.py#L28-L47", "method_name": "json_to_csv", "code": "def json_to_csv(json_input):\n     try:\n         json_input = json.loads(json_input)\n     except:\n         pass \n     headers = set()\n     for json_row in json_input:\n         headers.update(json_row.keys())\n     csv_io = StringIO.StringIO()\n     csv_out = csv.DictWriter(csv_io,headers)\n     csv_out.writeheader()\n     for json_row in json_input:\n         csv_out.writerow(json_row)\n     csv_io.seek(0)\n     return csv_io.read()", "query": "convert json to csv"}
{"id": "https://github.com/maxpowel/scrapium/blob/bc12c425aa5978f953a87d05920ba0f61a00409c/scrapium/scrapium.py#L140-L142", "method_name": "get_html", "code": "def get_html(self, url):\n        r = self.get(url)\n        return self.html(r.text)", "query": "get inner html"}
{"id": "https://github.com/thewca/wca-regulations-compiler/blob/3ebbd8fe8fec7c9167296f59b2677696fe61a954/wrc/wrc.py#L85-L113", "method_name": "html_to_pdf", "code": "def html_to_pdf(tmp_filenames, output_directory, lang_options):\n     input_html = output_directory + \"/\" + tmp_filenames[0]\n     wkthml_cmd = [\"wkhtmltopdf\"]\n     wkthml_cmd.extend([\"--margin-left\", \"18\"])\n     wkthml_cmd.extend([\"--margin-right\", \"18\"])\n     wkthml_cmd.extend([\"--page-size\", \"Letter\"])\n     header_file = pkg_resources.resource_filename(\"wrc\", \"data/header.html\")\n     footer_file = pkg_resources.resource_filename(\"wrc\", \"data/footer.html\")\n     wkthml_cmd.extend([\"--header-html\", header_file])\n     wkthml_cmd.extend([\"--footer-html\", footer_file])\n     wkthml_cmd.extend([\"--header-spacing\", \"8\"])\n     wkthml_cmd.extend([\"--footer-spacing\", \"8\"])\n     wkthml_cmd.append(input_html)\n     wkthml_cmd.append(output_directory + \"/\" + lang_options[\"pdf\"] + \".pdf\")\n     try:\n         check_call(wkthml_cmd)\n         print \"Successfully generated pdf file!\"\n         print \"Cleaning temporary file (%s)...\" % input_html\n         os.remove(input_html)\n     except CalledProcessError as err:\n         print \"Error while generating pdf:\"\n         print err\n         sys.exit(1)\n     except OSError as err:\n         print \"Error when running command \\\"\" + \" \".join(wkthml_cmd) + \"\\\"\"\n         print err\n         sys.exit(1)", "query": "convert html to pdf"}
{"id": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/syncmap/__init__.py#L309-L368", "method_name": "output_html_for_tuning", "code": "def output_html_for_tuning(\n             self,\n             audio_file_path,\n             output_file_path,\n             parameters=None\n     ):\n         if not gf.file_can_be_written(output_file_path):\n             self.log_exc(u\"Cannot output HTML file \"%s\". Wrong permissions?\" % (output_file_path), None, True, OSError)\n         if parameters is None:\n             parameters = {}\n         audio_file_path_absolute = gf.fix_slash(os.path.abspath(audio_file_path))\n         template_path_absolute = gf.absolute_path(self.FINETUNEAS_PATH, __file__)\n         with io.open(template_path_absolute, \"r\", encoding=\"utf-8\") as file_obj:\n             template = file_obj.read()\n         for repl in self.FINETUNEAS_REPLACEMENTS:\n             template = template.replace(repl[0], repl[1])\n         template = template.replace(\n             self.FINETUNEAS_REPLACE_AUDIOFILEPATH,\n             u\"audioFilePath = \\\"file://%s\\\";\" % audio_file_path_absolute\n         )\n         template = template.replace(\n             self.FINETUNEAS_REPLACE_FRAGMENTS,\n             u\"fragments = (%s).fragments;\" % self.json_string\n         )\n         if gc.PPN_TASK_OS_FILE_FORMAT in parameters:\n             output_format = parameters[gc.PPN_TASK_OS_FILE_FORMAT]\n             if output_format in self.FINETUNEAS_ALLOWED_FORMATS:\n                 template = template.replace(\n                     self.FINETUNEAS_REPLACE_OUTPUT_FORMAT,\n                     u\"outputFormat = \\\"%s\\\";\" % output_format\n                 )\n                 if output_format == \"smil\":\n                     for key, placeholder, replacement in [\n                             (\n                                 gc.PPN_TASK_OS_FILE_SMIL_AUDIO_REF,\n                                 self.FINETUNEAS_REPLACE_SMIL_AUDIOREF,\n                                 \"audioref = \\\"%s\\\";\"\n                             ),\n                             (\n                                 gc.PPN_TASK_OS_FILE_SMIL_PAGE_REF,\n                                 self.FINETUNEAS_REPLACE_SMIL_PAGEREF,\n                                 \"pageref = \\\"%s\\\";\"\n                             ),\n                     ]:\n                         if key in parameters:\n                             template = template.replace(\n                                 placeholder,\n                                 replacement % parameters[key]\n                             )\n         with io.open(output_file_path, \"w\", encoding=\"utf-8\") as file_obj:\n             file_obj.write(template)", "query": "output to html file"}
{"id": "https://github.com/toejough/pimento/blob/cdb00a93976733aa5521f8504152cedeedfc711a/pimento/__init__.py#L298-L311", "method_name": "_exact_match", "code": "def _exact_match(response, matches, insensitive, fuzzy):\n     for match in matches:\n         if response == match:\n             return match\n         elif insensitive and response.lower() == match.lower():\n             return match\n         elif fuzzy and _exact_fuzzy_match(response, match, insensitive):\n             return match\n     else:\n         return None", "query": "fuzzy match ranking"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/components/periphery.py#L163-L168", "method_name": "copy_to_clipboard", "code": "def copy_to_clipboard(self, event):\n         log.critical(\"Copy to clipboard\")\n         text = self.text.get(\"1.0\", tkinter.END)\n         print(text)\n         self.root.clipboard_clear()\n         self.root.clipboard_append(text)", "query": "copy to clipboard"}
{"id": "https://github.com/Guake/guake/blob/4153ef38f9044cbed6494075fce80acd5809df2b/guake/terminal.py#L136-L141", "method_name": "copy_clipboard", "code": "def copy_clipboard(self):\n         if self.get_has_selection():\n             super(GuakeTerminal, self).copy_clipboard()\n         elif self.matched_value:\n             guake_clipboard = Gtk.Clipboard.get_default(self.guake.window.get_display())\n             guake_clipboard.set_text(self.matched_value, len(self.matched_value))", "query": "copy to clipboard"}
{"id": "https://github.com/fishtown-analytics/dbt/blob/aa4f771df28b307af0cf9fe2fc24432f10a8236b/core/dbt/graph/selector.py#L317-L344", "method_name": "get_ancestor_ephemeral_nodes", "code": "def get_ancestor_ephemeral_nodes(self, selected_nodes):\n         node_names = {}\n         for node_id in selected_nodes:\n             if node_id not in self.manifest.nodes:\n                 continue\n             node = self.manifest.nodes[node_id]\n             if node.resource_type == NodeType.Source:\n                 continue\n             node_names[node_id] = node.name\n         include_spec = [\n             \"+{}\".format(node_names[node])\n             for node in selected_nodes if node in node_names\n         ]\n         if not include_spec:\n             return set()\n         all_ancestors = self.select_nodes(self.linker.graph, include_spec, [])\n         res = []\n         for ancestor in all_ancestors:\n             ancestor_node = self.manifest.nodes.get(ancestor, None)\n             if ancestor_node and self.is_ephemeral_model(ancestor_node):\n                 res.append(ancestor)\n         return set(res)", "query": "get all parents of xml node"}
{"id": "https://github.com/tjguk/winshell/blob/1509d211ab3403dd1cff6113e4e13462d6dec35b/winshell.py#L266-L294", "method_name": "copy_file", "code": "def copy_file(\n     source_path,\n     target_path,\n     allow_undo=True,\n     no_confirm=False,\n     rename_on_collision=True,\n     silent=False,\n     extra_flags=0,\n     hWnd=None\n ):\n     return _file_operation(\n         shellcon.FO_COPY,\n         source_path,\n         target_path,\n         allow_undo,\n         no_confirm,\n         rename_on_collision,\n         silent,\n         extra_flags,\n         hWnd\n     )", "query": "copying a file to a path"}
{"id": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/io/sas/sas_xport.py#L427-L465", "method_name": "read", "code": "def read(self, nrows=None):\n         if nrows is None:\n             nrows = self.nobs\n         read_lines = min(nrows, self.nobs - self._lines_read)\n         read_len = read_lines * self.record_length\n         if read_len <= 0:\n             self.close()\n             raise StopIteration\n         raw = self.filepath_or_buffer.read(read_len)\n         data = np.frombuffer(raw, dtype=self._dtype, count=read_lines)\n         df = pd.DataFrame(index=range(read_lines))\n         for j, x in enumerate(self.columns):\n             vec = data[\"s%d\" % j]\n             ntype = self.fields[j][\"ntype\"]\n             if ntype == \"numeric\":\n                 vec = _handle_truncated_float_vec(\n                     vec, self.fields[j][\"field_length\"])\n                 miss = self._missing_double(vec)\n                 v = _parse_float_vec(vec)\n                 v[miss] = np.nan\n             elif self.fields[j][\"ntype\"] == \"char\":\n                 v = [y.rstrip() for y in vec]\n                 if self._encoding is not None:\n                     v = [y.decode(self._encoding) for y in v]\n             df[x] = v\n         if self._index is None:\n             df.index = range(self._lines_read, self._lines_read + read_lines)\n         else:\n             df = df.set_index(self._index)\n         self._lines_read += read_lines\n         return df", "query": "readonly array"}
{"id": "https://github.com/crytic/slither/blob/04c147f7e50223c6af458ca430befae747ccd259/slither/core/declarations/contract.py#L431-L439", "method_name": "get_enum_from_name", "code": "def get_enum_from_name(self, enum_name):\n         return next((e for e in self.enums if e.name == enum_name), None)", "query": "get name of enumerated value"}
{"id": "https://github.com/chitamoor/Rester/blob/1865b17f70b7c597aeadde2d0907cb1b59f10c0f/rester/apirunner.py#L9-L18", "method_name": "parse_cmdln_args", "code": "def parse_cmdln_args():\n     parser = argparse.ArgumentParser(description=\"Process command line args\")\n     parser.add_argument(\"--log\", help=\"log help\", default=\"INFO\")\n     parser.add_argument(\n         \"--tc\", help=\"tc help\")\n     parser.add_argument(\n         \"--ts\", help=\"ts help\")\n     args = parser.parse_args()\n     return (args.log.upper(), args.tc, args.ts)", "query": "parse command line argument"}
{"id": "https://github.com/fkarb/xltable/blob/7a592642d27ad5ee90d2aa8c26338abaa9d84bea/xltable/workbook.py#L82-L123", "method_name": "to_excel", "code": "def to_excel(self, xl_app=None, resize_columns=True):\n         from win32com.client import Dispatch, gencache\n         if xl_app is None:\n             xl_app = Dispatch(\"Excel.Application\")\n         xl_app = gencache.EnsureDispatch(xl_app)\n         assert self.worksheets, \"Can\"t export workbook with no worksheets\"\n         sheets_in_new_workbook = xl_app.SheetsInNewWorkbook\n         try:\n             xl_app.SheetsInNewWorkbook = float(len(self.worksheets))\n             self.workbook_obj = xl_app.Workbooks.Add()\n         finally:\n             xl_app.SheetsInNewWorkbook = sheets_in_new_workbook\n         sheet_names = {s.name for s in self.worksheets}\n         assert len(sheet_names) == len(self.worksheets), \"Worksheets must have unique names\"\n         for worksheet in self.workbook_obj.Sheets:\n             i = 1\n             original_name = worksheet.Name\n             while worksheet.Name in sheet_names:\n                 worksheet.Name = \"%s_%d\" % (original_name, i)\n                 i += 1\n         for worksheet, sheet in zip(self.workbook_obj.Sheets, self.worksheets):\n             worksheet.Name = sheet.name\n         for worksheet, sheet in zip(self.workbook_obj.Sheets, self.itersheets()):\n             worksheet.Select()\n             sheet.to_excel(workbook=self,\n                            worksheet=worksheet,\n                            xl_app=xl_app,\n                            rename=False,\n                            resize_columns=resize_columns)\n         return self.workbook_obj", "query": "export to excel"}
{"id": "https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/lib/cls_filelist.py#L192-L228", "method_name": "save_filelist", "code": "def save_filelist(self, opFile, opFormat, delim=\",\", qu= \n         uses a List of files and collects meta data on them and saves \n         to an text file as a list or with metadata depending on opFormat. \n         \"\"\" \n         op_folder = os.path.dirname(opFile) \n         if op_folder is not None:   \n             if not os.path.exists(op_folder): \n                 os.makedirs(op_folder) \n         with open(opFile,\"w\") as fout: \n             fout.write(\"fullFilename\" + delim) \n             for colHeading in opFormat: \n                 fout.write(colHeading + delim) \n             fout.write(\"\n\") \n             for f in self.filelist: \n                 line = qu + f + qu + delim \n                 try: \n                     for fld in opFormat: \n                         if fld == \"name\": \n                             line = line + qu + os.path.basename(f) + qu + delim \n                         if fld == \"date\": \n                             line = line + qu + self.GetDateAsString(f) + qu + delim \n                         if fld == \"size\": \n                             line = line + qu + str(os.path.getsize(f)) + qu + delim \n                         if fld == \"path\": \n                             line = line + qu + os.path.dirname(f) + qu + delim \n                 except IOError: \n                     line += \"\n\"   \n                 try: \n                     fout.write (str(line.encode(\"ascii\", \"ignore\").decode(\"utf-8\"))) \n                     fout.write (\"\n\") \n                 except IOError: \n                     pass", "query": "save list to file"}
{"id": "https://github.com/fridiculous/estimators/blob/ab5b3d70f16f8372ae1114ac7e54e7791631eb74/estimators/hashing.py#L120-L127", "method_name": "memoize", "code": "def memoize(self, obj):\n         if isinstance(obj, _bytes_or_unicode):\n             return\n         Pickler.memoize(self, obj)", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/Simple6809/Simple6809_rom.py#L40-L66", "method_name": "extract_zip", "code": "def extract_zip(self):\n         assert self.FILE_COUNT>0\n         try:\n             with zipfile.ZipFile(self.archive_path, \"r\") as zip:\n                 namelist = zip.namelist()\n                 print(\"namelist():\", namelist)\n                 if namelist != self.ARCHIVE_NAMES:\n                     msg = (\n                         \"Wrong archive content?!?\"\n                         \" namelist should be: %r\"\n                     ) % self.ARCHIVE_NAMES\n                     log.error(msg)\n                     raise RuntimeError(msg)\n                 zip.extractall(path=self.ROM_PATH)\n         except BadZipFile as err:\n             msg = \"Error extracting archive %r: %s\" % (self.archive_path, err)\n             log.error(msg)\n             raise BadZipFile(msg)\n         hex2bin(\n             src=os.path.join(self.ROM_PATH, \"ExBasROM.hex\"),\n             dst=self.rom_path,\n             verbose=False\n         )", "query": "extract zip file recursively"}
{"id": "https://github.com/poldracklab/niworkflows/blob/254f4b4fcc5e6ecb29d2f4602a30786b913ecce5/niworkflows/interfaces/utils.py#L793-L864", "method_name": "_tsv2json", "code": "def _tsv2json(in_tsv, out_json, index_column, additional_metadata=None,\n               drop_columns=None, enforce_case=True):\n     import pandas as pd\n     re_to_camel = r\"(.*?)_([a-zA-Z0-9])\"\n     re_to_snake = r\"(^.+? .*?)((?<![_A-Z])[A-Z] (?<![_0-9])[0-9]+)\"\n     def snake(match):\n         return \"{}_{}\".format(match.group(1).lower(), match.group(2).lower())\n     def camel(match):\n         return \"{}{}\".format(match.group(1), match.group(2).upper())\n     def less_breakable(a_string):\n         return \"\".join(a_string.split()).strip(\"\n     drop_columns = drop_columns or []\n     additional_metadata = additional_metadata or {}\n     tsv_data = pd.read_csv(in_tsv, \"\\t\")\n     for k, v in additional_metadata.items():\n         tsv_data[k] = v\n     for col in drop_columns:\n         tsv_data.drop(labels=col, axis=\"columns\", inplace=True)\n     tsv_data.set_index(index_column, drop=True, inplace=True)\n     if enforce_case:\n         tsv_data.index = [re.sub(re_to_snake, snake,\n                                  less_breakable(i), 0).lower()\n                           for i in tsv_data.index]\n         tsv_data.columns = [re.sub(re_to_camel, camel,\n                                    less_breakable(i).title(), 0)\n                             for i in tsv_data.columns]\n     json_data = tsv_data.to_json(orient=\"index\")\n     json_data = json.JSONDecoder(\n         object_pairs_hook=OrderedDict).decode(json_data)\n     if out_json is None:\n         return json_data\n     with open(out_json, \"w\") as f:\n         json.dump(json_data, f, indent=4)\n     return out_json", "query": "convert json to csv"}
{"id": "https://github.com/atarashansky/self-assembling-manifold/blob/4db4793f65af62047492327716932ba81a67f679/SAM.py#L1318-L1350", "method_name": "kmeans_clustering", "code": "def kmeans_clustering(self, numc, X=None, npcs=15):\n         from sklearn.cluster import KMeans\n         if X is None:\n             D_sub = self.adata.uns[\"X_processed\"]\n             X = (\n                 D_sub -\n                 D_sub.mean(0)).dot(\n                 self.adata.uns[\"pca_obj\"].components_[\n                     :npcs,\n                     :].T)\n             save = True\n         else:\n             save = False\n         cl = KMeans(n_clusters=numc).fit_predict(Normalizer().fit_transform(X))\n         if save:\n             self.adata.obs[\"kmeans_clusters\"] = pd.Categorical(cl)\n         else:\n             return cl", "query": "k means clustering"}
{"id": "https://github.com/bakwc/JamSpell/blob/bfdfd889436df4dbcd9ec86b20d2baeb815068bd/evaluate/utils.py#L25-L28", "method_name": "loadText", "code": "def loadText(fname):\n     with codecs.open(fname, \"r\", \"utf-8\") as f:\n         data = f.read()\n         return normalize(data).split()", "query": "extracting data from a text file"}
{"id": "https://github.com/Games-and-Simulations/sc-docker/blob/1d7adb9b5839783655564afc4bbcd204a0055dcb/scbw/utils.py#L18-L37", "method_name": "levenshtein_dist", "code": "def levenshtein_dist(s1: str, s2: str) -> int:\n     if len(s1) < len(s2):\n         return levenshtein_dist(s2, s1)\n     if len(s2) == 0:\n         return len(s1)\n     previous_row = range(len(s2) + 1)\n     for i, c1 in enumerate(s1):\n         current_row = [i + 1]\n         for j, c2 in enumerate(s2):\n             insertions = previous_row[j + 1] + 1\n             deletions = current_row[j] + 1  \n             substitutions = previous_row[j] + (c1 != c2)\n             current_row.append(min(insertions, deletions, substitutions))\n         previous_row = current_row\n     return previous_row[-1]", "query": "string similarity levenshtein"}
{"id": "https://github.com/GreatFruitOmsk/tailhead/blob/a3b1324a39935f8ffcfda59328a9a458672889d9/tailhead/__init__.py#L72-L79", "method_name": "read", "code": "def read(self, read_size=-1):\n         read_str = self.file.read(read_size)\n         return len(read_str), read_str", "query": "read properties file"}
{"id": "https://github.com/mattja/sdeint/blob/7cf807cdf97b3bb39d29e1c2dc834b519499b601/sdeint/_broadcast.py#L53-L67", "method_name": "_broadcast_to", "code": "def _broadcast_to(array, shape, subok, readonly):\n     shape = tuple(shape) if np.iterable(shape) else (shape,)\n     array = np.array(array, copy=False, subok=subok)\n     if not shape and array.shape:\n         raise ValueError(\"cannot broadcast a non-scalar to a scalar array\")\n     if any(size < 0 for size in shape):\n         raise ValueError(\"all elements of broadcast shape must be non-\"\n                          \"negative\")\n     broadcast = np.nditer(\n         (array,), flags=[\"multi_index\", \"refs_ok\", \"zerosize_ok\"],\n         op_flags=[\"readonly\"], itershape=shape, order=\"C\").itviews[0]\n     result = _maybe_view_as_subclass(array, broadcast)\n     if not readonly and array.flags.writeable:\n         result.flags.writeable = True\n     return result", "query": "readonly array"}
{"id": "https://github.com/kalefranz/auxlib/blob/6ff2d6b57d128d0b9ed8f01ad83572e938da064f/auxlib/crypt.py#L77-L97", "method_name": "aes_encrypt", "code": "def aes_encrypt(base64_encryption_key, data):\n     if isinstance(data, text_type):\n         data = data.encode(\"UTF-8\")\n     aes_key_bytes, hmac_key_bytes = _extract_keys(base64_encryption_key)\n     data = _pad(data)\n     iv_bytes = os.urandom(AES_BLOCK_SIZE)\n     cipher = AES.new(aes_key_bytes, mode=AES.MODE_CBC, IV=iv_bytes)\n     data = iv_bytes + cipher.encrypt(data)  \n     hmac_signature = hmac.new(hmac_key_bytes, data, hashlib.sha256).digest()\n     return as_base64(data + hmac_signature)", "query": "aes encryption"}
{"id": "https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilebuild/iotile/build/config/scons-local-3.0.1/SCons/Util.py#L1300-L1303", "method_name": "__make_unique", "code": "def __make_unique(self):\n         if not self.unique:\n             self.data = uniquer_hashables(self.data)\n             self.unique = True", "query": "unique elements"}
{"id": "https://github.com/markbaas/python-iresolve/blob/ba91e37221e91265e4ac5dbc6e8f5cffa955a04f/iresolve.py#L136-L141", "method_name": "output", "code": "def output(results, output_format=\"pretty\"):\n     if output_format == \"pretty\":\n         for u, meta in results.items():\n             print(\"* {} can be imported from: {}\".format(u, \", \".join(meta[\"paths\"])))\n     elif output_format == \"json\":\n         print(json.dumps(results))", "query": "pretty print json"}
{"id": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Main.py#L13-L18", "method_name": "func", "code": "def func(X, y):\n     from sklearn.linear_model import LinearRegression\n     from sklearn.model_selection import cross_val_score\n     model = LinearRegression()\n     model.fit(X, y)\n     return model.predict(X)", "query": "linear regression"}
{"id": "https://github.com/bxlab/bx-python/blob/09cb725284803df90a468d910f2274628d8647de/lib/bx/align/lav.py#L287-L296", "method_name": "fetch_line", "code": "def fetch_line(self,strip=True,requireLine=True,report=\"\"):\n \t\tif   (strip == None): line = self.file.readline()\n \t\telif (strip == True): line = self.file.readline().strip()\n \t\telse:                 line = self.file.readline().strip().strip(strip)\n \t\tself.lineNumber += 1\n \t\tif (requireLine):\n \t\t\tassert (line), \\\n \t\t\t       \"unexpected blank line or end of file%s (line %d)\" \\\n \t\t\t     % (report,self.lineNumber)\n \t\treturn line", "query": "read text file line by line"}
{"id": "https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/system/time_helper.py#L78-L82", "method_name": "day_to_month", "code": "def day_to_month(timeperiod):\n     t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN)\n     return t.strftime(SYNERGY_MONTHLY_PATTERN)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/jordanjoz1/flickr-views-counter/blob/cba89285823ab39afd9f40a19db82371d45bd830/count_views.py#L130-L138", "method_name": "write_to_csv", "code": "def write_to_csv(fname, header, rows):\n     with open(fname, \"wb\") as csvfile:\n         csvwriter = csv.writer(csvfile, delimiter=\",\", quotechar=\" \",\n                                quoting=csv.QUOTE_MINIMAL)\n         csvwriter.writerow(header)\n         for row in rows:\n             csvwriter.writerow(\n                 [s.encode(\"utf-8\").replace(\",\", \"\").replace(\"\n\", \"\")\n                  for s in row])", "query": "write csv"}
{"id": "https://github.com/rhenanbartels/hrv/blob/cd4c7e6e508299d943930886d20413f63845f60f/hrv/rri.py#L123-L124", "method_name": "median", "code": "def median(self, *args, **kwargs):\n        return np.median(self.rri, *args, **kwargs)", "query": "deducting the median from each column"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/excel.py#L1227-L1269", "method_name": "compile_geometry", "code": "def compile_geometry(lat, lon, elev):\n     logger_excel.info(\"enter compile_geometry\")\n     lat = _remove_geo_placeholders(lat)\n     lon = _remove_geo_placeholders(lon)\n     if len(lat) == 2 and len(lon) == 2:\n         logger_excel.info(\"found 4 coordinates\")\n         geo_dict = geometry_linestring(lat, lon, elev)\n     elif len(lat) == 1 and len(lon) == 1:\n         logger_excel.info(\"found 2 coordinates\")\n         geo_dict = geometry_point(lat, lon, elev)\n     elif (None in lon and None not in lat) or (len(lat) > 0 and len(lon) == 0):\n         geo_dict = geometry_range(lat, elev, \"lat\")\n     elif (None in lat and None not in lon) or (len(lon) > 0 and len(lat) == 0):\n         geo_dict = geometry_range(lat, elev, \"lon\")\n     else:\n         geo_dict = {}\n         logger_excel.warn(\"compile_geometry: invalid coordinates: lat: {}, lon: {}\".format(lat, lon))\n     logger_excel.info(\"exit compile_geometry\")\n     return geo_dict", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/nkmathew/yasi-sexp-indenter/blob/6ec2a4675e79606c555bcb67494a0ba994b05805/yasi.py#L551-L568", "method_name": "parse_rc_json", "code": "def parse_rc_json():\n     fname = \".yasirc.json\"\n     path = os.path.expanduser(\"~/\" + fname)\n     if os.path.exists(fname):\n         path = os.path.abspath(fname)\n     elif not os.path.exists(path):\n         path = \"\"\n     content = \"\"\n     if path:\n         with open(path) as f:\n             content = f.read()\n     ret = {}\n     if content:\n         ret = json.loads(content)\n     return collections.defaultdict(dict, ret)", "query": "parse json file"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_rdf.py#L9-L17", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser()\n     parser.add_argument( \"xdatcar\" )\n     parser.add_argument( \"label\", nargs = 2 )\n     parser.add_argument( \"max_r\", type = float )\n     parser.add_argument( \"n_bins\", type = int )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/pdf.py#L161-L186", "method_name": "add_to_writer", "code": "def add_to_writer(self,\n                       writer: PdfFileWriter,\n                       start_recto: bool = True) -> None:\n         if self.is_html:\n             pdf = get_pdf_from_html(\n                 html=self.html,\n                 header_html=self.header_html,\n                 footer_html=self.footer_html,\n                 wkhtmltopdf_filename=self.wkhtmltopdf_filename,\n                 wkhtmltopdf_options=self.wkhtmltopdf_options)\n             append_memory_pdf_to_writer(pdf, writer, start_recto=start_recto)\n         elif self.is_filename:\n             if start_recto and writer.getNumPages() % 2 != 0:\n                 writer.addBlankPage()\n             writer.appendPagesFromReader(PdfFileReader(\n                 open(self.filename, \"rb\")))\n         else:\n             raise AssertionError(\"PdfPlan: shouldn\"t get here!\")", "query": "convert html to pdf"}
{"id": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L359-L361", "method_name": "assert_not_checked_checkbox", "code": "def assert_not_checked_checkbox(step, value):\n    check_box = find_field(world.browser, \"checkbox\", value)\n    assert_true(step, not check_box.is_selected())", "query": "check if a checkbox is checked"}
{"id": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/event/pqueue.py#L1032-L1040", "method_name": "setPriority", "code": "def setPriority(self, queue, priority):\n         q = self.queueindex[queue]\n         self.queues[q[0]].removeSubQueue(q[1])\n         newPriority = self.queues.setdefault(priority, CBQueue.MultiQueue(self, priority))\n         q[0] = priority\n         newPriority.addSubQueue(q[1])", "query": "priority queue"}
{"id": "https://github.com/rcook/upload-haddocks/blob/a33826be1873da68ba073a42ec828c8ec150d576/setup.py#L16-L26", "method_name": "_read_properties", "code": "def _read_properties():\n     init_path = os.path.abspath(os.path.join(\"uploadhaddocks\", \"__init__.py\"))\n     regex = re.compile(\"^\\\\s*__(?P<key>.*)__\\\\s*=\\\\s*\\\"(?P<value>.*)\\\"\\\\s*$\")\n     with open(init_path, \"rt\") as f:\n         props = {}\n         for line in f.readlines():\n             m = regex.match(line)\n             if m is not None:\n                 props[m.group(\"key\")] = m.group(\"value\")\n     return props", "query": "read properties file"}
{"id": "https://github.com/roclark/sportsreference/blob/ea0bae432be76450e137671d2998eb38f962dffd/sportsreference/mlb/schedule.py#L172-L179", "method_name": "datetime", "code": "def datetime(self):\n         date_string = \"%s %s\" % (self._date, self._year)\n         date_string = re.sub(r\" \\(\\d+\\)\", \"\", date_string)\n         return datetime.strptime(date_string, \"%A, %b %d %Y\")", "query": "string to date"}
{"id": "https://github.com/frictionlessdata/datapackage-pipelines/blob/3a34bbdf042d13c3bec5eef46ff360ee41403874/datapackage_pipelines/lib/dump/to_path.py#L16-L25", "method_name": "write_file_to_output", "code": "def write_file_to_output(self, filename, path):\n         path = os.path.join(self.out_path, path)\n         if self.add_filehash_to_path and os.path.exists(path):\n             return\n         path_part = os.path.dirname(path)\n         PathDumper.__makedirs(path_part)\n         shutil.copy(filename, path)\n         os.chmod(path, 0o666)\n         return path", "query": "copying a file to a path"}
{"id": "https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/plugin_html.py#L177-L183", "method_name": "tag_to_dict", "code": "def tag_to_dict(html):\n     element = document_fromstring(html).xpath(\"//html/body/child::*\")[0]\n     attributes = dict(element.attrib)\n     attributes[\"text\"] = element.text_content()\n     return attributes", "query": "reading element from html - <td>"}
{"id": "https://github.com/CEA-COSMIC/ModOpt/blob/019b189cb897cbb4d210c44a100daaa08468830c/modopt/math/stats.py#L75-L106", "method_name": "mad", "code": "def mad(data):\n     r\n     return np.median(np.abs(data - np.median(data)))", "query": "deducting the median from each column"}
{"id": "https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L662-L667", "method_name": "checkbox_uncheck", "code": "def checkbox_uncheck(self, force_check=False):\n         if self.get_attribute(\"checked\"):\n             self.click(force_click=force_check)", "query": "make the checkbox checked"}
{"id": "https://github.com/dendory/menu3/blob/350414966e8f5c7737cd527369c8087f4f8f600b/menu3/menu3.py#L45-L49", "method_name": "underline", "code": "def underline(self, text): \n \t\tif self._windows(): \n \t\t\treturn text \n \t\telse: \n \t\t\treturn self.UNDERLINE + text + self.NORMAL", "query": "underline text in label widget"}
{"id": "https://github.com/petermelias/valhalla/blob/b08d7a4a94fd8d85a6f5ea86200ec60ef4525e3d/valhalla/filters/strings.py#L90-L99", "method_name": "regex", "code": "def regex(regex, case=False, _value=None, *args, **kwargs):\n     if kwargs.get(\"case\"):\n         regex = re.compile(regex)\n     else:\n         regex = re.compile(regex, re.IGNORECASE)\n     if not regex.match(_value):\n         raise ValidationError(\"The _value must match the regex %s\" % regex)\n     return _value", "query": "regex case insensitive"}
{"id": "https://github.com/a-tal/nagaram/blob/2edcb0ef8cb569ebd1c398be826472b4831d6110/nagaram/scrabble.py#L136-L184", "method_name": "valid_scrabble_word", "code": "def valid_scrabble_word(word):\n     letters_in_bag = {\n         \"a\": 9,\n         \"b\": 2,\n         \"c\": 2,\n         \"d\": 4,\n         \"e\": 12,\n         \"f\": 2,\n         \"g\": 3,\n         \"h\": 2,\n         \"i\": 9,\n         \"j\": 1,\n         \"k\": 1,\n         \"l\": 4,\n         \"m\": 2,\n         \"n\": 6,\n         \"o\": 8,\n         \"p\": 2,\n         \"q\": 1,\n         \"r\": 6,\n         \"s\": 4,\n         \"t\": 6,\n         \"u\": 4,\n         \"v\": 2,\n         \"w\": 2,\n         \"x\": 1,\n         \"y\": 2,\n         \"z\": 1,\n         \"_\": 2,\n     }\n     for letter in word:\n         if letter == \"?\":\n             continue\n         try:\n             letters_in_bag[letter] -= 1\n         except KeyError:\n             return False\n         if letters_in_bag[letter] < 0:\n             letters_in_bag[\"_\"] -= 1\n             if letters_in_bag[\"_\"] < 0:\n                 return False\n     return True", "query": "determine a string is a valid word"}
{"id": "https://github.com/cablehead/vanilla/blob/c9f5b86f45720a30e8840fb68b1429b919c4ca66/vanilla/message.py#L57-L58", "method_name": "recv_n", "code": "def recv_n(self, n, timeout=-1):\n        return self.recver.recv_n(n, timeout=timeout)", "query": "socket recv timeout"}
{"id": "https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/transformations.py#L11-L75", "method_name": "toUIntArray", "code": "def toUIntArray(img, dtype=None, cutNegative=True, cutHigh=True, \n                 range=None, copy=True): \n     mn, mx = None, None \n     if range is not None: \n         mn, mx = range \n     if dtype is None: \n         if mx is None: \n             mx = np.nanmax(img) \n         dtype = np.uint16 if mx > 255 else np.uint8 \n     dtype = np.dtype(dtype) \n     if dtype == img.dtype: \n         return img \n     b = {\"uint8\": 255, \n          \"uint16\": 65535, \n          \"uint32\": 4294967295, \n          \"uint64\": 18446744073709551615}[dtype.name] \n     if copy: \n         img = img.copy() \n     if range is not None: \n         img = np.asfarray(img) \n         img -= mn \n         img *= b / (mx - mn) \n         img = np.clip(img, 0, b) \n     else: \n         if cutNegative: \n             img[img < 0] = 0 \n         else: \n             mn = np.min(img) \n             if mn < 0: \n                 img -= mn  \n         if cutHigh: \n             img[img > b] = b \n         else: \n             mx = np.nanmax(img) \n             img = np.asfarray(img) * (float(b) / mx) \n     img = img.astype(dtype) \n     return img", "query": "converting uint8 array to image"}
{"id": "https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/url.py#L250-L272", "method_name": "url_parse_query", "code": "def url_parse_query(query, encoding=None):\n     if isinstance(query, unicode):\n         if encoding is None:\n             encoding = url_encoding\n         query = query.encode(encoding, \"ignore\")\n     append = \"\"\n     while \"?\" in query:\n         query, rest = query.rsplit(\"?\", 1)\n         append = \"?\"+url_parse_query(rest)+append\n     l = []\n     for k, v, sep in parse_qsl(query, keep_blank_values=True):\n         k = url_quote_part(k, \"/-:,;\")\n         if v:\n             v = url_quote_part(v, \"/-:,;\")\n             l.append(\"%s=%s%s\" % (k, v, sep))\n         elif v is None:\n             l.append(\"%s%s\" % (k, sep))\n         else:\n             l.append(\"%s=%s\" % (k, sep))\n     return \"\".join(l) + append", "query": "parse query string in url"}
{"id": "https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/Dumper/file.py#L109-L114", "method_name": "binary_file", "code": "def binary_file(self, file=None):\n         if file is None:\n             file = BytesIO()\n         self._binary_file(file)\n         return file", "query": "parse binary file to custom class"}
{"id": "https://github.com/diffeo/yakonfig/blob/412e195da29b4f4fc7b72967c192714a6f5eaeb5/yakonfig/cmd.py#L132-L148", "method_name": "parseline", "code": "def parseline(self, line):\n         cmd, arg, line = Cmd.parseline(self, line)\n         if cmd and cmd.strip() != \"\":\n             dof = getattr(self, \"do_\" + cmd, None)\n             argf = getattr(self, \"args_\" + cmd, None)\n         else:\n             argf = None\n         if argf:\n             parser = argparse.ArgumentParser(\n                 prog=cmd,\n                 description=getattr(dof, \"__doc__\", None))\n             argf(parser)\n             try:\n                 arg = parser.parse_args(shlex.split(arg))\n             except SystemExit, e:\n                 return \"\", \"\", \"\"\n         return cmd, arg, line", "query": "parse command line argument"}
{"id": "https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/filter.py#L459-L463", "method_name": "params", "code": "def params(self):\n         if not any(filter.params for filter in self):\n             return None\n         else:\n             return Array(filter.params or Null() for filter in self)", "query": "filter array"}
{"id": "https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/server.py#L25-L53", "method_name": "maps_json", "code": "def maps_json():\n     map_sources = {\n         id: {\n                 \"id\": map_source.id,\n                 \"name\": map_source.name,\n                 \"folder\": map_source.folder,\n                 \"min_zoom\": map_source.min_zoom,\n                 \"max_zoom\": map_source.max_zoom,\n                 \"layers\": [\n                     {\n                         \"min_zoom\": layer.min_zoom,\n                         \"max_zoom\": layer.max_zoom,\n                         \"tile_url\": layer.tile_url.replace(\"$\", \"\"),\n                     } for layer in map_source.layers\n                     ]\n             } for id, map_source in app.config[\"mapsources\"].items()\n         }\n     return jsonify(map_sources)", "query": "map to json"}
{"id": "https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hmtk/seismicity/smoothing/smoothed_seismicity.py#L491-L518", "method_name": "write_to_csv", "code": "def write_to_csv(self, filename):\n         fid = open(filename, \"wt\")\n         header_info = [\"Longitude\", \"Latitude\", \"Depth\", \"Observed Count\",\n                        \"Smoothed Rate\", \"b-value\"]\n         writer = csv.DictWriter(fid, fieldnames=header_info)\n         headers = dict((name0, name0) for name0 in header_info)\n         writer.writerow(headers)\n         for row in self.data:\n             if row[4] == 0:\n                 continue\n             row_dict = {\"Longitude\": \"%g\" % row[0],\n                         \"Latitude\": \"%g\" % row[1],\n                         \"Depth\": \"%g\" % row[2],\n                         \"Observed Count\": \"%d\" % row[3],\n                         \"Smoothed Rate\": \"%.6g\" % row[4],\n                         \"b-value\": \"%g\" % self.bval}\n             writer.writerow(row_dict)\n         fid.close()", "query": "write csv"}
{"id": "https://github.com/myusuf3/delorean/blob/3e8a7b8cfd4c26546f62bde2f34002893adfa08a/delorean/dates.py#L492-L512", "method_name": "epoch", "code": "def epoch(self):\n         epoch_sec = pytz.utc.localize(datetime.utcfromtimestamp(0))\n         now_sec = pytz.utc.normalize(self._dt)\n         delta_sec = now_sec - epoch_sec\n         return get_total_second(delta_sec)", "query": "convert a utc time to epoch"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40", "method_name": "binomial", "code": "def binomial(n,k):\n     if n==k: return 1\n     assert n>k, \"Attempting to call binomial(%d,%d)\" % (n,k)\n     return factorial(n)//(factorial(k)*factorial(n-k))", "query": "binomial distribution"}
{"id": "https://github.com/galaxyproject/pulsar/blob/9ab6683802884324652da0a9f0808c7eb59d3ab4/pulsar/client/client.py#L392-L395", "method_name": "_copy", "code": "def _copy(from_path, to_path):\n     message = \"Copying path [%s] to [%s]\"\n     log.debug(message, from_path, to_path)\n     copy(from_path, to_path)", "query": "copying a file to a path"}
{"id": "https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/utils.py#L515-L539", "method_name": "get_random_int", "code": "def get_random_int(min_v=0, max_v=10, number=5, seed=None):\n     rnd = random.Random()\n     if seed:\n         rnd = random.Random(seed)\n     return [rnd.randint(min_v, max_v) for p in range(0, number)]", "query": "randomly pick a number"}
{"id": "https://github.com/vfxetc/sgactions/blob/e2c6ee14a1f8092e97f57a501650581428a6ac6e/sgactions/dispatch.py#L14-L33", "method_name": "parse_url", "code": "def parse_url(url):\n     m = re.match(r\"^(?:(\\w+):)?(.*?)(?:/(.*?))?(?:\\?(.*))?$\", url)\n     scheme, netloc, path, query = m.groups()\n     kwargs = urlparse.parse_qs(query, keep_blank_values=True) if query else {}\n     for k, v in kwargs.iteritems():\n         if len(v) == 1 and k not in (\"cols\", \"column_display_names\"):\n             kwargs[k] = v = v[0]\n         if k.endswith(\"_id\") and v.isdigit():\n             kwargs[k] = int(v)\n     m = re.match(r\"^([\\w.]+:\\w+)$\", netloc)\n     if not m:\n         raise ValueError(\"entrypoint must be like \"package.module:function\"; got \"%s\"\" % netloc)\n         return 1\n     return m.group(1), kwargs", "query": "parse query string in url"}
{"id": "https://github.com/majerteam/sylk_parser/blob/0f62f9d3ca1c273017d7ea728f9db809f2241389/sylk_parser/sylk_parser.py#L27-L40", "method_name": "to_csv", "code": "def to_csv(self, fbuf, quotechar=\"\"\", delimiter=\",\"):\n         csvwriter = csv.writer(\n             fbuf,\n             quotechar=quotechar,\n             delimiter=delimiter,\n             lineterminator=\"\n\",\n             quoting=csv.QUOTE_ALL\n         )\n         if self.headers:\n             csvwriter.writerow(self.headers)\n         for line in self.sylk_handler.stream_rows():\n             csvwriter.writerow(line)", "query": "write csv"}
{"id": "https://github.com/amaas-fintech/amaas-core-sdk-python/blob/347b71f8e776b2dde582b015e31b4802d91e8040/amaascore/core/amaas_model.py#L30-L31", "method_name": "to_json_string", "code": "def to_json_string(dict_to_convert):\n    return json.dumps(dict_to_convert, ensure_ascii=False, default=json_handler, indent=4, separators=(\",\", \": \"))", "query": "json to xml conversion"}
{"id": "https://github.com/MIT-LCP/wfdb-python/blob/cc8c9e9e44f10af961b7a9d8ae03708b31ac8a8c/wfdb/io/_header.py#L675-L730", "method_name": "_read_header_lines", "code": "def _read_header_lines(base_record_name, dir_name, pb_dir):\n     file_name = base_record_name + \".hea\"\n     if pb_dir is None:\n         with open(os.path.join(dir_name, file_name), \"r\") as fp:\n             header_lines = []\n             comment_lines = []\n             for line in fp:\n                 line = line.strip()\n                 if line.startswith(\"\n                     comment_lines.append(line)\n                 elif line:\n                     ci = line.find(\"\n                     if ci > 0:\n                         header_lines.append(line[:ci])\n                         comment_lines.append(line[ci:])\n                     else:\n                         header_lines.append(line)\n     else:\n         header_lines, comment_lines = download._stream_header(file_name,\n                                                               pb_dir)\n     return header_lines, comment_lines", "query": "read text file line by line"}
{"id": "https://github.com/scheibler/khard/blob/0f69430c2680f1ff5f073a977a3c5b753b96cc17/khard/helpers.py#L81-L107", "method_name": "string_to_date", "code": "def string_to_date(input):\n     for format_string in (\"--%m%d\", \"--%m-%d\", \"%Y%m%d\", \"%Y-%m-%d\",\n                           \"%Y%m%dT%H%M%S\", \"%Y-%m-%dT%H:%M:%S\",\n                           \"%Y%m%dT%H%M%SZ\", \"%Y-%m-%dT%H:%M:%SZ\"):\n         try:\n             return datetime.strptime(input, format_string)\n         except ValueError:\n             pass\n     for format_string in (\"%Y%m%dT%H%M%S%z\", \"%Y-%m-%dT%H:%M:%S%z\"):\n         try:\n             return datetime.strptime(\"\".join(input.rsplit(\":\", 1)),\n                                      format_string)\n         except ValueError:\n             pass\n     raise ValueError", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/toejough/pimento/blob/cdb00a93976733aa5521f8504152cedeedfc711a/pimento/__init__.py#L298-L311", "method_name": "_exact_match", "code": "def _exact_match(response, matches, insensitive, fuzzy):\n     for match in matches:\n         if response == match:\n             return match\n         elif insensitive and response.lower() == match.lower():\n             return match\n         elif fuzzy and _exact_fuzzy_match(response, match, insensitive):\n             return match\n     else:\n         return None", "query": "fuzzy match ranking"}
{"id": "https://github.com/yyuu/botornado/blob/fffb056f5ff2324d1d5c1304014cfb1d899f602e/boto/cloudfront/identity.py#L77-L84", "method_name": "to_xml", "code": "def to_xml(self):\n         s = \"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\"\n         s += \"<CloudFrontOriginAccessIdentityConfig xmlns=\"http://cloudfront.amazonaws.com/doc/2009-09-09/\">\n\"\n         s += \"  <CallerReference>%s</CallerReference>\n\" % self.caller_reference\n         if self.comment:\n             s += \"  <Comment>%s</Comment>\n\" % self.comment\n         s += \"</CloudFrontOriginAccessIdentityConfig>\n\"\n         return s", "query": "json to xml conversion"}
{"id": "https://github.com/lepture/flask-oauthlib/blob/9e6f152a5bb360e7496210da21561c3e6d41b0e1/flask_oauthlib/provider/oauth1.py#L902-L905", "method_name": "_error_response", "code": "def _error_response(e):\n     res = make_response(e.urlencoded, e.status_code)\n     res.headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n     return res", "query": "custom http error response"}
{"id": "https://github.com/python-xlib/python-xlib/blob/8901e831737e79fe5645f48089d70e1d1046d2f2/Xlib/protocol/rq.py#L1135-L1210", "method_name": "parse_binary", "code": "def parse_binary(self, data, display, rawdict = 0):\n         ret = {}\n         val = struct.unpack(self.static_codes, data[:self.static_size])\n         lengths = {}\n         formats = {}\n         vno = 0\n         for f in self.static_fields:\n             if not f.name:\n                 pass\n             elif isinstance(f, LengthField):\n                 f_names = [f.name]\n                 if f.other_fields:\n                     f_names.extend(f.other_fields)\n                 field_val = val[vno]\n                 if f.parse_value is not None:\n                     field_val = f.parse_value(field_val, display)\n                 for f_name in f_names:\n                     lengths[f_name] = field_val\n             elif isinstance(f, FormatField):\n                 formats[f.name] = val[vno]\n             else:\n                 if f.structvalues == 1:\n                     field_val = val[vno]\n                 else:\n                     field_val = val[vno:vno+f.structvalues]\n                 if f.parse_value is not None:\n                     field_val = f.parse_value(field_val, display)\n                 ret[f.name] = field_val\n             vno = vno + f.structvalues\n         data = data[self.static_size:]\n         for f in self.var_fields:\n             ret[f.name], data = f.parse_binary_value(data, display,\n                                                      lengths.get(f.name),\n                                                      formats.get(f.name),\n                                                     )\n         if not rawdict:\n             ret = DictWrapper(ret)\n         return ret, data", "query": "parse binary file to custom class"}
{"id": "https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/sma.py#L69-L75", "method_name": "get_monitor_observables", "code": "def get_monitor_observables(self, name):\n         try:\n             lines = self.items(name)\n         except NoSectionError:\n             return []\n         lines = [ItemLine(key, value) for key, value in lines]\n         return get_observables_from_lines(lines)", "query": "get current observable value"}
{"id": "https://github.com/MIT-LCP/wfdb-python/blob/cc8c9e9e44f10af961b7a9d8ae03708b31ac8a8c/wfdb/io/_header.py#L675-L730", "method_name": "_read_header_lines", "code": "def _read_header_lines(base_record_name, dir_name, pb_dir):\n     file_name = base_record_name + \".hea\"\n     if pb_dir is None:\n         with open(os.path.join(dir_name, file_name), \"r\") as fp:\n             header_lines = []\n             comment_lines = []\n             for line in fp:\n                 line = line.strip()\n                 if line.startswith(\"\n                     comment_lines.append(line)\n                 elif line:\n                     ci = line.find(\"\n                     if ci > 0:\n                         header_lines.append(line[:ci])\n                         comment_lines.append(line[ci:])\n                     else:\n                         header_lines.append(line)\n     else:\n         header_lines, comment_lines = download._stream_header(file_name,\n                                                               pb_dir)\n     return header_lines, comment_lines", "query": "read text file line by line"}
{"id": "https://github.com/totalgood/pugnlp/blob/c43445b14afddfdeadc5f3076675c9e8fc1ee67c/src/pugnlp/stats.py#L536-L562", "method_name": "from_existing", "code": "def from_existing(cls, confusion, *args, **kwargs):\n         df = []\n         for t, p in product(confusion.index.values, confusion.columns.values):\n             df += [[t, p]] * confusion[p][t]\n         if confusion.index.name is not None and confusion.columns.name is not None:\n             return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name]))\n         return Confusion(pd.DataFrame(df))", "query": "confusion matrix"}
{"id": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/syncmap/__init__.py#L309-L368", "method_name": "output_html_for_tuning", "code": "def output_html_for_tuning(\n             self,\n             audio_file_path,\n             output_file_path,\n             parameters=None\n     ):\n         if not gf.file_can_be_written(output_file_path):\n             self.log_exc(u\"Cannot output HTML file \"%s\". Wrong permissions?\" % (output_file_path), None, True, OSError)\n         if parameters is None:\n             parameters = {}\n         audio_file_path_absolute = gf.fix_slash(os.path.abspath(audio_file_path))\n         template_path_absolute = gf.absolute_path(self.FINETUNEAS_PATH, __file__)\n         with io.open(template_path_absolute, \"r\", encoding=\"utf-8\") as file_obj:\n             template = file_obj.read()\n         for repl in self.FINETUNEAS_REPLACEMENTS:\n             template = template.replace(repl[0], repl[1])\n         template = template.replace(\n             self.FINETUNEAS_REPLACE_AUDIOFILEPATH,\n             u\"audioFilePath = \\\"file://%s\\\";\" % audio_file_path_absolute\n         )\n         template = template.replace(\n             self.FINETUNEAS_REPLACE_FRAGMENTS,\n             u\"fragments = (%s).fragments;\" % self.json_string\n         )\n         if gc.PPN_TASK_OS_FILE_FORMAT in parameters:\n             output_format = parameters[gc.PPN_TASK_OS_FILE_FORMAT]\n             if output_format in self.FINETUNEAS_ALLOWED_FORMATS:\n                 template = template.replace(\n                     self.FINETUNEAS_REPLACE_OUTPUT_FORMAT,\n                     u\"outputFormat = \\\"%s\\\";\" % output_format\n                 )\n                 if output_format == \"smil\":\n                     for key, placeholder, replacement in [\n                             (\n                                 gc.PPN_TASK_OS_FILE_SMIL_AUDIO_REF,\n                                 self.FINETUNEAS_REPLACE_SMIL_AUDIOREF,\n                                 \"audioref = \\\"%s\\\";\"\n                             ),\n                             (\n                                 gc.PPN_TASK_OS_FILE_SMIL_PAGE_REF,\n                                 self.FINETUNEAS_REPLACE_SMIL_PAGEREF,\n                                 \"pageref = \\\"%s\\\";\"\n                             ),\n                     ]:\n                         if key in parameters:\n                             template = template.replace(\n                                 placeholder,\n                                 replacement % parameters[key]\n                             )\n         with io.open(output_file_path, \"w\", encoding=\"utf-8\") as file_obj:\n             file_obj.write(template)", "query": "output to html file"}
{"id": "https://github.com/EntilZha/PyFunctional/blob/ac04e4a8552b0c464a7f492f7c9862424867b63e/functional/io.py#L115-L122", "method_name": "read", "code": "def read(self):\n         with gzip.GzipFile(self.path, compresslevel=self.compresslevel) as gz_file:\n             gz_file.read1 = gz_file.read\n             with io.TextIOWrapper(gz_file,\n                                   encoding=self.encoding,\n                                   errors=self.errors,\n                                   newline=self.newline) as file_content:\n                 return file_content.read()", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/controller/widgets/SignalFrame.py#L31-L33", "method_name": "perform_filter", "code": "def perform_filter(result_array: Array, data, f_low, f_high, filter_bw):\n    result_array = np.frombuffer(result_array.get_obj(), dtype=np.complex64)\n    result_array[:] = Filter.apply_bandpass_filter(data, f_low, f_high, filter_bw=filter_bw)", "query": "filter array"}
{"id": "https://github.com/jborean93/smbprotocol/blob/d8eb00fbc824f97d0f4946e3f768c5e6c723499a/smbprotocol/structure.py#L775-L785", "method_name": "_to_string", "code": "def _to_string(self):\n         enum_name = None\n         value = self._get_calculated_value(self.value)\n         for enum, enum_value in vars(self.enum_type).items():\n             if value == enum_value:\n                 enum_name = enum\n                 break\n         if enum_name is None:\n             return \"(%d) UNKNOWN_ENUM\" % value\n         else:\n             return \"(%d) %s\" % (value, enum_name)", "query": "get name of enumerated value"}
{"id": "https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/experimental/streaming/batched_queue.py#L222-L227", "method_name": "read_next", "code": "def read_next(self):\n         if not self.read_buffer:\n             self._read_next_batch()\n             assert self.read_buffer\n         self.read_item_offset += 1\n         return self.read_buffer.pop(0)", "query": "buffered file reader read text"}
{"id": "https://github.com/mattja/sdeint/blob/7cf807cdf97b3bb39d29e1c2dc834b519499b601/sdeint/_broadcast.py#L53-L67", "method_name": "_broadcast_to", "code": "def _broadcast_to(array, shape, subok, readonly):\n     shape = tuple(shape) if np.iterable(shape) else (shape,)\n     array = np.array(array, copy=False, subok=subok)\n     if not shape and array.shape:\n         raise ValueError(\"cannot broadcast a non-scalar to a scalar array\")\n     if any(size < 0 for size in shape):\n         raise ValueError(\"all elements of broadcast shape must be non-\"\n                          \"negative\")\n     broadcast = np.nditer(\n         (array,), flags=[\"multi_index\", \"refs_ok\", \"zerosize_ok\"],\n         op_flags=[\"readonly\"], itershape=shape, order=\"C\").itviews[0]\n     result = _maybe_view_as_subclass(array, broadcast)\n     if not readonly and array.flags.writeable:\n         result.flags.writeable = True\n     return result", "query": "readonly array"}
{"id": "https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/lib/upload.py#L225-L246", "method_name": "_file_size", "code": "def _file_size(file_path, uncompressed=False):\n     _, ext = os.path.splitext(file_path)\n     if uncompressed:\n         if ext in {\".gz\", \".gzip\"}:\n             with gzip.GzipFile(file_path, mode=\"rb\") as fp:\n                 try:\n                     fp.seek(0, os.SEEK_END)\n                     return fp.tell()\n                 except ValueError:\n                     fp.seek(0)\n                     while len(fp.read(8192)) != 0:\n                         pass\n                     return fp.tell()\n         elif ext in {\".bz\", \".bz2\", \".bzip\", \".bzip2\"}:\n             with bz2.BZ2File(file_path, mode=\"rb\") as fp:\n                 fp.seek(0, os.SEEK_END)\n                 return fp.tell()\n     return os.path.getsize(file_path)", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/whiteclover/dbpy/blob/3d9ce85f55cfb39cced22081e525f79581b26b3a/db/pymysql/connection.py#L53-L56", "method_name": "connect", "code": "def connect(self):\n         self.close()\n         self._connect = pymysql.connect(**self._db_options)\n         self._connect.autocommit(True)", "query": "connect to sql"}
{"id": "https://github.com/ModisWorks/modis/blob/1f1225c9841835ec1d1831fc196306527567db8b/modis/discord_modis/modules/music/api_music.py#L94-L234", "method_name": "parse_query", "code": "def parse_query(query, ilogger):\n     p = urlparse(query)\n     if p and p.scheme and p.netloc:\n         if \"youtube\" in p.netloc and p.query and ytdiscoveryapi is not None:\n             query_parts = p.query.split(\"&\")\n             yturl_parts = {}\n             for q in query_parts:\n                 s = q.split(\"=\")\n                 if len(s) < 2:\n                     continue\n                 q_name = s[0]\n                 q_val = \"=\".join(s[1:])\n                 if q_name not in yturl_parts:\n                     yturl_parts[q_name] = q_val\n             if \"list\" in yturl_parts:\n                 ilogger.info(\"Queued YouTube playlist from link\")\n                 return get_queue_from_playlist(yturl_parts[\"list\"])\n             elif \"v\" in yturl_parts:\n                 ilogger.info(\"Queued YouTube video from link\")\n                 return [[\"https://www.youtube.com/watch?v={}\".format(yturl_parts[\"v\"]), query]]\n         elif \"soundcloud\" in p.netloc:\n             if scclient is None:\n                 ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                 return [[query, query]]\n             try:\n                 result = scclient.get(\"/resolve\", url=query)\n                 track_list = []\n                 if isinstance(result, ResourceList):\n                     for r in result.data:\n                         tracks = get_sc_tracks(r)\n                         if tracks is not None:\n                             for t in tracks:\n                                 track_list.append(t)\n                 elif isinstance(result, Resource):\n                     tracks = get_sc_tracks(result)\n                     if tracks is not None:\n                         for t in tracks:\n                             track_list.append(t)\n                 if track_list is not None and len(track_list) > 0:\n                     ilogger.info(\"Queued SoundCloud songs from link\")\n                     return track_list\n                 else:\n                     ilogger.error(\"Could not queue from SoundCloud API\")\n                     return [[query, query]]\n             except Exception as e:\n                 logger.exception(e)\n                 ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                 return [[query, query]]\n         else:\n             ilogger.debug(\"Using url: {}\".format(query))\n             return [[query, query]]\n     args = query.split(\" \")\n     if len(args) == 0:\n         ilogger.error(\"No query given\")\n         return []\n     if args[0].lower() in [\"sp\", \"spotify\"] and spclient is not None:\n         if spclient is None:\n             ilogger.error(\"Host does not support Spotify\")\n             return []\n         try:\n             if len(args) > 2 and args[1] in [\"album\", \"artist\", \"song\", \"track\", \"playlist\"]:\n                 query_type = args[1].lower()\n                 query_search = \" \".join(args[2:])\n             else:\n                 query_type = \"track\"\n                 query_search = \" \".join(args[1:])\n             query_type = query_type.replace(\"song\", \"track\")\n             ilogger.info(\"Queueing Spotify {}: {}\".format(query_type, query_search))\n             spotify_tracks = search_sp_tracks(query_type, query_search)\n             if spotify_tracks is None or len(spotify_tracks) == 0:\n                 ilogger.error(\"Could not queue Spotify {}: {}\".format(query_type, query_search))\n                 return []\n             ilogger.info(\"Queued Spotify {}: {}\".format(query_type, query_search))\n             return spotify_tracks\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Error queueing from Spotify\")\n             return []\n     elif args[0].lower() in [\"sc\", \"soundcloud\"]:\n         if scclient is None:\n             ilogger.error(\"Host does not support SoundCloud\")\n             return []\n         try:\n             requests = [\"song\", \"songs\", \"track\", \"tracks\", \"user\", \"playlist\", \"tagged\", \"genre\"]\n             if len(args) > 2 and args[1] in requests:\n                 query_type = args[1].lower()\n                 query_search = \" \".join(args[2:])\n             else:\n                 query_type = \"track\"\n                 query_search = \" \".join(args[1:])\n             query_type = query_type.replace(\"song\", \"track\")\n             ilogger.info(\"Queueing SoundCloud {}: {}\".format(query_type, query_search))\n             soundcloud_tracks = search_sc_tracks(query_type, query_search)\n             ilogger.info(\"Queued SoundCloud {}: {}\".format(query_type, query_search))\n             return soundcloud_tracks\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Could not queue from SoundCloud\")\n             return []\n     elif args[0].lower() in [\"yt\", \"youtube\"] and ytdiscoveryapi is not None:\n         if ytdiscoveryapi is None:\n             ilogger.error(\"Host does not support YouTube\")\n             return []\n         try:\n             query_search = \" \".join(args[1:])\n             ilogger.info(\"Queued Youtube search: {}\".format(query_search))\n             return get_ytvideos(query_search, ilogger)\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Could not queue YouTube search\")\n             return []\n     if ytdiscoveryapi is not None:\n         ilogger.info(\"Queued YouTube search: {}\".format(query))\n         return get_ytvideos(query, ilogger)\n     else:\n         ilogger.error(\"Host does not support YouTube\".format(query))\n         return []", "query": "parse query string in url"}
{"id": "https://github.com/etcher-be/elib_run/blob/c9d8ba9f067ab90c5baa27375a92b23f1b97cdde/elib_run/_find_exe.py#L18-L62", "method_name": "find_executable", "code": "def find_executable(executable: str, *paths: str) -> typing.Optional[Path]:\n     if not executable.endswith(\".exe\"):\n         executable = f\"{executable}.exe\"\n     if executable in _KNOWN_EXECUTABLES:\n         return _KNOWN_EXECUTABLES[executable]\n     output = f\"{executable}\"\n     if not paths:\n         path = os.environ[\"PATH\"]\n         paths = tuple([str(Path(sys.exec_prefix, \"Scripts\").absolute())] + path.split(os.pathsep))\n     executable_path = Path(executable).absolute()\n     if not executable_path.is_file():\n         for path_ in paths:\n             executable_path = Path(path_, executable).absolute()\n             if executable_path.is_file():\n                 break\n         else:\n             _LOGGER.error(\"%s -> not found\", output)\n             return None\n     _KNOWN_EXECUTABLES[executable] = executable_path\n     _LOGGER.info(\"%s -> %s\", output, str(executable_path))\n     return executable_path", "query": "get executable path"}
{"id": "https://github.com/albertyw/csv-ical/blob/cdb55a226cd0cb6cc214d896a6cea41a5b92c9ed/csv_ical/convert.py#L92-L97", "method_name": "save_csv", "code": "def save_csv(self, csv_location):  \n         with open(csv_location, \"w\") as csv_handle:\n             writer = csv.writer(csv_handle)\n             for row in self.csv_data:\n                 writer.writerow(row)", "query": "convert json to csv"}
{"id": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341", "method_name": "check_checkbox", "code": "def check_checkbox(step, value):\n     with AssertContextManager(step):\n         check_box = find_field(world.browser, \"checkbox\", value)\n         if not check_box.is_selected():\n             check_box.click()", "query": "make the checkbox checked"}
{"id": "https://github.com/toejough/pimento/blob/cdb00a93976733aa5521f8504152cedeedfc711a/pimento/__init__.py#L298-L311", "method_name": "_exact_match", "code": "def _exact_match(response, matches, insensitive, fuzzy):\n     for match in matches:\n         if response == match:\n             return match\n         elif insensitive and response.lower() == match.lower():\n             return match\n         elif fuzzy and _exact_fuzzy_match(response, match, insensitive):\n             return match\n     else:\n         return None", "query": "fuzzy match ranking"}
{"id": "https://github.com/datajoint/datajoint-python/blob/4f29bb154a7ed2b8b64b4d3a9c8be4c16b39621c/datajoint/connection.py#L20-L44", "method_name": "conn", "code": "def conn(host=None, user=None, password=None, init_fun=None, reset=False):\n     if not hasattr(conn, \"connection\") or reset:\n         host = host if host is not None else config[\"database.host\"]\n         user = user if user is not None else config[\"database.user\"]\n         password = password if password is not None else config[\"database.password\"]\n         if user is None:  \n             user = input(\"Please enter DataJoint username: \")\n         if password is None:  \n             password = getpass(prompt=\"Please enter DataJoint password: \")\n         init_fun = init_fun if init_fun is not None else config[\"connection.init_function\"]\n         conn.connection = Connection(host, user, password, init_fun)\n     return conn.connection", "query": "postgresql connection"}
{"id": "https://github.com/danielhrisca/asammdf/blob/3c7a1fd19c957ceebe4dcdbb2abf00806c2bdb66/asammdf/mdf.py#L1821-L2190", "method_name": "concatenate", "code": "def concatenate(files, version=\"4.10\", sync=True, add_samples_origin=False, **kwargs):\n         if not files:\n             raise MdfException(\"No files given for merge\")\n         callback = kwargs.get(\"callback\", None)\n         if callback:\n             callback(0, 100)\n         mdf_nr = len(files)\n         versions = []\n         if sync:\n             timestamps = []\n             for file in files:\n                 if isinstance(file, MDF):\n                     timestamps.append(file.header.start_time)\n                     versions.append(file.version)\n                 else:\n                     with open(file, \"rb\") as mdf:\n                         mdf.seek(64)\n                         blk_id = mdf.read(2)\n                         if blk_id == b\"HD\":\n                             header = HeaderV3\n                             versions.append(\"3.00\")\n                         else:\n                             versions.append(\"4.00\")\n                             blk_id += mdf.read(2)\n                             if blk_id == b\"\n                                 header = HeaderV4\n                             else:\n                                 raise MdfException(f\"\"{file}\" is not a valid MDF file\")\n                         header = header(address=64, stream=mdf)\n                         timestamps.append(header.start_time)\n             try:\n                 oldest = min(timestamps)\n             except TypeError:\n                 timestamps = [\n                     timestamp.astimezone(timezone.utc)\n                     for timestamp in timestamps\n                 ]\n                 oldest = min(timestamps)\n             offsets = [(timestamp - oldest).total_seconds() for timestamp in timestamps]\n             offsets = [offset if offset > 0 else 0 for offset in offsets]\n         else:\n             file = files[0]\n             if isinstance(file, MDF):\n                 oldest = file.header.start_time\n                 versions.append(file.version)\n             else:\n                 with open(file, \"rb\") as mdf:\n                     mdf.seek(64)\n                     blk_id = mdf.read(2)\n                     if blk_id == b\"HD\":\n                         versions.append(\"3.00\")\n                         header = HeaderV3\n                     else:\n                         versions.append(\"4.00\")\n                         blk_id += mdf.read(2)\n                         if blk_id == b\"\n                             header = HeaderV4\n                         else:\n                             raise MdfException(f\"\"{file}\" is not a valid MDF file\")\n                     header = header(address=64, stream=mdf)\n                     oldest = header.start_time\n             offsets = [0 for _ in files]\n         version = validate_version_argument(version)\n         merged = MDF(version=version, callback=callback)\n         merged.header.start_time = oldest\n         encodings = []\n         included_channel_names = []\n         if add_samples_origin:\n             origin_conversion = {}\n             for i, mdf in enumerate(files):\n                 origin_conversion[f\"val_{i}\"] = i\n                 if isinstance(mdf, MDF):\n                     origin_conversion[f\"text_{i}\"] = str(mdf.name)\n                 else:\n                     origin_conversion[f\"text_{i}\"] = str(mdf)\n             origin_conversion = from_dict(origin_conversion)\n         for mdf_index, (offset, mdf) in enumerate(zip(offsets, files)):\n             if not isinstance(mdf, MDF):\n                 mdf = MDF(mdf)\n             try:\n                 for can_id, info in mdf.can_logging_db.items():\n                     if can_id not in merged.can_logging_db:\n                         merged.can_logging_db[can_id] = {}\n                     merged.can_logging_db[can_id].update(info)\n             except AttributeError:\n                 pass\n             if mdf_index == 0:\n                 last_timestamps = [None for gp in mdf.groups]\n                 groups_nr = len(mdf.groups)\n             cg_nr = -1\n             for i, group in enumerate(mdf.groups):\n                 included_channels = mdf._included_channels(i)\n                 if mdf_index == 0:\n                     included_channel_names.append(\n                         [group.channels[k].name for k in included_channels]\n                     )\n                 else:\n                     names = [group.channels[k].name for k in included_channels]\n                     if names != included_channel_names[i]:\n                         if sorted(names) != sorted(included_channel_names[i]):\n                             raise MdfException(f\"internal structure of file {mdf_index} is different\")\n                         else:\n                             logger.warning(\n                                 f\"Different channel order in channel group {i} of file {mdf_index}.\"\n                                 \" Data can be corrupted if the there are channels with the same \"\n                                 \"name in this channel group\"\n                             )\n                             included_channels = [\n                                 mdf._validate_channel_selection(\n                                     name=name_,\n                                     group=i,\n                                 )[1]\n                                 for name_ in included_channel_names[i]\n                             ]\n                 if included_channels:\n                     cg_nr += 1\n                 else:\n                     continue\n                 channels_nr = len(group.channels)\n                 y_axis = MERGE\n                 idx = np.searchsorted(CHANNEL_COUNT, channels_nr, side=\"right\") - 1\n                 if idx < 0:\n                     idx = 0\n                 read_size = y_axis[idx]\n                 idx = 0\n                 last_timestamp = last_timestamps[i]\n                 first_timestamp = None\n                 original_first_timestamp = None\n                 if read_size:\n                     mdf.configure(read_fragment_size=int(read_size))\n                 parents, dtypes = mdf._prepare_record(group)\n                 data = mdf._load_data(group)\n                 for fragment in data:\n                     if dtypes.itemsize:\n                         group.record = np.core.records.fromstring(\n                             fragment[0], dtype=dtypes\n                         )\n                     else:\n                         group.record = None\n                     if mdf_index == 0 and idx == 0:\n                         encodings_ = []\n                         encodings.append(encodings_)\n                         signals = []\n                         for j in included_channels:\n                             sig = mdf.get(\n                                 group=i,\n                                 index=j,\n                                 data=fragment,\n                                 raw=True,\n                                 ignore_invalidation_bits=True,\n                                 copy_master=False,\n                             )\n                             if version < \"4.00\":\n                                 if sig.samples.dtype.kind == \"S\":\n                                     encodings_.append(sig.encoding)\n                                     strsig = mdf.get(\n                                         group=i,\n                                         index=j,\n                                         samples_only=True,\n                                         ignore_invalidation_bits=True,\n                                     )[0]\n                                     sig.samples = sig.samples.astype(strsig.dtype)\n                                     del strsig\n                                     if sig.encoding != \"latin-1\":\n                                         if sig.encoding == \"utf-16-le\":\n                                             sig.samples = (\n                                                 sig.samples.view(np.uint16)\n                                                 .byteswap()\n                                                 .view(sig.samples.dtype)\n                                             )\n                                             sig.samples = encode(\n                                                 decode(sig.samples, \"utf-16-be\"),\n                                                 \"latin-1\",\n                                             )\n                                         else:\n                                             sig.samples = encode(\n                                                 decode(sig.samples, sig.encoding),\n                                                 \"latin-1\",\n                                             )\n                                 else:\n                                     encodings_.append(None)\n                             if not sig.samples.flags.writeable:\n                                 sig.samples = sig.samples.copy()\n                             signals.append(sig)\n                         if signals and len(signals[0]):\n                             if offset > 0:\n                                 timestamps = sig[0].timestamps + offset\n                                 for sig in signals:\n                                     sig.timestamps = timestamps\n                             last_timestamp = signals[0].timestamps[-1]\n                             first_timestamp = signals[0].timestamps[0]\n                             original_first_timestamp = first_timestamp\n                         if add_samples_origin:\n                             if signals:\n                                 _s = signals[-1]\n                                 signals.append(\n                                     Signal(\n                                         samples=np.ones(len(_s), dtype=\"<u2\")*mdf_index,\n                                         timestamps=_s.timestamps,\n                                         conversion=origin_conversion,\n                                         name=\"__samples_origin\",\n                                     )\n                                 )\n                                 _s = None\n                         if signals:\n                             merged.append(signals, common_timebase=True)\n                             try:\n                                 if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT:\n                                     merged.groups[-1].channel_group.flags = group.channel_group.flags\n                                     merged.groups[-1].channel_group.acq_name = group.channel_group.acq_name\n                                     merged.groups[-1].channel_group.acq_source = group.channel_group.acq_source\n                                     merged.groups[-1].channel_group.comment = group.channel_group.comment\n                             except AttributeError:\n                                 pass\n                         else:\n                             break\n                         idx += 1\n                     else:\n                         master = mdf.get_master(i, fragment, copy_master=False)\n                         _copied = False\n                         if len(master):\n                             if original_first_timestamp is None:\n                                 original_first_timestamp = master[0]\n                             if offset > 0:\n                                 master = master + offset\n                                 _copied = True\n                             if last_timestamp is None:\n                                 last_timestamp = master[-1]\n                             else:\n                                 if last_timestamp >= master[0]:\n                                     if len(master) >= 2:\n                                         delta = master[1] - master[0]\n                                     else:\n                                         delta = 0.001\n                                     if _copied:\n                                         master -= master[0]\n                                     else:\n                                         master = master - master[0]\n                                         _copied = True\n                                     master += last_timestamp + delta\n                                 last_timestamp = master[-1]\n                             signals = [(master, None)]\n                             for k, j in enumerate(included_channels):\n                                 sig = mdf.get(\n                                     group=i,\n                                     index=j,\n                                     data=fragment,\n                                     raw=True,\n                                     samples_only=True,\n                                     ignore_invalidation_bits=True,\n                                 )\n                                 signals.append(sig)\n                                 if version < \"4.00\":\n                                     encoding = encodings[i][k]\n                                     samples = sig[0]\n                                     if encoding:\n                                         if encoding != \"latin-1\":\n                                             if encoding == \"utf-16-le\":\n                                                 samples = (\n                                                     samples.view(np.uint16)\n                                                     .byteswap()\n                                                     .view(samples.dtype)\n                                                 )\n                                                 samples = encode(\n                                                     decode(samples, \"utf-16-be\"),\n                                                     \"latin-1\",\n                                                 )\n                                             else:\n                                                 samples = encode(\n                                                     decode(samples, encoding), \"latin-1\"\n                                                 )\n                                             sig.samples = samples\n                             if signals:\n                                 if add_samples_origin:\n                                     _s = signals[-1][0]\n                                     signals.append(\n                                         (np.ones(len(_s), dtype=\"<u2\")*mdf_index, None)\n                                     )\n                                     _s = None\n                                 merged.extend(cg_nr, signals)\n                             if first_timestamp is None:\n                                 first_timestamp = master[0]\n                         idx += 1\n                     group.record = None\n                 last_timestamps[i] = last_timestamp\n             if callback:\n                 callback(i + 1 + mdf_index * groups_nr, groups_nr * mdf_nr)\n             if MDF._terminate:\n                 return\n             merged._transfer_events(mdf)\n         return merged", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/lawsie/guizero/blob/84c7f0b314fa86f9fc88eb11c9a0f6c4b57155e2/examples/binary_counter.py#L6-L11", "method_name": "to_str", "code": "def to_str(n,base):\n     convert_string = \"0123456789ABCDEF\"\n     if n < base:\n         return convert_string[n]\n     else:\n         return to_str(n//base,base) + convert_string[n%base]", "query": "convert string to number"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mssql_hook.py#L38-L49", "method_name": "get_conn", "code": "def get_conn(self):\n         conn = self.get_connection(self.mssql_conn_id)\n         conn = pymssql.connect(\n             server=conn.host,\n             user=conn.login,\n             password=conn.password,\n             database=self.schema or conn.schema,\n             port=conn.port)\n         return conn", "query": "postgresql connection"}
{"id": "https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2492-L2508", "method_name": "distinct", "code": "def distinct(self):\n         d = collections.defaultdict(set)\n         for i in range(self.shape[1]):\n             k = hash(self.values[:, i].tobytes())\n             d[k].add(i)\n         return sorted(d.values(), key=len, reverse=True)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/Jaymon/endpoints/blob/2f1c4ae2c69a168e69447d3d8395ada7becaa5fb/endpoints/interface/uwsgi/client.py#L265-L307", "method_name": "recv_raw", "code": "def recv_raw(self, timeout, opcodes, **kwargs):\n         orig_timeout = self.get_timeout(timeout)\n         timeout = orig_timeout\n         while timeout > 0.0:\n             start = time.time()\n             if not self.connected: self.connect(timeout=timeout, **kwargs)\n             with self.wstimeout(timeout, **kwargs) as timeout:\n                 logger.debug(\"{} waiting to receive for {} seconds\".format(self.client_id, timeout))\n                 try:\n                     opcode, data = self.ws.recv_data()\n                     if opcode in opcodes:\n                         timeout = 0.0\n                         break\n                     else:\n                         if opcode == websocket.ABNF.OPCODE_CLOSE:\n                             raise websocket.WebSocketConnectionClosedException()\n                 except websocket.WebSocketTimeoutException:\n                     pass\n                 except websocket.WebSocketConnectionClosedException:\n                     try:\n                         self.ws.shutdown()\n                     except AttributeError:\n                         pass\n             if timeout:\n                 stop = time.time()\n                 timeout -= (stop - start)\n             else:\n                 break\n         if timeout < 0.0:\n             raise IOError(\"recv timed out in {} seconds\".format(orig_timeout))\n         return opcode, data", "query": "socket recv timeout"}
{"id": "https://github.com/release-engineering/productmd/blob/49256bf2e8c84124f42346241140b986ad7bfc38/productmd/common.py#L302-L315", "method_name": "parse_file", "code": "def parse_file(self, f):\n         if hasattr(f, \"seekable\"):\n             if f.seekable():\n                 f.seek(0)\n         elif hasattr(f, \"seek\"):\n             f.seek(0)\n         if six.PY3 and isinstance(f, six.moves.http_client.HTTPResponse):\n             reader = codecs.getreader(\"utf-8\")\n             parser = json.load(reader(f))\n         else:\n             parser = json.load(f)\n         return parser", "query": "parse json file"}
{"id": "https://github.com/bram85/topydo/blob/b59fcfca5361869a6b78d4c9808c7c6cd0a18b58/topydo/lib/Utils.py#L28-L46", "method_name": "date_string_to_date", "code": "def date_string_to_date(p_date):\n     result = None\n     if p_date:\n         parsed_date = re.match(r\"(\\d{4})-(\\d{2})-(\\d{2})\", p_date)\n         if parsed_date:\n             result = date(\n                 int(parsed_date.group(1)),  \n                 int(parsed_date.group(2)),  \n                 int(parsed_date.group(3))   \n             )\n         else:\n             raise ValueError\n     return result", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/ofek/pypinfo/blob/48d56e690d7667ae5854752c3a2dc07e321d5637/pypinfo/core.py#L65-L70", "method_name": "format_date", "code": "def format_date(date, timestamp_format):\n     try:\n         date = DATE_ADD.format(int(date))\n     except ValueError:\n         date = timestamp_format.format(date)\n     return date", "query": "format date"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L578-L591", "method_name": "get_table", "code": "def get_table(self, table_name, db=\"default\"):\n         if db == \"default\" and \".\" in table_name:\n             db, table_name = table_name.split(\".\")[:2]\n         with self.metastore as client:\n             return client.get_table(dbname=db, tbl_name=table_name)", "query": "get database table name"}
{"id": "https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/filter.py#L459-L463", "method_name": "params", "code": "def params(self):\n         if not any(filter.params for filter in self):\n             return None\n         else:\n             return Array(filter.params or Null() for filter in self)", "query": "filter array"}
{"id": "https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/Dumper/file.py#L109-L114", "method_name": "binary_file", "code": "def binary_file(self, file=None):\n         if file is None:\n             file = BytesIO()\n         self._binary_file(file)\n         return file", "query": "parse binary file to custom class"}
{"id": "https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L59-L63", "method_name": "aes_ctr_encrypt", "code": "def aes_ctr_encrypt(plain_text: bytes, key: bytes):\n         cipher = AES.new(key=key, mode=AES.MODE_CTR)\n         cipher_text = cipher.encrypt(plain_text)\n         nonce = cipher.nonce\n         return nonce, cipher_text", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/stickers.py#L281-L291", "method_name": "pdf_from_post", "code": "def pdf_from_post(self):\n         html = self.request.form.get(\"html\")\n         style = self.request.form.get(\"style\")\n         reporthtml = \"<html><head>{0}</head><body>{1}</body></html>\"\n         reporthtml = reporthtml.format(style, html)\n         reporthtml = safe_unicode(reporthtml).encode(\"utf-8\")\n         pdf_fn = tempfile.mktemp(suffix=\".pdf\")\n         pdf_file = createPdf(htmlreport=reporthtml, outfile=pdf_fn)\n         return pdf_file", "query": "convert html to pdf"}
{"id": "https://github.com/tensorflow/tensorboard/blob/8e5f497b48e40f2a774f85416b8a35ac0693c35e/tensorboard/plugins/histogram/histograms_demo.py#L32-L103", "method_name": "run_all", "code": "def run_all(logdir, verbose=False, num_summaries=400):\n   del verbose\n   tf.compat.v1.set_random_seed(0)\n   k = tf.compat.v1.placeholder(tf.float32)\n   mean_moving_normal = tf.random.normal(shape=[1000], mean=(5*k), stddev=1)\n   histogram_summary.op(\"normal/moving_mean\",\n                        mean_moving_normal,\n                        description=\"A normal distribution whose mean changes \"\n                                    \"over time.\")\n   shrinking_normal = tf.random.normal(shape=[1000], mean=0, stddev=1-(k))\n   histogram_summary.op(\"normal/shrinking_variance\", shrinking_normal,\n                        description=\"A normal distribution whose variance \"\n                                    \"shrinks over time.\")\n   normal_combined = tf.concat([mean_moving_normal, shrinking_normal], 0)\n   histogram_summary.op(\"normal/bimodal\", normal_combined,\n                        description=\"A combination of two normal distributions, \"\n                                    \"one with a moving mean and one with  \"\n                                    \"shrinking variance. The result is a \"\n                                    \"distribution that starts as unimodal and \"\n                                    \"becomes more and more bimodal over time.\")\n   gamma = tf.random.gamma(shape=[1000], alpha=k)\n   histogram_summary.op(\"gamma\", gamma,\n                        description=\"A gamma distribution whose shape \"\n                                    \"parameter, 伪, changes over time.\")\n   poisson = tf.compat.v1.random_poisson(shape=[1000], lam=k)\n   histogram_summary.op(\"poisson\", poisson,\n                        description=\"A Poisson distribution, which only \"\n                                    \"takes on integer values.\")\n   uniform = tf.random.uniform(shape=[1000], maxval=k*10)\n   histogram_summary.op(\"uniform\", uniform,\n                        description=\"A simple uniform distribution.\")\n   all_distributions = [mean_moving_normal, shrinking_normal,\n                        gamma, poisson, uniform]\n   all_combined = tf.concat(all_distributions, 0)\n   histogram_summary.op(\"all_combined\", all_combined,\n                        description=\"An amalgamation of five distributions: a \"\n                                    \"uniform distribution, a gamma \"\n                                    \"distribution, a Poisson distribution, and \"\n                                    \"two normal distributions.\")\n   summaries = tf.compat.v1.summary.merge_all()\n   sess = tf.compat.v1.Session()\n   writer = tf.summary.FileWriter(logdir)\n   N = num_summaries\n   for step in xrange(N):\n     k_val = step/float(N)\n     summ = sess.run(summaries, feed_dict={k: k_val})\n     writer.add_summary(summ, global_step=step)", "query": "normal distribution"}
{"id": "https://github.com/wecatch/app-turbo/blob/75faf97371a9a138c53f92168d0a486636cb8a9c/turbo/session.py#L185-L193", "method_name": "_set_cookie", "code": "def _set_cookie(self, name, value):\n         cookie_domain = self._config.cookie_domain\n         cookie_path = self._config.cookie_path\n         cookie_expires = self._config.cookie_expires\n         if self._config.secure:\n             return self.handler.set_secure_cookie(\n                 name, value, expires_days=cookie_expires / (3600 * 24), domain=cookie_domain, path=cookie_path)\n         else:\n             return self.handler.set_cookie(name, value, expires=cookie_expires, domain=cookie_domain, path=cookie_path)", "query": "create cookie"}
{"id": "https://github.com/sentinel-hub/eo-learn/blob/b8c390b9f553c561612fe9eb64e720611633a035/ml_tools/eolearn/ml_tools/validator.py#L201-L208", "method_name": "confusion_matrix", "code": "def confusion_matrix(self):\n         confusion_matrix = self.pixel_classification_sum.astype(np.float)\n         confusion_matrix = np.divide(confusion_matrix.T, self.pixel_truth_sum.T).T\n         return confusion_matrix * 100.0", "query": "confusion matrix"}
{"id": "https://github.com/eliangcs/pystock-crawler/blob/8b803c8944f36af46daf04c6767a74132e37a101/pystock_crawler/spiders/yahoo.py#L12-L16", "method_name": "parse_date", "code": "def parse_date(date_str):\n     if date_str:\n         date = datetime.strptime(date_str, \"%Y%m%d\")\n         return date.year, date.month - 1, date.day\n     return \"\", \"\", \"\"", "query": "string to date"}
{"id": "https://github.com/honzajavorek/redis-collections/blob/07ca8efe88fb128f7dc7319dfa6a26cd39b3776b/redis_collections/lists.py#L482-L498", "method_name": "reverse", "code": "def reverse(self):\n         def reverse_trans(pipe):\n             if self.writeback:\n                 self._sync_helper(pipe)\n             n = self.__len__(pipe)\n             for i in range(n // 2):\n                 left = pipe.lindex(self.key, i)\n                 right = pipe.lindex(self.key, n - i - 1)\n                 pipe.lset(self.key, i, right)\n                 pipe.lset(self.key, n - i - 1, left)\n         self._transaction(reverse_trans)", "query": "reverse a string"}
{"id": "https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/system/time_helper.py#L78-L82", "method_name": "day_to_month", "code": "def day_to_month(timeperiod):\n     t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN)\n     return t.strftime(SYNERGY_MONTHLY_PATTERN)", "query": "convert a date string into yyyymmdd"}
{"id": "https://github.com/LIVVkit/LIVVkit/blob/680120cd437e408673e62e535fc0a246c7fc17db/livvkit/components/validation.py#L141-L149", "method_name": "_print_summary", "code": "def _print_summary(module, case, summary):\n     try:\n         try:\n             module.print_summary(summary[case])\n         except TypeError:\n             module.print_summary(case, summary[case])\n     except (NotImplementedError, AttributeError):\n         print(\"    Ran \" + case + \"!\")\n         print(\"\")", "query": "print model summary"}
{"id": "https://github.com/firstprayer/monsql/blob/6285c15b574c8664046eae2edfeb548c7b173efd/monsql/wrapper_postgresql.py#L48-L50", "method_name": "get_table_obj", "code": "def get_table_obj(self, name):\n        table = PostgreSQLTable(db=self.db, name=name, mode=self.mode)\n        return table", "query": "get database table name"}
{"id": "https://github.com/noahbenson/pimms/blob/9051b86d6b858a7a13511b72c48dc21bc903dab2/pimms/calculation.py#L516-L573", "method_name": "_run_node", "code": "def _run_node(self, node):\n         res = None\n         if (\"memoize\" in self.afferents and self.afferents[\"memoize\"]) and node.memoize:\n             memdat = self.plan._memoized_data\n             try:\n                 h = qhash({k:self.afferents[k] for k in self.plan.afferent_dependencies[node.name]})\n                 ho = (node.name, h)\n                 if ho in memdat:\n                     res = memdat[ho]\n                     h = None\n                     ho = None\n                 else:\n                     cpath = self.afferents[\"cache_directory\"] \\\n                             if node.cache and \"cache_directory\" in self.afferents else \\\n                             None\n                     if cpath is not None:\n                         ureg = self.afferents[\"unit_registry\"] \\\n                                if \"unit_registry\" in self.afferents else \\\n                                \"pimms\"\n                         cpath = os.path.join(cpath, node.name, (\"0\" + str(-h)) if h < 0 else str(h))\n                         try:\n                             res = self._uncache(cpath, node, ureg)\n                             cpath = None\n                         except: pass\n             except:\n                 h = None\n                 ho = None\n                 res = None\n         else:\n             h = None\n             ho = None\n         if res is None: res = node(self)\n         effs = reduce(lambda m,v: m.set(v[0],v[1]), six.iteritems(res), self.efferents)\n         object.__setattr__(self, \"efferents\", effs)\n         if h is not None:\n             memdat[ho] = res\n             if cpath is not None:\n                 try:\n                     self._cache(cpath, res)\n                 except: pass", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/sendgrid/sendgrid-python/blob/266c2abde7a35dfcce263e06bedc6a0bbdebeac9/examples/helpers/stats/stats_example.py#L13-L14", "method_name": "pprint_json", "code": "def pprint_json(json_raw):\n    print(json.dumps(json.loads(json_raw), indent=2, sort_keys=True))", "query": "pretty print json"}
{"id": "https://github.com/noahbenson/pimms/blob/9051b86d6b858a7a13511b72c48dc21bc903dab2/pimms/calculation.py#L516-L573", "method_name": "_run_node", "code": "def _run_node(self, node):\n         res = None\n         if (\"memoize\" in self.afferents and self.afferents[\"memoize\"]) and node.memoize:\n             memdat = self.plan._memoized_data\n             try:\n                 h = qhash({k:self.afferents[k] for k in self.plan.afferent_dependencies[node.name]})\n                 ho = (node.name, h)\n                 if ho in memdat:\n                     res = memdat[ho]\n                     h = None\n                     ho = None\n                 else:\n                     cpath = self.afferents[\"cache_directory\"] \\\n                             if node.cache and \"cache_directory\" in self.afferents else \\\n                             None\n                     if cpath is not None:\n                         ureg = self.afferents[\"unit_registry\"] \\\n                                if \"unit_registry\" in self.afferents else \\\n                                \"pimms\"\n                         cpath = os.path.join(cpath, node.name, (\"0\" + str(-h)) if h < 0 else str(h))\n                         try:\n                             res = self._uncache(cpath, node, ureg)\n                             cpath = None\n                         except: pass\n             except:\n                 h = None\n                 ho = None\n                 res = None\n         else:\n             h = None\n             ho = None\n         if res is None: res = node(self)\n         effs = reduce(lambda m,v: m.set(v[0],v[1]), six.iteritems(res), self.efferents)\n         object.__setattr__(self, \"efferents\", effs)\n         if h is not None:\n             memdat[ho] = res\n             if cpath is not None:\n                 try:\n                     self._cache(cpath, res)\n                 except: pass", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/tmoerman/arboreto/blob/3ff7b6f987b32e5774771751dea646fa6feaaa52/arboreto/core.py#L105-L141", "method_name": "fit_model", "code": "def fit_model(regressor_type,\n               regressor_kwargs,\n               tf_matrix,\n               target_gene_expression,\n               early_stop_window_length=EARLY_STOP_WINDOW_LENGTH,\n               seed=DEMON_SEED):\n     regressor_type = regressor_type.upper()\n     assert tf_matrix.shape[0] == len(target_gene_expression)\n     def do_sklearn_regression():\n         regressor = SKLEARN_REGRESSOR_FACTORY[regressor_type](random_state=seed, **regressor_kwargs)\n         with_early_stopping = is_oob_heuristic_supported(regressor_type, regressor_kwargs)\n         if with_early_stopping:\n             regressor.fit(tf_matrix, target_gene_expression, monitor=EarlyStopMonitor(early_stop_window_length))\n         else:\n             regressor.fit(tf_matrix, target_gene_expression)\n         return regressor\n     if is_sklearn_regressor(regressor_type):\n         return do_sklearn_regression()\n     else:\n         raise ValueError(\"Unsupported regressor type: {0}\".format(regressor_type))", "query": "linear regression"}
{"id": "https://github.com/abilian/abilian-core/blob/0a71275bf108c3d51e13ca9e093c0249235351e3/abilian/web/admin/panels/audit.py#L29-L33", "method_name": "format_date_for_input", "code": "def format_date_for_input(date):\n     date_fmt = get_locale().date_formats[\"short\"].pattern\n     date_fmt = date_fmt.replace(\"MMMM\", \"MM\").replace(\"MMM\", \"MM\")\n     return format_date(date, date_fmt)", "query": "format date"}
{"id": "https://github.com/rberrelleza/511-transit/blob/ab676d6e3b57a073405cbfa2ffe1a57b85808fd1/fiveoneone/model.py#L18-L27", "method_name": "to_bool", "code": "def to_bool(self, value):\n \t \tif value == None:\n \t \t\treturn False\n \t \telif isinstance(value, bool):\n \t \t\treturn value\n \t \telse:\n \t \t\tif str(value).lower() in [\"true\", \"1\", \"yes\"]:\n \t \t\t\treturn True\n \t \t\telse:\n \t \t\t\treturn False", "query": "convert int to bool"}
{"id": "https://github.com/nugget/python-insteonplm/blob/65548041f1b0729ae1ae904443dd81b0c6cbf1bf/insteonplm/tools.py#L877-L903", "method_name": "do_set_workdir", "code": "def do_set_workdir(self, args):\n         params = args.split()\n         workdir = None\n         try:\n             workdir = params[0]\n         except IndexError:\n             _LOGGING.error(\"Device name required.\")\n             self.do_help(\"set_workdir\")\n         if workdir:\n             self.tools.workdir = workdir", "query": "set working directory"}
{"id": "https://github.com/cmbruns/pyopenvr/blob/68395d26bb3df6ab1f0f059c38d441f962938be6/src/openvr/gl_renderer.py#L17-L32", "method_name": "matrixForOpenVrMatrix", "code": "def matrixForOpenVrMatrix(mat):\n     if len(mat.m) == 4: \n         result = numpy.matrix(\n                 ((mat.m[0][0], mat.m[1][0], mat.m[2][0], mat.m[3][0]),\n                  (mat.m[0][1], mat.m[1][1], mat.m[2][1], mat.m[3][1]), \n                  (mat.m[0][2], mat.m[1][2], mat.m[2][2], mat.m[3][2]), \n                  (mat.m[0][3], mat.m[1][3], mat.m[2][3], mat.m[3][3]),)\n             , numpy.float32)\n     elif len(mat.m) == 3: \n         result = numpy.matrix(\n                 ((mat.m[0][0], mat.m[1][0], mat.m[2][0], 0.0),\n                  (mat.m[0][1], mat.m[1][1], mat.m[2][1], 0.0), \n                  (mat.m[0][2], mat.m[1][2], mat.m[2][2], 0.0), \n                  (mat.m[0][3], mat.m[1][3], mat.m[2][3], 1.0),)\n             , numpy.float32)\n     return result", "query": "matrix multiply"}
{"id": "https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/tree_graph.py#L39-L44", "method_name": "get_parents", "code": "def get_parents(self, node):\n         parent = self.parents.get(node)\n         if parent == None:\n             return set()\n         else:\n             return {parent}", "query": "get all parents of xml node"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40", "method_name": "binomial", "code": "def binomial(n,k):\n     if n==k: return 1\n     assert n>k, \"Attempting to call binomial(%d,%d)\" % (n,k)\n     return factorial(n)//(factorial(k)*factorial(n-k))", "query": "binomial distribution"}
{"id": "https://github.com/InQuest/python-sandboxapi/blob/9bad73f453e25d7d23e7b4b1ae927f44a35a5bc3/sandboxapi/joe.py#L37-L51", "method_name": "check", "code": "def check(self, item_id):\n         try:\n             return self.jbx.info(item_id).get(\"status\").lower() == \"finished\"\n         except jbxapi.JoeException:\n             return False\n         return False", "query": "check if a checkbox is checked"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/ipythonconsole/plugin.py#L577-L583", "method_name": "set_working_directory", "code": "def set_working_directory(self, dirname): \n         if dirname: \n             self.main.workingdirectory.chdir(dirname, refresh_explorer=True, \n                                              refresh_console=False)", "query": "set working directory"}
{"id": "https://github.com/couchbase/couchbase-python-client/blob/a7bada167785bf79a29c39f820d932a433a6a535/couchbase/connstr.py#L126-L143", "method_name": "encode", "code": "def encode(self):\n         opt_dict = {}\n         for k, v in self.options.items():\n             opt_dict[k] = v[0]\n         ss = \"{0}://{1}\".format(self.scheme, \",\".join(self.hosts))\n         if self.bucket:\n             ss += \"/\" + self.bucket\n         ss += \"?\" + urlencode(opt_dict).replace(\"%2F\", \"/\")\n         return ss", "query": "encode url"}
{"id": "https://github.com/danielfrg/datasciencebox/blob/6b7aa642c6616a46547035fcb815acc1de605a6f/datasciencebox/core/cloud/instance.py#L80-L83", "method_name": "get_ip", "code": "def get_ip(self):\n         if self._ip is None:\n             self._ip = self.fetch_ip()\n         return self._ip", "query": "get current ip address"}
{"id": "https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/snep/client.py#L51-L74", "method_name": "recv_response", "code": "def recv_response(socket, acceptable_length, timeout):\n     if socket.poll(\"recv\", timeout):\n         snep_response = socket.recv()\n         if len(snep_response) < 6:\n             log.debug(\"snep response initial fragment too short\")\n             return None\n         version, status, length = struct.unpack(\">BBL\", snep_response[:6])\n         if length > acceptable_length:\n             log.debug(\"snep response exceeds acceptable length\")\n             return None\n         if len(snep_response) - 6 < length:\n             socket.send(b\"\\x10\\x00\\x00\\x00\\x00\\x00\")\n             while len(snep_response) - 6 < length:\n                 if socket.poll(\"recv\", timeout):\n                     snep_response += socket.recv()\n                 else:\n                     return None\n         return bytearray(snep_response)", "query": "socket recv timeout"}
{"id": "https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149", "method_name": "_dt_to_epoch", "code": "def _dt_to_epoch(self, dt):\n         if PY2:\n             time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n             return int(time_delta.total_seconds())\n         else:\n             return int(dt.timestamp())", "query": "convert a utc time to epoch"}
{"id": "https://github.com/fboender/ansible-cmdb/blob/ebd960ac10684e8c9ec2b12751bba2c4c9504ab7/lib/mako/filters.py#L159-L174", "method_name": "htmlentityreplace_errors", "code": "def htmlentityreplace_errors(ex):\n     if isinstance(ex, UnicodeEncodeError):\n         bad_text = ex.object[ex.start:ex.end]\n         text = _html_entities_escaper.escape(bad_text)\n         return (compat.text_type(text), ex.end)\n     raise ex", "query": "html entities replace"}
{"id": "https://github.com/pymupdf/PyMuPDF/blob/917f2d83482510e26ba0ff01fd2392c26f3a8e90/fitz/fitz.py#L270-L275", "method_name": "__mul__", "code": "def __mul__(self, m):\n         if hasattr(m, \"__float__\"):\n             return Matrix(self.a * m, self.b * m, self.c * m,\n                           self.d * m, self.e * m, self.f * m)\n         m1 = Matrix(1,1)\n         return m1.concat(self, m)", "query": "matrix multiply"}
{"id": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/regions/knn_classifier_region.py#L684-L688", "method_name": "_getAccuracy", "code": "def _getAccuracy(self):\n     n = self.confusion.shape[0]\n     assert n == self.confusion.shape[1], \"Confusion matrix is non-square.\"\n     return self.confusion[range(n), range(n)].sum(), self.confusion.sum()", "query": "confusion matrix"}
{"id": "https://github.com/jldantas/libmft/blob/65a988605fe7663b788bd81dcb52c0a4eaad1549/libmft/attribute.py#L1599-L1630", "method_name": "_from_binary_reparse", "code": "def _from_binary_reparse(cls, binary_stream):\n     reparse_tag, data_len = cls._REPR.unpack(binary_stream[:cls._REPR.size])\n     reparse_type = ReparseType(reparse_tag & 0x0000FFFF)\n     reparse_flags = ReparseFlags((reparse_tag & 0xF0000000) >> 28)\n     guid = None \n     if reparse_flags & ReparseFlags.IS_MICROSOFT:\n         if reparse_type is ReparseType.SYMLINK:\n             data = SymbolicLink.create_from_binary(binary_stream[cls._REPR.size:])\n         elif reparse_type is ReparseType.MOUNT_POINT:\n             data = JunctionOrMount.create_from_binary(binary_stream[cls._REPR.size:])\n         else:\n             data = binary_stream[cls._REPR.size:].tobytes()\n     else:\n         guid = UUID(bytes_le=binary_stream[cls._REPR.size:cls._REPR.size+16].tobytes())\n         data = binary_stream[cls._REPR.size+16:].tobytes()\n     nw_obj = cls((reparse_type, reparse_flags, data_len, guid, data))\n     _MOD_LOGGER.debug(\"Attempted to unpack REPARSE_POINT from \\\"%s\\\"\nResult: %s\", binary_stream.tobytes(), nw_obj)\n     return nw_obj", "query": "parse binary file to custom class"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40", "method_name": "binomial", "code": "def binomial(n,k):\n     if n==k: return 1\n     assert n>k, \"Attempting to call binomial(%d,%d)\" % (n,k)\n     return factorial(n)//(factorial(k)*factorial(n-k))", "query": "binomial distribution"}
{"id": "https://github.com/snower/TorMySQL/blob/01ff87f03a850a2a6e466700d020878eecbefa5d/tormysql/pool.py#L102-L111", "method_name": "connect", "code": "def connect(self):\n         future = super(RecordQueryConnection, self).connect()\n         origin_query = self._connection.query\n         def query(sql, unbuffered=False):\n             self._last_query_sql = sql\n             return origin_query(sql, unbuffered)\n         self._connection.query = query\n         return future", "query": "connect to sql"}
{"id": "https://github.com/decryptus/httpdis/blob/5d198cdc5558f416634602689b3df2c8aeb34984/httpdis/httpdis.py#L760-L769", "method_name": "set_cookie", "code": "def set_cookie(self, name, value = \"\", expires = 0, path = \"/\", domain = \"\", secure = False, http_only = False):\n         cook                    = Cookie.SimpleCookie()\n         cook[name]              = value\n         cook[name][\"expires\"]   = expires\n         cook[name][\"path\"]      = path\n         cook[name][\"domain\"]    = domain\n         cook[name][\"secure\"]    = secure\n         cook[name][\"httponly\"]  = http_only\n         self.send_header(\"Set-Cookie\", cook.output(header = \"\"))", "query": "create cookie"}
{"id": "https://github.com/istresearch/scrapy-cluster/blob/13aaed2349af5d792d6bcbfcadc5563158aeb599/crawler/crawling/distributed_scheduler.py#L275-L298", "method_name": "update_ipaddress", "code": "def update_ipaddress(self):\n         self.old_ip = self.my_ip\n         self.my_ip = \"127.0.0.1\"\n         try:\n             obj = urllib.request.urlopen(settings.get(\"PUBLIC_IP_URL\",\n                                   \"http://ip.42.pl/raw\"))\n             results = self.ip_regex.findall(obj.read())\n             if len(results) > 0:\n                 self.my_ip = results[0]\n             else:\n                 raise IOError(\"Could not get valid IP Address\")\n             obj.close()\n             self.logger.debug(\"Current public ip: {ip}\".format(ip=self.my_ip))\n         except IOError:\n             self.logger.error(\"Could not reach out to get public ip\")\n             pass\n         if self.old_ip != self.my_ip:\n             self.logger.info(\"Changed Public IP: {old} -> {new}\".format(\n                              old=self.old_ip, new=self.my_ip))", "query": "get current ip address"}
{"id": "https://github.com/GibbsConsulting/jupyter-plotly-dash/blob/19e3898372ddf7c1d20292eae1ea0df9e0808fe2/jupyter_plotly_dash/dash_wrapper.py#L104-L120", "method_name": "_repr_html_", "code": "def _repr_html_(self):\n         url = self.get_app_root_url()\n         da_id = self.session_id()\n         comm = locate_jpd_comm(da_id, self, url[1:-1])\n         external = self.add_external_link and \"<hr/><a href=\"{url}\" target=\"_new\">Open in new window</a>\".format(url=url) or \"\"\n         fb = \"frameborder=\"%i\"\" %(self.frame and 1 or 0)\n         iframe =  %{\"url\" : url,\n             \"da_id\" : da_id,\n             \"external\" : external,\n             \"width\" : self.width,\n             \"height\" : self.height,\n             \"frame\": fb,}\n         return iframe", "query": "get html of website"}
{"id": "https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2492-L2508", "method_name": "distinct", "code": "def distinct(self):\n         d = collections.defaultdict(set)\n         for i in range(self.shape[1]):\n             k = hash(self.values[:, i].tobytes())\n             d[k].add(i)\n         return sorted(d.values(), key=len, reverse=True)", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/skorch-dev/skorch/blob/5b9b8b7b7712cb6e5aaa759d9608ea6269d5bcd3/skorch/net.py#L541-L554", "method_name": "initialize", "code": "def initialize(self):\n         self.initialize_virtual_params()\n         self.initialize_callbacks()\n         self.initialize_criterion()\n         self.initialize_module()\n         self.initialize_optimizer()\n         self.initialize_history()\n         self.initialized_ = True\n         return self", "query": "initializing array"}
{"id": "https://github.com/jazzband/sorl-thumbnail/blob/22ccd9781462a820f963f57018ad3dcef85053ed/sorl/thumbnail/helpers.py#L55-L58", "method_name": "deserialize", "code": "def deserialize(s):\n     if isinstance(s, bytes):\n         return json.loads(s.decode(\"utf-8\"))\n     return json.loads(s)", "query": "deserialize json"}
{"id": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/gzip_util.py#L74-L106", "method_name": "compress_file", "code": "def compress_file(fh_, compresslevel=9, chunk_size=1048576):\n     try:\n         bytes_read = int(chunk_size)\n         if bytes_read != chunk_size:\n             raise ValueError\n     except ValueError:\n         raise ValueError(\"chunk_size must be an integer\")\n     try:\n         while bytes_read == chunk_size:\n             buf = BytesIO()\n             with open_fileobj(buf, \"wb\", compresslevel) as ogz:\n                 try:\n                     bytes_read = ogz.write(fh_.read(chunk_size))\n                 except AttributeError:\n                     fh_ = salt.utils.files.fopen(fh_, \"rb\")\n                     bytes_read = ogz.write(fh_.read(chunk_size))\n             yield buf.getvalue()\n     finally:\n         try:\n             fh_.close()\n         except AttributeError:\n             pass", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/chitamoor/Rester/blob/1865b17f70b7c597aeadde2d0907cb1b59f10c0f/rester/apirunner.py#L9-L18", "method_name": "parse_cmdln_args", "code": "def parse_cmdln_args():\n     parser = argparse.ArgumentParser(description=\"Process command line args\")\n     parser.add_argument(\"--log\", help=\"log help\", default=\"INFO\")\n     parser.add_argument(\n         \"--tc\", help=\"tc help\")\n     parser.add_argument(\n         \"--ts\", help=\"ts help\")\n     args = parser.parse_args()\n     return (args.log.upper(), args.tc, args.ts)", "query": "parse command line argument"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/stickers.py#L281-L291", "method_name": "pdf_from_post", "code": "def pdf_from_post(self):\n         html = self.request.form.get(\"html\")\n         style = self.request.form.get(\"style\")\n         reporthtml = \"<html><head>{0}</head><body>{1}</body></html>\"\n         reporthtml = reporthtml.format(style, html)\n         reporthtml = safe_unicode(reporthtml).encode(\"utf-8\")\n         pdf_fn = tempfile.mktemp(suffix=\".pdf\")\n         pdf_file = createPdf(htmlreport=reporthtml, outfile=pdf_fn)\n         return pdf_file", "query": "convert html to pdf"}
{"id": "https://github.com/pikepdf/pikepdf/blob/07154f4dec007e2e9c0c6a8c07b964fd06bc5f77/src/pikepdf/_methods.py#L76-L83", "method_name": "_single_page_pdf", "code": "def _single_page_pdf(page):\n     pdf = Pdf.new()\n     pdf.pages.append(page)\n     bio = BytesIO()\n     pdf.save(bio)\n     bio.seek(0)\n     return bio.read()", "query": "convert html to pdf"}
{"id": "https://github.com/nickolas360/librecaptcha/blob/bd247082fd98118be220f326e5cd162e6d926655/librecaptcha/recaptcha.py#L354-L380", "method_name": "find_challenge_goal", "code": "def find_challenge_goal(self, id, raw=False):\n         start = 0\n         matching_strings = []\n         def try_find():\n             nonlocal start\n             index = self.js_strings.index(id, start)\n             for i in range(FIND_GOAL_SEARCH_DISTANCE):\n                 next_str = self.js_strings[index + i + 1]\n                 if re.search(r\"\\bselect all\\b\", next_str, re.I):\n                     matching_strings.append((i, index, next_str))\n             start = index + FIND_GOAL_SEARCH_DISTANCE + 1\n         try:\n             while True:\n                 try_find()\n         except (ValueError, IndexError):\n             pass\n         try:\n             goal = min(matching_strings)[2]\n         except ValueError:\n             return None, None\n         raw = goal\n         plain = raw.replace(\"<strong>\", \"\").replace(\"</strong>\", \"\")\n         return raw, plain", "query": "find int in string"}
{"id": "https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/tune/automlboard/common/utils.py#L58-L92", "method_name": "parse_multiple_json", "code": "def parse_multiple_json(json_file, offset=None):\n     json_info_list = []\n     if not os.path.exists(json_file):\n         return json_info_list\n     try:\n         with open(json_file, \"r\") as f:\n             if offset:\n                 f.seek(offset)\n             for line in f:\n                 if line[-1] != \"\n\":\n                     break\n                 json_info = json.loads(line)\n                 json_info_list.append(json_info)\n                 offset += len(line)\n     except BaseException as e:\n         logging.error(e.message)\n     return json_info_list, offset", "query": "parse json file"}
{"id": "https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/wfapi.py#L123-L134", "method_name": "update_website", "code": "def update_website(self, website):\n         self.connect()\n         website = self.server.update_website(\n             self.session_id,\n             website[\"name\"],\n             website[\"ip\"],\n             website[\"https\"],\n             website[\"subdomains\"],\n             website[\"certificate\"],\n             *website[\"website_apps\"]\n         )\n         return website", "query": "get html of website"}
{"id": "https://github.com/amaas-fintech/amaas-core-sdk-python/blob/347b71f8e776b2dde582b015e31b4802d91e8040/amaascore/core/amaas_model.py#L30-L31", "method_name": "to_json_string", "code": "def to_json_string(dict_to_convert):\n    return json.dumps(dict_to_convert, ensure_ascii=False, default=json_handler, indent=4, separators=(\",\", \": \"))", "query": "json to xml conversion"}
{"id": "https://github.com/sods/paramz/blob/ae6fc6274b70fb723d91e48fc5026a9bc5a06508/paramz/optimization/scg.py#L44-L174", "method_name": "SCG", "code": "def SCG(f, gradf, x, optargs=(), maxiters=500, max_f_eval=np.inf, xtol=None, ftol=None, gtol=None):\n     if xtol is None:\n         xtol = 1e-6\n     if ftol is None:\n         ftol = 1e-6\n     if gtol is None:\n         gtol = 1e-5\n     sigma0 = 1.0e-7\n     fold = f(x, *optargs) \n     function_eval = 1\n     fnow = fold\n     gradnew = gradf(x, *optargs) \n     function_eval += 1\n     current_grad = np.dot(gradnew, gradnew)\n     gradold = gradnew.copy()\n     d = -gradnew \n     success = True \n     nsuccess = 0 \n     beta = 1.0 \n     betamin = 1.0e-15 \n     betamax = 1.0e15 \n     status = \"Not converged\"\n     flog = [fold]\n     iteration = 0\n     while iteration < maxiters:\n         if success:\n             mu = np.dot(d, gradnew)\n             if mu >= 0:  \n                 d = -gradnew\n                 mu = np.dot(d, gradnew)\n             kappa = np.dot(d, d)\n             sigma = sigma0 / np.sqrt(kappa)\n             xplus = x + sigma * d\n             gplus = gradf(xplus, *optargs)\n             function_eval += 1\n             theta = np.dot(d, (gplus - gradnew)) / sigma\n         delta = theta + beta * kappa\n         if delta <= 0: \n             delta = beta * kappa\n             beta = beta - theta / kappa\n         alpha = -mu / delta\n         xnew = x + alpha * d\n         fnew = f(xnew, *optargs)\n         function_eval += 1\n         Delta = 2.*(fnew - fold) / (alpha * mu)\n         if Delta >= 0.:\n             success = True\n             nsuccess += 1\n             x = xnew\n             fnow = fnew\n         else:\n             success = False\n             fnow = fold\n         flog.append(fnow) \n         iteration += 1\n         if success:\n             if (np.abs(fnew - fold) < ftol):\n                 status = \"converged - relative reduction in objective\"\n                 break\n             elif (np.max(np.abs(alpha * d)) < xtol):\n                 status = \"converged - relative stepsize\"\n                 break\n             else:\n                 gradold = gradnew\n                 gradnew = gradf(x, *optargs)\n                 function_eval += 1\n                 current_grad = np.dot(gradnew, gradnew)\n                 fold = fnew\n                 if current_grad <= gtol:\n                     status = \"converged - relative reduction in gradient\"\n                     break\n         if Delta < 0.25:\n             beta = min(4.0 * beta, betamax)\n         if Delta > 0.75:\n             beta = max(0.25 * beta, betamin)\n         if nsuccess == x.size:\n             d = -gradnew\n             beta = 1. \n             nsuccess = 0\n         elif success:\n             Gamma = np.dot(gradold - gradnew, gradnew) / (mu)\n             d = Gamma * d - gradnew\n     else:\n         status = \"maxiter exceeded\"\n     return x, flog, function_eval, status", "query": "nelder mead optimize"}
{"id": "https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/utils/parse.py#L21-L80", "method_name": "thread", "code": "def thread(data, default=u\"Untitled.\", id=None):\n     html = html5lib.parse(data, treebuilder=\"dom\")\n     assert html.lastChild.nodeName == \"html\"\n     html = html.lastChild\n     el = list(filter(lambda i: i.attributes[\"id\"].value == \"isso-thread\",\n                      filter(lambda i: \"id\" in i.attributes,\n                             chain(*map(html.getElementsByTagName, (\"div\", \"section\"))))))\n     if not el:\n         return id, default\n     el = el[0]\n     visited = []\n     def recurse(node):\n         for child in node.childNodes:\n             if child.nodeType != child.ELEMENT_NODE:\n                 continue\n             if child.nodeName.upper() == \"H1\":\n                 return child\n             if child not in visited:\n                 return recurse(child)\n     def gettext(rv):\n         for child in rv.childNodes:\n             if child.nodeType == child.TEXT_NODE:\n                 yield child.nodeValue\n             if child.nodeType == child.ELEMENT_NODE:\n                 for item in gettext(child):\n                     yield item\n     try:\n         id = unquote(el.attributes[\"data-isso-id\"].value)\n     except (KeyError, AttributeError):\n         pass\n     try:\n         return id, unquote(el.attributes[\"data-title\"].value)\n     except (KeyError, AttributeError):\n         pass\n     while el is not None:  \n         visited.append(el)\n         rv = recurse(el)\n         if rv:\n             return id, \"\".join(gettext(rv)).strip()\n         el = el.parentNode\n     return id, default", "query": "reading element from html - <td>"}
{"id": "https://github.com/PeerAssets/pypeerassets/blob/8927b4a686887f44fe2cd9de777e2c827c948987/pypeerassets/transactions.py#L99-L114", "method_name": "serialize", "code": "def serialize(self):\n         from itertools import chain\n         result = Stream()\n         result << self.version.to_bytes(4, \"little\")\n         if self.network.tx_timestamp:\n             result << self.timestamp.to_bytes(4, \"little\")\n         result << Parser.to_varint(len(self.ins))\n         result << bytearray(chain.from_iterable(txin.serialize() for txin in self.ins))\n         result << Parser.to_varint(len(self.outs))\n         result << bytearray(chain.from_iterable(txout.serialize() for txout in self.outs))\n         result << self.locktime\n         return result.serialize()", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/devassistant/devassistant/blob/2dbfeaa666a64127263664d18969c55d19ecc83e/devassistant/gui/gui_helper.py#L466-L473", "method_name": "create_clipboard", "code": "def create_clipboard(self, text, selection=Gdk.SELECTION_CLIPBOARD):\n         clipboard = Gtk.Clipboard.get(selection)\n         clipboard.set_text(\"\n\".join(text), -1)\n         clipboard.store()\n         return clipboard", "query": "copy to clipboard"}
{"id": "https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/web_node.py#L305-L311", "method_name": "innerHTML", "code": "def innerHTML(self, html: str) -> None:  \n         df = self._parse_html(html)\n         if self.connected:\n             self._set_inner_html_web(df.html)\n         self._empty()\n         self._append_child(df)", "query": "get inner html"}
{"id": "https://github.com/google/apitools/blob/f3745a7ea535aa0e88b0650c16479b696d6fd446/apitools/base/protorpclite/messages.py#L1786-L1809", "method_name": "validate_default_element", "code": "def validate_default_element(self, value):\n         if isinstance(value, (six.string_types, six.integer_types)):\n             if self.__type:\n                 self.__type(value)\n             return value\n         return super(EnumField, self).validate_default_element(value)", "query": "get name of enumerated value"}
{"id": "https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/doc/figures.py#L43-L45", "method_name": "heatmap2", "code": "def heatmap2():\n    x = np.arange(5)\n    pw.heatmap(z=np.arange(25), x=np.tile(x, 5), y=x.repeat(5)).save(\"fig_heatmap2.html\", **options)", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/yaybu/callsign/blob/e70e5368bfe4fd3ae3fdd1ed43944b53ffa1e100/callsign/config.py#L54-L60", "method_name": "to_bool", "code": "def to_bool(x):\n     if x.lower() in (\"true\", \"yes\", \"on\", \"1\"):\n         return True\n     elif x.lower() in (\"false\", \"no\", \"off\", \"0\"):\n         return False\n     else:\n         raise ValueError(\"%r in config file is not boolean\" % x)", "query": "convert int to bool"}
{"id": "https://github.com/Azure/azure-cli-extensions/blob/3d4854205b0f0d882f688cfa12383d14506c2e35/src/db-up/azext_db_up/custom.py#L381-L394", "method_name": "_run_postgresql_commands", "code": "def _run_postgresql_commands(host, user, password, database):\n     connection = psycopg2.connect(user=user, host=host, password=password, database=database)\n     connection.set_session(autocommit=True)\n     logger.warning(\"Successfully Connected to PostgreSQL.\")\n     cursor = connection.cursor()\n     try:\n         db_password = _create_db_password(database)\n         cursor.execute(\"CREATE USER root WITH ENCRYPTED PASSWORD \"{}\"\".format(db_password))\n         logger.warning(\"Ran Database Query: `CREATE USER root WITH ENCRYPTED PASSWORD \"%s\"`\", db_password)\n     except psycopg2.ProgrammingError:\n         pass\n     cursor.execute(\"GRANT ALL PRIVILEGES ON DATABASE {} TO root\".format(database))\n     logger.warning(\"Ran Database Query: `GRANT ALL PRIVILEGES ON DATABASE %s TO root`\", database)", "query": "postgresql connection"}
{"id": "https://github.com/klen/starter/blob/24a65c10d4ac5a9ca8fc1d8b3d54b3fb13603f5f/starter/core.py#L52-L58", "method_name": "copy_file", "code": "def copy_file(self, from_path, to_path):\n         if not op.exists(op.dirname(to_path)):\n             self.make_directory(op.dirname(to_path))\n         shutil.copy(from_path, to_path)\n         logging.debug(\"File copied: {0}\".format(to_path))", "query": "copying a file to a path"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40", "method_name": "binomial", "code": "def binomial(n,k):\n     if n==k: return 1\n     assert n>k, \"Attempting to call binomial(%d,%d)\" % (n,k)\n     return factorial(n)//(factorial(k)*factorial(n-k))", "query": "binomial distribution"}
{"id": "https://github.com/lrq3000/pyFileFixity/blob/fd5ef23bb13835faf1e3baa773619b86a1cc9bdf/pyFileFixity/lib/profilers/pyinstrument/profiler.py#L159-L185", "method_name": "output_html", "code": "def output_html(self, root=False):\n         resources_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources/\")\n         with open(os.path.join(resources_dir, \"style.css\")) as f:\n             css = f.read()\n         with open(os.path.join(resources_dir, \"profile.js\")) as f:\n             js = f.read()\n         with open(os.path.join(resources_dir, \"jquery-1.11.0.min.js\")) as f:\n             jquery_js = f.read()\n         body = self.starting_frame(root).as_html()\n         page = .format(css=css, js=js, jquery_js=jquery_js, body=body)\n         return page", "query": "output to html file"}
{"id": "https://github.com/Jaymon/prom/blob/b7ad2c259eca198da03e1e4bc7d95014c168c361/prom/query.py#L1458-L1464", "method_name": "process_id", "code": "def process_id(self):\n         ret = \"\"\n         if thread:\n             f = getattr(os, \"getpid\", None)\n             if f:\n                 ret = str(f())\n         return ret", "query": "get current process id"}
{"id": "https://github.com/EdwinvO/pyutillib/blob/6d773c31d1f27cc5256d47feb8afb5c3ae5f0db5/pyutillib/date_utils.py#L35-L84", "method_name": "datestr2date", "code": "def datestr2date(date_str):\n     if any(c not in \"0123456789-/\" for c in date_str):\n         raise ValueError(\"Illegal character in date string\")\n     if \"/\" in date_str:\n         try:\n             m, d, y = date_str.split(\"/\")\n         except:\n             raise ValueError(\"Date {} must have no or exactly 2 slashes. {}\".\n                     format(date_str, VALID_DATE_FORMATS_TEXT))\n     elif \"-\" in date_str:\n         try:\n             d, m, y = date_str.split(\"-\")\n         except:\n             raise ValueError(\"Date {} must have no or exactly 2 dashes. {}\".\n                     format(date_str, VALID_DATE_FORMATS_TEXT))\n     elif len(date_str) == 8 or len(date_str) == 6:\n         d = date_str[-2:]\n         m = date_str[-4:-2]\n         y = date_str[:-4]\n     else:\n         raise ValueError(\"Date format not recognised. {}\".format(\n                 VALID_DATE_FORMATS_TEXT))\n     if len(y) == 2:\n         year = 2000 + int(y)\n     elif len(y) == 4:\n         year = int(y)\n     else:\n         raise ValueError(\"year must be 2 or 4 digits\")\n     for s in (m, d):\n         if 1 <= len(s) <= 2:\n             month, day = int(m), int(d)\n         else:\n             raise ValueError(\"m and d must be 1 or 2 digits\")\n     try:\n         return datetime.date(year, month, day)\n     except ValueError:\n         raise ValueError(\"Invalid date {}. {}\".format(date_str, \n                 VALID_DATE_FORMATS_TEXT))", "query": "string to date"}
{"id": "https://github.com/sparklingpandas/sparklingpandas/blob/7d549df4348c979042b683c355aa778fc6d3a768/sparklingpandas/pcontext.py#L68-L155", "method_name": "read_csv", "code": "def read_csv(self, file_path, use_whole_file=False, names=None, skiprows=0,\n                  *args, **kwargs):\n         def csv_file(partition_number, files):\n             file_count = 0\n             for _, contents in files:\n                 if partition_number == 0 and file_count == 0 and _skiprows > 0:\n                     yield pandas.read_csv(\n                         sio(contents), *args,\n                         header=None,\n                         names=mynames,\n                         skiprows=_skiprows,\n                         **kwargs)\n                 else:\n                     file_count += 1\n                     yield pandas.read_csv(\n                         sio(contents), *args,\n                         header=None,\n                         names=mynames,\n                         **kwargs)\n         def csv_rows(partition_number, rows):\n             in_str = \"\n\".join(rows)\n             if partition_number == 0:\n                 return iter([\n                     pandas.read_csv(\n                         sio(in_str), *args, header=None,\n                         names=mynames,\n                         skiprows=_skiprows,\n                         **kwargs)])\n             else:\n                 return iter([pandas.read_csv(sio(in_str), *args, header=None,\n                                              names=mynames, **kwargs)])\n         mynames = None\n         _skiprows = skiprows\n         if names:\n             mynames = names\n         else:\n             first_line = self.spark_ctx.textFile(file_path).first()\n             frame = pandas.read_csv(sio(first_line), **kwargs)\n             mynames = list(frame.columns)\n             _skiprows += 1\n         if use_whole_file:\n             return self.from_pandas_rdd(\n                 self.spark_ctx.wholeTextFiles(file_path)\n                 .mapPartitionsWithIndex(csv_file))\n         else:\n             return self.from_pandas_rdd(\n                 self.spark_ctx.textFile(file_path)\n                     .mapPartitionsWithIndex(csv_rows))", "query": "read .csv file in an efficient way?"}
{"id": "https://github.com/oplatek/csv2json/blob/f2f95db71ba2ce683fd6d0d3e2f13c9d0a77ceb6/csv2json/__init__.py#L20-L51", "method_name": "convert", "code": "def convert(csv, json, **kwargs):\n     csv_local, json_local = None, None\n     try:\n         if csv == \"-\" or csv is None:\n             csv = sys.stdin\n         elif isinstance(csv, str):\n             csv = csv_local = open(csv, \"r\")\n         if json == \"-\" or json is None:\n             json = sys.stdout\n         elif isinstance(json, str):\n             json = json_local = open(json, \"w\")\n         data = load_csv(csv, **kwargs)\n         save_json(data, json, **kwargs)\n     finally:\n         if csv_local is not None:\n             csv_local.close()\n         if json_local is not None:\n             json_local.close()", "query": "convert json to csv"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/rnc_db.py#L1649-L1908", "method_name": "_connect", "code": "def _connect(self,\n                  engine: str = None,\n                  interface: str = None,\n                  host: str = None,\n                  port: int = None,\n                  database: str = None,\n                  driver: str = None,\n                  dsn: str = None,\n                  odbc_connection_string: str = None,\n                  user: str = None,\n                  password: str = None,\n                  autocommit: bool = True,\n                  charset: str = \"utf8\",\n                  use_unicode: bool = True) -> bool:\n         if engine == ENGINE_MYSQL:\n             self.flavour = MySQL()\n             self.schema = database\n         elif engine == ENGINE_SQLSERVER:\n             self.flavour = SQLServer()\n             if database:\n                 self.schema = database\n             else:\n                 self.schema = \"dbo\"  \n         elif engine == ENGINE_ACCESS:\n             self.flavour = Access()\n             self.schema = \"dbo\"  \n         else:\n             raise ValueError(\"Unknown engine\")\n         if interface is None:\n             if engine == ENGINE_MYSQL:\n                 interface = INTERFACE_MYSQL\n             else:\n                 interface = INTERFACE_ODBC\n         if port is None:\n             if engine == ENGINE_MYSQL:\n                 port = 3306\n             elif engine == ENGINE_SQLSERVER:\n                 port = 1433\n         if driver is None:\n             if engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n                 driver = \"{MySQL ODBC 5.1 Driver}\"\n         self._engine = engine\n         self._interface = interface\n         self._server = host\n         self._port = port\n         self._database = database\n         self._user = user\n         self._password = password\n         self._charset = charset\n         self._use_unicode = use_unicode\n         self.autocommit = autocommit\n         log.info(\n             \"Opening database: engine={e}, interface={i}, \"\n             \"use_unicode={u}, autocommit={a}\".format(\n                 e=engine, i=interface, u=use_unicode, a=autocommit))\n         if interface == INTERFACE_MYSQL:\n             if pymysql:\n                 self.db_pythonlib = PYTHONLIB_PYMYSQL\n             elif MySQLdb:\n                 self.db_pythonlib = PYTHONLIB_MYSQLDB\n             else:\n                 raise ImportError(_MSG_MYSQL_DRIVERS_UNAVAILABLE)\n         elif interface == INTERFACE_ODBC:\n             if not pyodbc:\n                 raise ImportError(_MSG_PYODBC_UNAVAILABLE)\n             self.db_pythonlib = PYTHONLIB_PYODBC\n         elif interface == INTERFACE_JDBC:\n             if not jaydebeapi:\n                 raise ImportError(_MSG_JDBC_UNAVAILABLE)\n             if host is None:\n                 raise ValueError(\"Missing host parameter\")\n             if port is None:\n                 raise ValueError(\"Missing port parameter\")\n             if user is None:\n                 raise ValueError(\"Missing user parameter\")\n             self.db_pythonlib = PYTHONLIB_JAYDEBEAPI\n         else:\n             raise ValueError(\"Unknown interface\")\n         if engine == ENGINE_MYSQL and interface == INTERFACE_MYSQL:\n             datetimetype = datetime.datetime  \n             converters = mysql.converters.conversions.copy()\n             converters[datetimetype] = datetime2literal_rnc\n             log.info(\n                 \"{i} connect: host={h}, port={p}, user={u}, \"\n                 \"database={d}\".format(\n                     i=interface, h=host, p=port, u=user, d=database))\n             self.db = mysql.connect(\n                 host=host,\n                 port=port,\n                 user=user,\n                 passwd=password,\n                 db=database,\n                 charset=charset,\n                 use_unicode=use_unicode,\n                 conv=converters\n             )\n             self.db.autocommit(autocommit)\n         elif engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n             log.info(\n                 \"ODBC connect: DRIVER={dr};SERVER={s};PORT={p};\"\n                 \"DATABASE={db};USER={u};PASSWORD=[censored]\".format(\n                     dr=driver, s=host, p=port,\n                     db=database, u=user))\n             dsn = (\n                 \"DRIVER={0};SERVER={1};PORT={2};DATABASE={3};\"\n                 \"USER={4};PASSWORD={5}\".format(driver, host, port, database,\n                                                user, password)\n             )\n             self.db = pyodbc.connect(dsn)\n             self.db.autocommit = autocommit\n         elif engine == ENGINE_MYSQL and interface == INTERFACE_JDBC:\n             jclassname = \"com.mysql.jdbc.Driver\"\n             url = \"jdbc:mysql://{host}:{port}/{database}\".format(\n                 host=host, port=port, database=database)\n             driver_args = [url, user, password]\n             jars = None\n             libs = None\n             log.info(\n                 \"JDBC connect: jclassname={jclassname}, \"\n                 \"url={url}, user={user}, password=[censored]\".format(\n                     jclassname=jclassname,\n                     url=url,\n                     user=user,\n                 )\n             )\n             self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n         elif engine == ENGINE_SQLSERVER and interface == INTERFACE_ODBC:\n             if odbc_connection_string:\n                 log.info(\"Using raw ODBC connection string [censored]\")\n                 connectstring = odbc_connection_string\n             elif dsn:\n                 log.info(\n                     \"ODBC connect: DSN={dsn};UID={u};PWD=[censored]\".format(\n                         dsn=dsn, u=user))\n                 connectstring = \"DSN={};UID={};PWD={}\".format(dsn, user,\n                                                               password)\n             else:\n                 log.info(\n                     \"ODBC connect: DRIVER={dr};SERVER={s};DATABASE={db};\"\n                     \"UID={u};PWD=[censored]\".format(\n                         dr=driver, s=host, db=database, u=user))\n                 connectstring = (\n                     \"DRIVER={};SERVER={};DATABASE={};UID={};PWD={}\".format(\n                         driver, host, database, user, password)\n                 )\n             self.db = pyodbc.connect(connectstring, unicode_results=True)\n             self.db.autocommit = autocommit\n         elif engine == ENGINE_SQLSERVER and interface == INTERFACE_JDBC:\n             jclassname = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n             urlstem = \"jdbc:sqlserver://{host}:{port};\".format(\n                 host=host,\n                 port=port\n             )\n             nvp = {}\n             if database:\n                 nvp[\"databaseName\"] = database\n             nvp[\"user\"] = user\n             nvp[\"password\"] = password\n             nvp[\"responseBuffering\"] = \"adaptive\"  \n             nvp[\"selectMethod\"] = \"cursor\"  \n             url = urlstem + \";\".join(\n                 \"{}={}\".format(x, y) for x, y in nvp.items())\n             nvp[\"password\"] = \"[censored]\"\n             url_censored = urlstem + \";\".join(\n                 \"{}={}\".format(x, y) for x, y in nvp.items())\n             log.info(\n                 \"jdbc connect: jclassname={jclassname}, url = {url}\".format(\n                     jclassname=jclassname,\n                     url=url_censored\n                 )\n             )\n             driver_args = [url]\n             jars = None\n             libs = None\n             self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n         elif engine == ENGINE_ACCESS and interface == INTERFACE_ODBC:\n             dsn = \"DSN={}\".format(dsn)\n             log.info(\"ODBC connect: DSN={}\", dsn)\n             self.db = pyodbc.connect(dsn)\n             self.db.autocommit = autocommit\n         else:\n             raise ValueError(\n                 \"Unknown \"engine\"/\"interface\" combination: {}/{}\".format(\n                     engine, interface\n                 )\n             )\n         return True", "query": "connect to sql"}
{"id": "https://github.com/rjrequina/Cebuano-POS-Tagger/blob/fb1b46448c40b23ac8e8e373f073f1f8934b6dc6/eval/evaluator.py#L72-L92", "method_name": "confusion_matrix", "code": "def confusion_matrix(actual=[], pred=[]):\n     idx = {\n         \"ADJ\" : 0,\n         \"ADV\" : 1,\n         \"CONJ\": 2,\n         \"DET\" : 3,\n         \"NOUN\": 4,\n         \"NUM\" : 5,\n         \"OTH\" : 6,\n         \"PART\": 7,\n         \"PRON\": 8,\n         \"SYM\" : 9,\n         \"VERB\": 10\n     }\n     matrix = [[0 for i in range(11)] for j in range(11)]\n     for i in range(0, len(actual)):\n        matrix[idx[actual[i]]][idx[pred[i]]] += 1\n     return matrix", "query": "confusion matrix"}
{"id": "https://github.com/halcy/Mastodon.py/blob/35c43562dd3d34d6ebf7a0f757c09e8fcccc957c/mastodon/Mastodon.py#L2443-L2458", "method_name": "__datetime_to_epoch", "code": "def __datetime_to_epoch(self, date_time):\n         date_time_utc = None\n         if date_time.tzinfo is None:\n             date_time_utc = date_time.replace(tzinfo=pytz.utc)\n         else:\n             date_time_utc = date_time.astimezone(pytz.utc)\n         epoch_utc = datetime.datetime.utcfromtimestamp(0).replace(tzinfo=pytz.utc)\n         return (date_time_utc - epoch_utc).total_seconds()", "query": "convert a utc time to epoch"}
{"id": "https://github.com/explosion/spaCy/blob/8ee4100f8ffb336886208a1ea827bf4c745e2709/examples/pipeline/custom_attr_methods.py#L43-L58", "method_name": "to_html", "code": "def to_html(doc, output=\"/tmp\", style=\"dep\"):\n     file_name = \"-\".join([w.text for w in doc[:6] if not w.is_punct]) + \".html\"\n     html = displacy.render(doc, style=style, page=True)  \n     if output is not None:\n         output_path = Path(output)\n         if not output_path.exists():\n             output_path.mkdir()\n         output_file = Path(output) / file_name\n         output_file.open(\"w\", encoding=\"utf-8\").write(html)  \n         print(\"Saved HTML to {}\".format(output_file))\n     else:\n         print(html)", "query": "output to html file"}
{"id": "https://github.com/crytic/slither/blob/04c147f7e50223c6af458ca430befae747ccd259/slither/core/declarations/contract.py#L431-L439", "method_name": "get_enum_from_name", "code": "def get_enum_from_name(self, enum_name):\n         return next((e for e in self.enums if e.name == enum_name), None)", "query": "get name of enumerated value"}
{"id": "https://github.com/hyperledger/indy-sdk/blob/55240dc170308d7883c48f03f308130a6d077be6/vcx/wrappers/python3/vcx/api/proof.py#L55-L70", "method_name": "deserialize", "code": "async def deserialize(data: dict):\n         return await Proof._deserialize(\"vcx_proof_deserialize\",\n                                         json.dumps(data),\n                                         data.get(\"data\").get(\"source_id\"))", "query": "deserialize json"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/rotate_poscar.py#L7-L13", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser( description=\"Rotates the cell lattice in VASP POSCAR files\" )\n     parser.add_argument( \"poscar\", help=\"filename of the VASP POSCAR to be processed\" )\n     parser.add_argument( \"-a\", \"--axis\", nargs=3, type=float, help=\"vector for rotation axis\", required=True )\n     parser.add_argument( \"-d\", \"--degrees\", type=int, help=\"rotation angle in degrees\", required=True )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/gabfl/dbschema/blob/37722e6654e9f0374fac5518ebdca22f4c39f92f/src/schema_change.py#L81-L91", "method_name": "get_connection", "code": "def get_connection(engine, host, user, port, password, database, ssl={}):\n     if engine == \"mysql\":\n         return get_mysql_connection(host, user, port, password, database, ssl)\n     elif engine == \"postgresql\":\n         return get_pg_connection(host, user, port, password, database, ssl)\n     else:\n         raise RuntimeError(\"`%s` is not a valid engine.\" % engine)", "query": "postgresql connection"}
{"id": "https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/src/pyoram/crypto/aes.py#L24-L27", "method_name": "CTREnc", "code": "def CTREnc(key, plaintext):\n         iv = os.urandom(AES.block_size)\n         cipher = _cipher(_aes(key), _ctrmode(iv), backend=_backend).encryptor()\n         return iv + cipher.update(plaintext) + cipher.finalize()", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/PySimpleGUI/PySimpleGUI/blob/08184197f5bd4580ab5e5aca28bdda30f87b86fc/PySimpleGUI27.py#L486-L493", "method_name": "CheckboxHandler", "code": "def CheckboxHandler(self):\n         if self.Key is not None:\n             self.ParentForm.LastButtonClicked = self.Key\n         else:\n             self.ParentForm.LastButtonClicked = \"\"\n         self.ParentForm.FormRemainedOpen = True\n         if self.ParentForm.CurrentlyRunningMainloop:\n             self.ParentForm.TKroot.quit()", "query": "make the checkbox checked"}
{"id": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/editor/plugin.py#L1537-L1542", "method_name": "__set_workdir", "code": "def __set_workdir(self): \n         fname = self.get_current_filename() \n         if fname is not None: \n             directory = osp.dirname(osp.abspath(fname)) \n             self.open_dir.emit(directory)", "query": "set working directory"}
{"id": "https://github.com/dshean/pygeotools/blob/5ac745717c0098d01eb293ff1fe32fd7358c76ab/pygeotools/lib/malib.py#L653-L667", "method_name": "linreg", "code": "def linreg(self, rsq=False, conf_test=False):\n         model=\"linear\"\n         if not self.datestack:\n             self.compute_dt_stats()\n         if np.isnan(self.min_dt_ptp):\n             max_dt_ptp = calcperc(self.dt_stack_ptp, (4, 96))[1]\n             self.min_dt_ptp = 0.20 * max_dt_ptp\n         if self.robust:\n             model=\"theilsen\"\n         print(\"Compute stack linear trend with model: %s\" % model)\n         self.stack_trend, self.stack_intercept, self.stack_detrended_std = \\\n                 ma_linreg(self.ma_stack, self.date_list, dt_stack_ptp=self.dt_stack_ptp, min_dt_ptp=self.min_dt_ptp, \\\n                 n_thresh=self.n_thresh, model=model, rsq=False, conf_test=False, smooth=False, n_cpu=self.n_cpu)", "query": "linear regression"}
{"id": "https://github.com/limodou/uliweb/blob/34472f25e4bc0b954a35346672f94e84ef18b076/uliweb/form/template_layout.py#L90-L108", "method_name": "do_td_field", "code": "def do_td_field(self, indent, value, **kwargs): \n         field_name = kwargs.pop(\"name\", None) \n         field = getattr(self.form, field_name) \n         obj = self.form.fields[field_name] \n         if \"label\" in kwargs: \n             label = kwargs.pop(\"label\") \n         else: \n             label = obj.label \n         if label: \n             obj.label = label \n             label_text = obj.get_label(_class=\"field\") \n         else: \n             label_text = \"\" \n         display = field.data or \"&nbsp;\" \n         if \"width\" not in kwargs: \n             kwargs[\"width\"] = 200 \n         td = begin_tag(\"td\", **kwargs) + u_str(display) + end_tag(\"td\") \n         return indent * \" \" + \"<th align=right width=200>%s</th>%s\" % (label_text, td)", "query": "underline text in label widget"}
{"id": "https://github.com/totalgood/pugnlp/blob/c43445b14afddfdeadc5f3076675c9e8fc1ee67c/src/pugnlp/stats.py#L536-L562", "method_name": "from_existing", "code": "def from_existing(cls, confusion, *args, **kwargs):\n         df = []\n         for t, p in product(confusion.index.values, confusion.columns.values):\n             df += [[t, p]] * confusion[p][t]\n         if confusion.index.name is not None and confusion.columns.name is not None:\n             return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name]))\n         return Confusion(pd.DataFrame(df))", "query": "confusion matrix"}
{"id": "https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilebuild/iotile/build/config/scons-local-3.0.1/SCons/Util.py#L1300-L1303", "method_name": "__make_unique", "code": "def __make_unique(self):\n         if not self.unique:\n             self.data = uniquer_hashables(self.data)\n             self.unique = True", "query": "unique elements"}
{"id": "https://github.com/pyGrowler/Growler/blob/90c923ff204f28b86a01d741224987a22f69540f/growler/middleware/cookieparser.py#L34-L56", "method_name": "__call__", "code": "def __call__(self, req, res):\n         if hasattr(req, \"cookies\"):\n             return\n         req.cookies, res.cookies = SimpleCookie(), SimpleCookie()\n         log.info(\"{:d} built with {}\", id(self), json.dumps(self.opts))\n         req.cookies.load(req.headers.get(\"COOKIE\", \"\"))\n         def _gen_cookie():\n             if res.cookies:\n                 cookie_string = res.cookies.output(header=\"\", sep=res.EOL)\n                 return cookie_string\n         res.headers[\"Set-Cookie\"] = _gen_cookie", "query": "create cookie"}
{"id": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/url.py#L75-L125", "method_name": "get", "code": "def get(cls):  \n         if Check().is_url_valid() or PyFunceble.CONFIGURATION[\"local\"]:\n             if \"current_test_data\" in PyFunceble.INTERN:\n                 PyFunceble.INTERN[\"current_test_data\"][\"url_syntax_validation\"] = True\n             PyFunceble.INTERN.update({\"http_code\": HTTPCode().get()})\n             active_list = []\n             active_list.extend(PyFunceble.HTTP_CODE[\"list\"][\"potentially_up\"])\n             active_list.extend(PyFunceble.HTTP_CODE[\"list\"][\"up\"])\n             inactive_list = []\n             inactive_list.extend(PyFunceble.HTTP_CODE[\"list\"][\"potentially_down\"])\n             inactive_list.append(\"*\" * 3)\n             if PyFunceble.INTERN[\"http_code\"] in active_list:\n                 return URLStatus(PyFunceble.STATUS[\"official\"][\"up\"]).handle()\n             if PyFunceble.INTERN[\"http_code\"] in inactive_list:\n                 return URLStatus(PyFunceble.STATUS[\"official\"][\"down\"]).handle()\n         if \"current_test_data\" in PyFunceble.INTERN:\n             PyFunceble.INTERN[\"current_test_data\"][\"url_syntax_validation\"] = False\n         return URLStatus(PyFunceble.STATUS[\"official\"][\"invalid\"]).handle()", "query": "get the description of a http status code"}
{"id": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/utils/serialization.py#L47-L60", "method_name": "deserialize", "code": "def deserialize(s):\n     s = s.strip()\n     try:\n         doc = etree.fromstring(s)\n         if is_tmdd(doc):\n             from ..converter.tmdd import tmdd_to_json\n             return (tmdd_to_json(doc), \"json\")\n         return (doc, \"xml\")\n     except etree.XMLSyntaxError:\n         try:\n             return (json.loads(s), \"json\")\n         except ValueError:\n             raise Exception(\"Doesn\"t look like either JSON or XML\")", "query": "deserialize json"}
{"id": "https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/ofctl_utils.py#L198-L203", "method_name": "to_match_masked_int", "code": "def to_match_masked_int(value):\n     if isinstance(value, str) and \"/\" in value:\n         value = value.split(\"/\")\n         return str_to_int(value[0]), str_to_int(value[1])\n     return str_to_int(value)", "query": "convert int to string"}
{"id": "https://github.com/LandRegistry/lr-utils/blob/811c9e5c11678a04ee203fa55a7c75080f4f9d89/lrutils/templatefilters/template_filters.py#L10-L12", "method_name": "dateformat", "code": "def dateformat(value, format=\"%-d %B %Y\"):\n    new_date = parser.parse(value, dayfirst=True)\n    return new_date.strftime(format)", "query": "format date"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/excel.py#L1227-L1269", "method_name": "compile_geometry", "code": "def compile_geometry(lat, lon, elev):\n     logger_excel.info(\"enter compile_geometry\")\n     lat = _remove_geo_placeholders(lat)\n     lon = _remove_geo_placeholders(lon)\n     if len(lat) == 2 and len(lon) == 2:\n         logger_excel.info(\"found 4 coordinates\")\n         geo_dict = geometry_linestring(lat, lon, elev)\n     elif len(lat) == 1 and len(lon) == 1:\n         logger_excel.info(\"found 2 coordinates\")\n         geo_dict = geometry_point(lat, lon, elev)\n     elif (None in lon and None not in lat) or (len(lat) > 0 and len(lon) == 0):\n         geo_dict = geometry_range(lat, elev, \"lat\")\n     elif (None in lat and None not in lon) or (len(lon) > 0 and len(lat) == 0):\n         geo_dict = geometry_range(lat, elev, \"lon\")\n     else:\n         geo_dict = {}\n         logger_excel.warn(\"compile_geometry: invalid coordinates: lat: {}, lon: {}\".format(lat, lon))\n     logger_excel.info(\"exit compile_geometry\")\n     return geo_dict", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/horejsek/python-webdriverwrapper/blob/a492f79ab60ed83d860dd817b6a0961500d7e3f5/webdriverwrapper/forms.py#L133-L138", "method_name": "fill_input_radio", "code": "def fill_input_radio(self, value, skip_reset=False):\n         elm = self.form_elm.get_elm(xpath=\"//input[@type=\"radio\"][@name=\"%s\"][@value=\"%s\"]\" % (\n             self.elm_name,\n             self.convert_value(value),\n         ))\n         self._click_on_elm_or_his_ancestor(elm)", "query": "reading element from html - <td>"}
{"id": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/syncmap/__init__.py#L309-L368", "method_name": "output_html_for_tuning", "code": "def output_html_for_tuning(\n             self,\n             audio_file_path,\n             output_file_path,\n             parameters=None\n     ):\n         if not gf.file_can_be_written(output_file_path):\n             self.log_exc(u\"Cannot output HTML file \"%s\". Wrong permissions?\" % (output_file_path), None, True, OSError)\n         if parameters is None:\n             parameters = {}\n         audio_file_path_absolute = gf.fix_slash(os.path.abspath(audio_file_path))\n         template_path_absolute = gf.absolute_path(self.FINETUNEAS_PATH, __file__)\n         with io.open(template_path_absolute, \"r\", encoding=\"utf-8\") as file_obj:\n             template = file_obj.read()\n         for repl in self.FINETUNEAS_REPLACEMENTS:\n             template = template.replace(repl[0], repl[1])\n         template = template.replace(\n             self.FINETUNEAS_REPLACE_AUDIOFILEPATH,\n             u\"audioFilePath = \\\"file://%s\\\";\" % audio_file_path_absolute\n         )\n         template = template.replace(\n             self.FINETUNEAS_REPLACE_FRAGMENTS,\n             u\"fragments = (%s).fragments;\" % self.json_string\n         )\n         if gc.PPN_TASK_OS_FILE_FORMAT in parameters:\n             output_format = parameters[gc.PPN_TASK_OS_FILE_FORMAT]\n             if output_format in self.FINETUNEAS_ALLOWED_FORMATS:\n                 template = template.replace(\n                     self.FINETUNEAS_REPLACE_OUTPUT_FORMAT,\n                     u\"outputFormat = \\\"%s\\\";\" % output_format\n                 )\n                 if output_format == \"smil\":\n                     for key, placeholder, replacement in [\n                             (\n                                 gc.PPN_TASK_OS_FILE_SMIL_AUDIO_REF,\n                                 self.FINETUNEAS_REPLACE_SMIL_AUDIOREF,\n                                 \"audioref = \\\"%s\\\";\"\n                             ),\n                             (\n                                 gc.PPN_TASK_OS_FILE_SMIL_PAGE_REF,\n                                 self.FINETUNEAS_REPLACE_SMIL_PAGEREF,\n                                 \"pageref = \\\"%s\\\";\"\n                             ),\n                     ]:\n                         if key in parameters:\n                             template = template.replace(\n                                 placeholder,\n                                 replacement % parameters[key]\n                             )\n         with io.open(output_file_path, \"w\", encoding=\"utf-8\") as file_obj:\n             file_obj.write(template)", "query": "output to html file"}
{"id": "https://github.com/lcharleux/argiope/blob/8170e431362dc760589f7d141090fd133dece259/argiope/abq/pypostproc.py#L23-L49", "method_name": "read_field_report", "code": "def read_field_report(path, data_flag = \"*DATA\", meta_data_flag = \"*METADATA\"):\n   text = open(path).read()\n   mdpos = text.find(meta_data_flag)\n   dpos = text.find(data_flag)\n   mdata = io.StringIO( \"\n\".join(text[mdpos:dpos].split(\"\n\")[1:]))\n   data = io.StringIO( \"\n\".join(text[dpos:].split(\"\n\")[1:]))\n   data = pd.read_csv(data, index_col = 0)\n   data = data.groupby(data.index).mean()\n   mdata = pd.read_csv(mdata, sep = \"=\", header = None, index_col = 0)[1]\n   mdata = mdata.to_dict()\n   out = {}\n   out[\"step_num\"] = int(mdata[\"step_num\"])\n   out[\"step_label\"] = mdata[\"step_label\"]\n   out[\"frame\"] = int(mdata[\"frame\"])\n   out[\"frame_value\"] = float(mdata[\"frame_value\"])\n   out[\"part\"] = mdata[\"instance\"]\n   position_map = {\"NODAL\": \"node\", \n                   \"ELEMENT_CENTROID\": \"element\", \n                   \"WHOLE_ELEMENT\": \"element\"}\n   out[\"position\"] = position_map[mdata[\"position\"]]\n   out[\"label\"] = mdata[\"label\"]  \n   out[\"data\"] = data\n   field_class = getattr(argiope.mesh, mdata[\"argiope_class\"])\n   return field_class(**out)", "query": "extracting data from a text file"}
{"id": "https://github.com/stianaske/pybotvac/blob/e3f655e81070ff209aaa4efb7880016cf2599e6d/pybotvac/robot.py#L65-L120", "method_name": "start_cleaning", "code": "def start_cleaning(self, mode=2, navigation_mode=1, category=None, boundary_id=None):\n         if category is None:\n             category = 4 if self.service_version in [\"basic-3\", \"basic-4\"] and self.has_persistent_maps else 2\n         if self.service_version == \"basic-1\":\n             json = {\"reqId\": \"1\",\n                     \"cmd\": \"startCleaning\",\n                     \"params\": {\n                         \"category\": category,\n                         \"mode\": mode,\n                         \"modifier\": 1}\n                     }\n         elif self.service_version == \"basic-3\" or \"basic-4\":\n             json = {\"reqId\": \"1\",\n                     \"cmd\": \"startCleaning\",\n                     \"params\": {\n                         \"category\": category,\n                         \"mode\": mode,\n                         \"modifier\": 1,\n                         \"navigationMode\": navigation_mode}\n                     }\n             if boundary_id:\n                 json[\"params\"][\"boundaryId\"] = boundary_id\n         elif self.service_version == \"minimal-2\":\n             json = {\"reqId\": \"1\",\n                     \"cmd\": \"startCleaning\",\n                     \"params\": {\n                         \"category\": category,\n                         \"navigationMode\": navigation_mode}\n                     }\n         else:   \n             json = {\"reqId\": \"1\",\n                     \"cmd\": \"startCleaning\",\n                     \"params\": {\n                         \"category\": category,\n                         \"mode\": mode,\n                         \"modifier\": 1,\n                         \"navigationMode\": navigation_mode}\n                     }\n         response = self._message(json)\n         response_dict = response.json()\n         if category == 4 and \"alert\" in response_dict and response_dict[\"alert\"] == \"nav_floorplan_load_fail\":\n             json[\"params\"][\"category\"] = 2\n             return self._message(json)\n         return response", "query": "map to json"}
{"id": "https://github.com/geometalab/pyGeoTile/blob/b1f44271698f5fc4d18c2add935797ed43254aa6/pygeotile/point.py#L12-L16", "method_name": "from_latitude_longitude", "code": "def from_latitude_longitude(cls, latitude=0.0, longitude=0.0):\n         assert -180.0 <= longitude <= 180.0, \"Longitude needs to be a value between -180.0 and 180.0.\"\n         assert -90.0 <= latitude <= 90.0, \"Latitude needs to be a value between -90.0 and 90.0.\"\n         return cls(latitude=latitude, longitude=longitude)", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/FileStorageSystem.py#L97-L106", "method_name": "__read_properties", "code": "def __read_properties(self):\n         properties = dict()\n         if self.__filepath and os.path.exists(self.__filepath):\n             try:\n                 with open(self.__filepath, \"r\") as fp:\n                     properties = json.load(fp)\n             except Exception:\n                 os.replace(self.__filepath, self.__filepath + \".bak\")\n         return properties", "query": "read properties file"}
{"id": "https://github.com/danidee10/Staticfy/blob/ebc555b00377394b0f714e4a173d37833fec90cb/staticfy/staticfy.py#L71-L89", "method_name": "get_elements", "code": "def get_elements(html_file, tags):\n     with open(html_file) as f:\n         document = BeautifulSoup(f, \"html.parser\")\n         def condition(tag, attr):\n             return lambda x: x.name == tag \\\n                 and not x.get(attr, \"http\").startswith((\"http\", \"//\"))\n         all_tags = [(attr, document.find_all(condition(tag, attr)))\n                     for tag, attr in tags]\n         return all_tags", "query": "reading element from html - <td>"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/Multicomp6809/Multicomp6809_rom.py#L39-L54", "method_name": "extract_zip", "code": "def extract_zip(self):\n         assert self.FILE_COUNT>0\n         try:\n             with zipfile.ZipFile(self.archive_path, \"r\") as zip:\n                 content = zip.read(\"ROMS/6809/EXT_BASIC_NO_USING.hex\")\n                 out_filename=os.path.join(self.ROM_PATH, \"EXT_BASIC_NO_USING.hex\")\n                 with open(out_filename, \"wb\") as f:\n                     f.write(content)\n                 print(\"%r extracted\" % out_filename)\n                 self.post_processing(out_filename)\n         except BadZipFile as err:\n             msg = \"Error extracting archive %r: %s\" % (self.archive_path, err)\n             log.error(msg)\n             raise BadZipFile(msg)", "query": "extract zip file recursively"}
{"id": "https://github.com/kevinsprong23/aperture/blob/d0420fef3b25d8afc0e5ddcfb6fe5f0ff42b9799/aperture/heatmaps.py#L123-L144", "method_name": "heatmap", "code": "def heatmap(x, y, step=None, min_pt=None, max_pt=None,\n                  colormap=\"Blues\", alpha=1, grid=False,\n                  colorbar=True, scale=\"lin\",\n                  vmax=\"auto\", vmin=\"auto\", crop=True):\n     (x_vec, y_vec, hist_matrix) = calc_2d_hist(x, y, step, min_pt, max_pt)\n     if scale == \"log\":\n         for row in hist_matrix:\n             for i, el in enumerate(row):\n                 row[i] = 0 if row[i] == 0 else log10(row[i])\n     fig = plt.figure()\n     init_heatmap(x_vec, y_vec, hist_matrix, fig, colormap=colormap,\n                  alpha=alpha, grid=grid, colorbar=colorbar,\n                  vmax=vmax, vmin=vmin, crop=crop)\n     return fig", "query": "heatmap from 3d coordinates"}
{"id": "https://github.com/konstantinstadler/pymrio/blob/d764aa0dd2150200e867a9713a98ddae203e12d4/pymrio/tools/ioutil.py#L459-L540", "method_name": "sniff_csv_format", "code": "def sniff_csv_format(csv_file,\n                      potential_sep=[\"\\t\", \",\", \";\", \" \", \"-\", \"_\"],\n                      max_test_lines=10,\n                      zip_file=None):\n     def read_first_lines(filehandle):\n         lines = []\n         for i in range(max_test_lines):\n             line = ff.readline()\n             if line == \"\":\n                 break\n             try:\n                 line = line.decode(\"utf-8\")\n             except AttributeError:\n                 pass\n             lines.append(line[:-1])\n         return lines\n     if zip_file:\n         with zipfile.ZipFile(zip_file, \"r\") as zz:\n             with zz.open(csv_file, \"r\") as ff:\n                 test_lines = read_first_lines(ff)\n     else:\n         with open(csv_file, \"r\") as ff:\n             test_lines = read_first_lines(ff)\n     sep_aly_lines = [sorted([(line.count(sep), sep)\n                      for sep in potential_sep if line.count(sep) > 0],\n                      key=lambda x: x[0], reverse=True) for line in test_lines]\n     for nr, (count, sep) in enumerate(sep_aly_lines[0]):\n         for line in sep_aly_lines:\n             if line[nr][0] == count:\n                 break\n         else:\n             sep = None\n         if sep:\n             break\n     nr_header_row = None\n     nr_index_col = None\n     if sep:\n         nr_index_col = find_first_number(test_lines[-1].split(sep))\n         if nr_index_col:\n             for nr_header_row, line in enumerate(test_lines):\n                 if find_first_number(line.split(sep)) == nr_index_col:\n                     break\n     return dict(sep=sep,\n                 nr_header_row=nr_header_row,\n                 nr_index_col=nr_index_col)", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/nilp0inter/cpe/blob/670d947472a7652af5149324977b50f9a7af9bcf/cpe/cpe.py#L115-L146", "method_name": "_trim", "code": "def _trim(cls, s):\n         reverse = s[::-1]\n         idx = 0\n         for i in range(0, len(reverse)):\n             if reverse[i] == \":\":\n                 idx += 1\n             else:\n                 break\n         new_s = reverse[idx: len(reverse)]\n         return new_s[::-1]", "query": "reverse a string"}
{"id": "https://github.com/dendory/menu3/blob/350414966e8f5c7737cd527369c8087f4f8f600b/menu3/menu3.py#L45-L49", "method_name": "underline", "code": "def underline(self, text): \n \t\tif self._windows(): \n \t\t\treturn text \n \t\telse: \n \t\t\treturn self.UNDERLINE + text + self.NORMAL", "query": "underline text in label widget"}
{"id": "https://github.com/danhper/python-i18n/blob/bbba4b7ec091997ea8df2067acd7af316ee00b31/i18n/loaders/json_loader.py#L10-L14", "method_name": "parse_file", "code": "def parse_file(self, file_content):\n         try:\n             return json.loads(file_content)\n         except ValueError as e:\n             raise I18nFileLoadError(\"invalid JSON: {0}\".format(e.strerror))", "query": "parse json file"}
{"id": "https://github.com/kencochrane/django-defender/blob/e3e547dbb83235e0d564a6d64652c7df00412ff2/defender/utils.py#L41-L50", "method_name": "get_ip", "code": "def get_ip(request):\n     if config.BEHIND_REVERSE_PROXY:\n         ip_address = request.META.get(config.REVERSE_PROXY_HEADER, \"\")\n         ip_address = ip_address.split(\",\", 1)[0].strip()\n         if ip_address == \"\":\n             ip_address = get_ip_address_from_request(request)\n     else:\n         ip_address = get_ip_address_from_request(request)\n     return ip_address", "query": "get current ip address"}
{"id": "https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/controller/widgets/SignalFrame.py#L31-L33", "method_name": "perform_filter", "code": "def perform_filter(result_array: Array, data, f_low, f_high, filter_bw):\n    result_array = np.frombuffer(result_array.get_obj(), dtype=np.complex64)\n    result_array[:] = Filter.apply_bandpass_filter(data, f_low, f_high, filter_bw=filter_bw)", "query": "filter array"}
{"id": "https://github.com/rfk/threading2/blob/7ec234ddd8c0d7e683b1a5c4a79a3d001143189f/threading2/t2_base.py#L372-L378", "method_name": "_set_priority", "code": "def _set_priority(self,priority):\n         if not 0 <= priority <= 1:\n             raise ValueError(\"priority must be between 0 and 1\")\n         self.__priority = priority\n         if self.is_alive():\n             self.priority = priority\n         return priority", "query": "priority queue"}
{"id": "https://github.com/couchbase/couchbase-python-client/blob/a7bada167785bf79a29c39f820d932a433a6a535/couchbase/connstr.py#L126-L143", "method_name": "encode", "code": "def encode(self):\n         opt_dict = {}\n         for k, v in self.options.items():\n             opt_dict[k] = v[0]\n         ss = \"{0}://{1}\".format(self.scheme, \",\".join(self.hosts))\n         if self.bucket:\n             ss += \"/\" + self.bucket\n         ss += \"?\" + urlencode(opt_dict).replace(\"%2F\", \"/\")\n         return ss", "query": "encode url"}
{"id": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/indexes/multi.py#L292-L350", "method_name": "from_arrays", "code": "def from_arrays(cls, arrays, sortorder=None, names=None):\n         error_msg = \"Input must be a list / sequence of array-likes.\"\n         if not is_list_like(arrays):\n             raise TypeError(error_msg)\n         elif is_iterator(arrays):\n             arrays = list(arrays)\n         for array in arrays:\n             if not is_list_like(array):\n                 raise TypeError(error_msg)\n         for i in range(1, len(arrays)):\n             if len(arrays[i]) != len(arrays[i - 1]):\n                 raise ValueError(\"all arrays must be same length\")\n         from pandas.core.arrays.categorical import _factorize_from_iterables\n         codes, levels = _factorize_from_iterables(arrays)\n         if names is None:\n             names = [getattr(arr, \"name\", None) for arr in arrays]\n         return MultiIndex(levels=levels, codes=codes, sortorder=sortorder,\n                           names=names, verify_integrity=False)", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L471-L491", "method_name": "findAllSubstrings", "code": "def findAllSubstrings(string, substring):\n     start = 0\n     positions = []\n     while True:\n         start = string.find(substring, start)\n         if start == -1:\n             break\n         positions.append(start)\n         start += 1\n     return positions", "query": "positions of substrings in string"}
{"id": "https://github.com/TheRobotCarlson/DocxMerge/blob/df8d2add5ff4f33e31d36a6bd3d4495140334538/DocxMerge/DocxMerge.py#L274-L293", "method_name": "replace_doc_text", "code": "def replace_doc_text(file, replacements):\n     document = Document(file)\n     paragraphs = document.paragraphs\n     changes = False\n     for paragraph in paragraphs:\n         text = paragraph.text\n         for original, replace in replacements.items():\n             if original in replace and replace in text:\n                 continue\n             if original in text:\n                 changes = True\n                 text = text.replace(original, replace)\n                 paragraph.text = text\n     if changes:\n         print(\"changing {}\".format(file))\n     document.save(file)", "query": "replace in file"}
{"id": "https://github.com/AshleySetter/optoanalysis/blob/9d390acc834d70024d47b574aea14189a5a5714e/optoanalysis/optoanalysis/optoanalysis.py#L3311-L3329", "method_name": "_GetRealImagArray", "code": "def _GetRealImagArray(Array):\n     ImagArray = _np.array([num.imag for num in Array])\n     RealArray = _np.array([num.real for num in Array])\n     return RealArray, ImagArray", "query": "initializing array"}
{"id": "https://github.com/StanfordBioinformatics/loom/blob/db2031a1a87124fee1aeb7414a668c03d774a698/worker/loomengine_worker/outputs.py#L71-L83", "method_name": "save", "code": "def save(self):\n         filename_list = self.output[\"source\"][\"filenames\"]\n         if not isinstance(filename_list, list):\n             filename_list = filename_list.split(\" \")\n         contents_list = []\n         for filename in filename_list:\n             file_path = os.path.join(\n                 self.working_dir, filename)\n             contents_list.append(self._read_file(file_path))\n         self.output.update({\"data\": {\"contents\": contents_list}})\n         self.connection.update_task_attempt_output(\n             self.output[\"uuid\"],\n             self.output)", "query": "save list to file"}
{"id": "https://github.com/maartenbreddels/ipyvolume/blob/e68b72852b61276f8e6793bc8811f5b2432a155f/ipyvolume/widgets.py#L434-L441", "method_name": "scatter", "code": "def scatter(x, y, z, color=(1, 0, 0), s=0.01):\n     global _last_figure\n     fig = _last_figure\n     if fig is None:\n         fig = volshow(None)\n     fig.scatter = Scatter(x=x, y=y, z=z, color=color, size=s)\n     fig.volume.scatter = fig.scatter\n     return fig", "query": "scatter plot"}
{"id": "https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L52-L57", "method_name": "aes_encrypt", "code": "def aes_encrypt(key, data, mode=\"ECB\", iv=None):\n     aes = AES()\n     aes.mode = mode\n     aes.iv = iv\n     aes.key = key\n     return aes.encrypt(data)", "query": "encrypt aes ctr mode"}
{"id": "https://github.com/explosion/spaCy/blob/8ee4100f8ffb336886208a1ea827bf4c745e2709/examples/pipeline/custom_attr_methods.py#L43-L58", "method_name": "to_html", "code": "def to_html(doc, output=\"/tmp\", style=\"dep\"):\n     file_name = \"-\".join([w.text for w in doc[:6] if not w.is_punct]) + \".html\"\n     html = displacy.render(doc, style=style, page=True)  \n     if output is not None:\n         output_path = Path(output)\n         if not output_path.exists():\n             output_path.mkdir()\n         output_file = Path(output) / file_name\n         output_file.open(\"w\", encoding=\"utf-8\").write(html)  \n         print(\"Saved HTML to {}\".format(output_file))\n     else:\n         print(html)", "query": "output to html file"}
{"id": "https://github.com/Games-and-Simulations/sc-docker/blob/1d7adb9b5839783655564afc4bbcd204a0055dcb/scbw/utils.py#L18-L37", "method_name": "levenshtein_dist", "code": "def levenshtein_dist(s1: str, s2: str) -> int:\n     if len(s1) < len(s2):\n         return levenshtein_dist(s2, s1)\n     if len(s2) == 0:\n         return len(s1)\n     previous_row = range(len(s2) + 1)\n     for i, c1 in enumerate(s1):\n         current_row = [i + 1]\n         for j, c2 in enumerate(s2):\n             insertions = previous_row[j + 1] + 1\n             deletions = current_row[j] + 1  \n             substitutions = previous_row[j] + (c1 != c2)\n             current_row.append(min(insertions, deletions, substitutions))\n         previous_row = current_row\n     return previous_row[-1]", "query": "string similarity levenshtein"}
{"id": "https://github.com/JesseAldridge/clipmon/blob/5e16a31cf38f64db2d9b7a490968f922aca57512/clipmon.py#L19-L38", "method_name": "test_replacements", "code": "def test_replacements(clip_str, path_exists):\n   replaced_str = clip_str\n   for find_regex, replace_str in conf.find_replace_map:\n     replaced_str = re.sub(find_regex, replace_str, replaced_str)\n   match = re.search(\n   if match and path_exists(os.path.expanduser(match.group(1))):\n     return \":\".join([match.group(1), match.group(2)])\n   match = re.search(\n   if match and path_exists(os.path.expanduser(match.group(1))):\n     return \":\".join([match.group(1), match.group(2)])", "query": "replace in file"}
{"id": "https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/util/summary.py#L11-L86", "method_name": "summary", "code": "def summary(model, input_size):\n     def register_hook(module):\n         def hook(module, input, output):\n             class_name = str(module.__class__).split(\".\")[-1].split(\"\"\")[0]\n             module_idx = len(summary)\n             m_key = \"%s-%i\" % (class_name, module_idx + 1)\n             summary[m_key] = OrderedDict()\n             summary[m_key][\"input_shape\"] = list(input[0].size())\n             summary[m_key][\"input_shape\"][0] = -1\n             if isinstance(output, (list, tuple)):\n                 summary[m_key][\"output_shape\"] = [[-1] + list(o.size())[1:] for o in output]\n             else:\n                 summary[m_key][\"output_shape\"] = list(output.size())\n                 summary[m_key][\"output_shape\"][0] = -1\n             params = 0\n             if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n                 params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                 summary[m_key][\"trainable\"] = module.weight.requires_grad\n             if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n                 params += torch.prod(torch.LongTensor(list(module.bias.size())))\n             summary[m_key][\"nb_params\"] = params\n         if (not isinstance(module, nn.Sequential) and\n                 not isinstance(module, nn.ModuleList) and\n                 not (module == model)):\n             hooks.append(module.register_forward_hook(hook))\n     if torch.cuda.is_available():\n         dtype = torch.cuda.FloatTensor\n         model = model.cuda()\n     else:\n         dtype = torch.FloatTensor\n         model = model.cpu()\n     if isinstance(input_size[0], (list, tuple)):\n         x = [Variable(torch.rand(2, *in_size)).type(dtype) for in_size in input_size]\n     else:\n         x = Variable(torch.rand(2, *input_size)).type(dtype)\n     summary = OrderedDict()\n     hooks = []\n     model.apply(register_hook)\n     model(x)\n     for h in hooks:\n         h.remove()\n     print(\"----------------------------------------------------------------\")\n     line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param \n     print(line_new)\n     print(\"================================================================\")\n     total_params = 0\n     trainable_params = 0\n     for layer in summary:\n         line_new = \"{:>20}  {:>25} {:>15}\".format(layer, str(summary[layer][\"output_shape\"]),\n                                                   \"{0:,}\".format(summary[layer][\"nb_params\"]))\n         total_params += summary[layer][\"nb_params\"]\n         if \"trainable\" in summary[layer]:\n             if summary[layer][\"trainable\"] == True:\n                 trainable_params += summary[layer][\"nb_params\"]\n         print(line_new)\n     print(\"================================================================\")\n     print(\"Total params: {0:,}\".format(total_params))\n     print(\"Trainable params: {0:,}\".format(trainable_params))\n     print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n     print(\"----------------------------------------------------------------\")", "query": "print model summary"}
{"id": "https://github.com/limpyd/redis-limpyd-jobs/blob/264c71029bad4377d6132bf8bb9c55c44f3b03a2/limpyd_jobs/workers.py#L492-L504", "method_name": "requeue_job", "code": "def requeue_job(self, job, queue, priority, delayed_for=None):\n         job.requeue(queue_name=queue._cached_name,\n                     priority=priority,\n                     delayed_for=delayed_for,\n                     queue_model=self.queue_model)\n         if hasattr(job, \"on_requeued\"):\n             job.on_requeued(queue)\n         self.log(self.job_requeue_message(job, queue))", "query": "priority queue"}
{"id": "https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/models.py#L55-L57", "method_name": "get_tables", "code": "def get_tables(self, database_name):\n        database = self.get_database(database_name)\n        return [table for table_name, table in database.tables.items()]", "query": "get database table name"}
{"id": "https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568", "method_name": "assert_checked_checkbox", "code": "def assert_checked_checkbox(self, value):\n     check_box = find_field(world.browser, \"checkbox\", value)\n     assert check_box, \"Cannot find checkbox \"{}\".\".format(value)\n     assert check_box.is_selected(), \"Check box should be selected.\"", "query": "make the checkbox checked"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L120-L137", "method_name": "get_table", "code": "def get_table(self, database_name, table_name):\n         result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name)\n         return result[\"Table\"]", "query": "get database table name"}
{"id": "https://github.com/pyGrowler/Growler/blob/90c923ff204f28b86a01d741224987a22f69540f/growler/middleware/cookieparser.py#L34-L56", "method_name": "__call__", "code": "def __call__(self, req, res):\n         if hasattr(req, \"cookies\"):\n             return\n         req.cookies, res.cookies = SimpleCookie(), SimpleCookie()\n         log.info(\"{:d} built with {}\", id(self), json.dumps(self.opts))\n         req.cookies.load(req.headers.get(\"COOKIE\", \"\"))\n         def _gen_cookie():\n             if res.cookies:\n                 cookie_string = res.cookies.output(header=\"\", sep=res.EOL)\n                 return cookie_string\n         res.headers[\"Set-Cookie\"] = _gen_cookie", "query": "create cookie"}
{"id": "https://github.com/loverajoel/sqlalchemy-elasticquery/blob/4c99b81f59e7bb20eaeedb3adbf5126e62bbc25c/sqlalchemy_elasticquery/elastic_query.py#L159-L167", "method_name": "sort", "code": "def sort(self, sort_list):\n         order = []\n         for sort in sort_list:\n             if sort_list[sort] == \"asc\":\n                 order.append(asc(getattr(self.model, sort, None)))\n             elif sort_list[sort] == \"desc\":\n                 order.append(desc(getattr(self.model, sort, None)))\n         return order", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/scikit-hep/uproot/blob/fc406827e36ed87cfb1062806e118f53fd3a3b0a/uproot/interp/numerical.py#L340-L341", "method_name": "empty", "code": "def empty(self):\n        return self.awkward.numpy.empty((0, self.numbytes), dtype=self.todtype)", "query": "empty array"}
{"id": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L9-L14", "method_name": "reverse_words", "code": "def reverse_words(string):\n     arr = string.strip().split()  \n     n = len(arr)\n     reverse(arr, 0, n-1)\n     return \" \".join(arr)", "query": "reverse a string"}
{"id": "https://github.com/OnroerendErfgoed/skosprovider_getty/blob/5aa0b5a8525d607e07b631499ff31bac7a0348b7/skosprovider_getty/providers.py#L390-L396", "method_name": "_sort", "code": "def _sort(self, items, sort, language=\"en\", reverse=False):\n         if sort is None:\n             sort = \"id\"\n         if sort == \"sortlabel\":\n             sort=\"label\"\n         items.sort(key=lambda item: item[sort], reverse=reverse)\n         return items", "query": "sort string list"}
{"id": "https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L102-L108", "method_name": "create_with", "code": "def create_with(\n             cls, event: str = None, observable: T.Union[str, Observable] = None\n     ) -> T.Callable[..., \"ObservableProperty\"]:\n         return functools.partial(cls, event=event, observable=observable)", "query": "get current observable value"}
{"id": "https://github.com/mbj4668/pyang/blob/f2a5cc3142162e5b9ee4e18d154568d939ff63dd/pyang/plugins/check_update.py#L584-L598", "method_name": "chk_unique", "code": "def chk_unique(old, new, ctx):\n     if not hasattr(old, \"i_unique\") or not hasattr(new, \"i_unique\"):\n         return\n     oldunique = []\n     for (u, l) in old.i_unique:\n         oldunique.append((u, [s.arg for s in l]))\n     for (u, l) in new.i_unique:\n         o = util.keysearch([s.arg for s in l], 1, oldunique)\n         if o is not None:\n             oldunique.remove(o)\n         else:\n             err_def_added(u, ctx)", "query": "unique elements"}
{"id": "https://github.com/sony/nnabla/blob/aaf3d33b7cbb38f2a03aa754178ba8f7c8481320/python/src/nnabla/utils/image_utils/common.py#L62-L66", "method_name": "upscale_float_image", "code": "def upscale_float_image(img, as_uint16):\n     if as_uint16:\n         return np.asarray((img * 65535), np.uint16)\n     return np.asarray((img * 255), np.uint8)", "query": "converting uint8 array to image"}
{"id": "https://github.com/fy0/slim/blob/9951a910750888dbe7dd3e98acae9c40efae0689/slim/base/view.py#L128-L136", "method_name": "get_ip", "code": "async def get_ip(self) -> Union[IPv4Address, IPv6Address]:\n         xff = await self.get_x_forwarded_for()\n         if xff: return xff[0]\n         ip_addr = self._request.transport.get_extra_info(\"peername\")[0]\n         return ip_address(ip_addr)", "query": "get current ip address"}
{"id": "https://github.com/snower/TorMySQL/blob/01ff87f03a850a2a6e466700d020878eecbefa5d/tormysql/pool.py#L102-L111", "method_name": "connect", "code": "def connect(self):\n         future = super(RecordQueryConnection, self).connect()\n         origin_query = self._connection.query\n         def query(sql, unbuffered=False):\n             self._last_query_sql = sql\n             return origin_query(sql, unbuffered)\n         self._connection.query = query\n         return future", "query": "connect to sql"}
{"id": "https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/api.py#L1645-L1692", "method_name": "cluster_mini_batch_kmeans", "code": "def cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric=\"euclidean\",\n                               init_strategy=\"kmeans++\", n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs):\n     r\n     from pyemma.coordinates.clustering.kmeans import MiniBatchKmeansClustering\n     res = MiniBatchKmeansClustering(n_clusters=k, max_iter=max_iter, metric=metric, init_strategy=init_strategy,\n                                     batch_size=batch_size, n_jobs=n_jobs, skip=skip, clustercenters=clustercenters)\n     from pyemma.util.reflection import get_default_args\n     cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_mini_batch_kmeans)[\"chunksize\"], **kwargs)\n     if data is not None:\n         res.estimate(data, chunksize=cs)\n     else:\n         res.chunksize = chunksize\n     return res", "query": "k means clustering"}
{"id": "https://github.com/rigetti/pyquil/blob/ec98e453084b0037d69d8c3245f6822a5422593d/pyquil/api/_base_connection.py#L52-L59", "method_name": "post_json", "code": "def post_json(session, url, json):\n     res = session.post(url, json=json)\n     if res.status_code >= 400:\n         raise parse_error(res)\n     return res", "query": "httpclient post json"}
{"id": "https://github.com/datajoint/datajoint-python/blob/4f29bb154a7ed2b8b64b4d3a9c8be4c16b39621c/datajoint/connection.py#L20-L44", "method_name": "conn", "code": "def conn(host=None, user=None, password=None, init_fun=None, reset=False):\n     if not hasattr(conn, \"connection\") or reset:\n         host = host if host is not None else config[\"database.host\"]\n         user = user if user is not None else config[\"database.user\"]\n         password = password if password is not None else config[\"database.password\"]\n         if user is None:  \n             user = input(\"Please enter DataJoint username: \")\n         if password is None:  \n             password = getpass(prompt=\"Please enter DataJoint password: \")\n         init_fun = init_fun if init_fun is not None else config[\"connection.init_function\"]\n         conn.connection = Connection(host, user, password, init_fun)\n     return conn.connection", "query": "postgresql connection"}
{"id": "https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/nfs_share.py#L38-L50", "method_name": "get_xml_node", "code": "def get_xml_node(self):\n         xb = xmlapi.XmlBuilder()\n         ret = []\n         if self.access_hosts is not None:\n             ret.append(xb.list_elements(\"AccessHosts\", self.access_hosts))\n         if self.rw_hosts is not None:\n             ret.append(xb.list_elements(\"RwHosts\", self.rw_hosts))\n         if self.ro_hosts is not None:\n             ret.append(xb.list_elements(\"RoHosts\", self.ro_hosts))\n         if self.root_hosts is not None:\n             ret.append(xb.list_elements(\"RootHosts\", self.root_hosts))\n         return ret", "query": "get all parents of xml node"}
{"id": "https://github.com/gunthercox/ChatterBot/blob/1a03dcb45cba7bdc24d3db5e750582e0cb1518e2/chatterbot/parsing.py#L506-L517", "method_name": "convert_string_to_number", "code": "def convert_string_to_number(value):\n     if value is None:\n         return 1\n     if isinstance(value, int):\n         return value\n     if value.isdigit():\n         return int(value)\n     num_list = map(lambda s: NUMBERS[s], re.findall(numbers + \"+\", value.lower()))\n     return sum(num_list)", "query": "convert string to number"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_disp.py#L7-L12", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser()\n     parser.add_argument( \"xdatcar\" )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hmtk/seismicity/smoothing/smoothed_seismicity.py#L491-L518", "method_name": "write_to_csv", "code": "def write_to_csv(self, filename):\n         fid = open(filename, \"wt\")\n         header_info = [\"Longitude\", \"Latitude\", \"Depth\", \"Observed Count\",\n                        \"Smoothed Rate\", \"b-value\"]\n         writer = csv.DictWriter(fid, fieldnames=header_info)\n         headers = dict((name0, name0) for name0 in header_info)\n         writer.writerow(headers)\n         for row in self.data:\n             if row[4] == 0:\n                 continue\n             row_dict = {\"Longitude\": \"%g\" % row[0],\n                         \"Latitude\": \"%g\" % row[1],\n                         \"Depth\": \"%g\" % row[2],\n                         \"Observed Count\": \"%d\" % row[3],\n                         \"Smoothed Rate\": \"%.6g\" % row[4],\n                         \"b-value\": \"%g\" % self.bval}\n             writer.writerow(row_dict)\n         fid.close()", "query": "write csv"}
{"id": "https://github.com/alexmojaki/littleutils/blob/1132d2d2782b05741a907d1281cd8c001f1d1d9d/littleutils/__init__.py#L716-L739", "method_name": "timer", "code": "def timer(description=\"Operation\", log=None):\n     start = time()\n     yield\n     elapsed = time() - start\n     message = \"%s took %s seconds\" % (description, elapsed)\n     (print if log is None else log.info)(message)", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/totalgood/nlpia/blob/efa01126275e9cd3c3a5151a644f1c798a9ec53f/src/nlpia/clean_alice.py#L59-L82", "method_name": "concatenate_aiml", "code": "def concatenate_aiml(path=\"aiml-en-us-foundation-alice.v1-9.zip\", outfile=\"aiml-en-us-foundation-alice.v1-9.aiml\"):\n     path = find_data_path(path) or path\n     zf = zipfile.ZipFile(path)\n     for name in zf.namelist():\n         if not name.lower().endswith(\".aiml\"):\n             continue\n         with zf.open(name) as fin:\n             happyending = \"\n             for i, line in enumerate(fin):\n                 try:\n                     line = line.decode(\"utf-8\").strip()\n                 except UnicodeDecodeError:\n                     line = line.decode(\"ISO-8859-1\").strip()\n                 if line.lower().startswith(\"</aiml>\") or line.lower().endswith(\"</aiml>\"):\n                     happyending = (i, line)\n                     break\n                 else:\n                     pass\n             if happyending != (i, line):\n                 print(\"Invalid AIML format: {}\nLast line (line number {}) was: {}\nexpected \"</aiml>\"\".format(\n                     name, i, line))", "query": "concatenate several file remove header lines"}
{"id": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/idsortingalgorithm.py#L79-L109", "method_name": "sort", "code": "def sort(self, ids):\n         def extract_int(string):\n             return int(re.sub(r\"[^0-9]\", \"\", string))\n         tmp = list(ids)\n         if self.algorithm == IDSortingAlgorithm.UNSORTED:\n             self.log(u\"Sorting using UNSORTED\")\n         elif self.algorithm == IDSortingAlgorithm.LEXICOGRAPHIC:\n             self.log(u\"Sorting using LEXICOGRAPHIC\")\n             tmp = sorted(ids)\n         elif self.algorithm == IDSortingAlgorithm.NUMERIC:\n             self.log(u\"Sorting using NUMERIC\")\n             tmp = ids\n             try:\n                 tmp = sorted(tmp, key=extract_int)\n             except (ValueError, TypeError) as exc:\n                 self.log_exc(u\"Not all id values contain a numeric part. Returning the id list unchanged.\", exc, False, None)\n         return tmp", "query": "sort string list"}
{"id": "https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/aaf2/core.py#L73-L113", "method_name": "read_properties", "code": "def read_properties(self):\n         stream = self.dir.get(\"properties\")\n         if stream is None:\n             return\n         s = stream.open()\n         f = BytesIO(s.read())\n         (byte_order, version, entry_count) = P_HEADER_STRUCT.unpack(f.read(4))\n         if byte_order != 0x4c:\n             raise NotImplementedError(\"be byteorder\")\n         props = array.array(str(\"H\"))\n         if hasattr(props, \"frombytes\"):\n             props.frombytes(f.read(6 * entry_count))\n         else:\n             props.fromstring(f.read(6 * entry_count))\n         if sys.byteorder == \"big\":\n             props.byteswap()\n         for i in range(entry_count):\n             index = i * 3\n             pid = props[index + 0]\n             format = props[index + 1]\n             byte_size = props[index + 2]\n             data = f.read(byte_size)\n             p = property_formats[format](self, pid, format, version)\n             p.data = data\n             self.property_entries[pid] = p\n         for p in self.property_entries.values():\n             p.decode()\n             if isinstance(p, (properties.StrongRefSetProperty,\n                               properties.StrongRefVectorProperty,\n                               properties.WeakRefArrayProperty)):\n                 p.read_index()", "query": "read properties file"}
{"id": "https://github.com/PSPC-SPAC-buyandsell/von_anchor/blob/78ac1de67be42a676274f4bf71fe12f66e72f309/von_anchor/frill.py#L30-L45", "method_name": "ppjson", "code": "def ppjson(dumpit: Any, elide_to: int = None) -> str:\n     if elide_to is not None:\n         elide_to = max(elide_to, 3) \n     try:\n         rv = json.dumps(json.loads(dumpit) if isinstance(dumpit, str) else dumpit, indent=4)\n     except TypeError:\n         rv = \"{}\".format(pformat(dumpit, indent=4, width=120))\n     return rv if elide_to is None or len(rv) <= elide_to else \"{}...\".format(rv[0 : elide_to - 3])", "query": "pretty print json"}
{"id": "https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L275-L283", "method_name": "get_prob", "code": "def get_prob(self):\n         if self.total < 5:\n             return 0.\n         a, b = self.dup + 1, self.nodup + 1\n         n = a + b\n         p = a / n\n         q = b / n\n         return p - 1.96 * math.sqrt(p * q / n)", "query": "binomial distribution"}
{"id": "https://github.com/azraq27/gini/blob/3c2b5265d096d606b303bfe25ac9adb74b8cee14/gini/matching.py#L63-L68", "method_name": "best_item_from_list", "code": "def best_item_from_list(item,options,fuzzy=90,fname_match=True,fuzzy_fragment=None,guess=False):\n     match = best_match_from_list(item,options,fuzzy,fname_match,fuzzy_fragment,guess)\n     if match:\n         return match[0]\n     return None", "query": "fuzzy match ranking"}
{"id": "https://github.com/facelessuser/backrefs/blob/3b3d60f5d57b02044f880aa29c9c5add0e31a34f/tools/unidatadownload.py#L32-L44", "method_name": "unzip_unicode", "code": "def unzip_unicode(output, version):\n     unzipper = zipfile.ZipFile(os.path.join(output, \"unicodedata\", \"%s.zip\" % version))\n     target = os.path.join(output, \"unicodedata\", version)\n     print(\"Unzipping %s.zip...\" % version)\n     os.makedirs(target)\n     for f in unzipper.namelist():\n         unzipper.extract(f, target)", "query": "unzipping large files"}
{"id": "https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/ouidb/pyauto/ouidb/config.py#L43-L50", "method_name": "get_top_entries", "code": "def get_top_entries(self):\n         query = \"select count(vendor_name) count, vendor_name from mac_vendor group by vendor_name having count > 10 order by count;\"\n         try:\n             db = self.get_db()\n             results = db.execute(query)\n             return [i for i in results]\n         finally:\n             db.close()", "query": "group by count"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/excel.py#L1227-L1269", "method_name": "compile_geometry", "code": "def compile_geometry(lat, lon, elev):\n     logger_excel.info(\"enter compile_geometry\")\n     lat = _remove_geo_placeholders(lat)\n     lon = _remove_geo_placeholders(lon)\n     if len(lat) == 2 and len(lon) == 2:\n         logger_excel.info(\"found 4 coordinates\")\n         geo_dict = geometry_linestring(lat, lon, elev)\n     elif len(lat) == 1 and len(lon) == 1:\n         logger_excel.info(\"found 2 coordinates\")\n         geo_dict = geometry_point(lat, lon, elev)\n     elif (None in lon and None not in lat) or (len(lat) > 0 and len(lon) == 0):\n         geo_dict = geometry_range(lat, elev, \"lat\")\n     elif (None in lat and None not in lon) or (len(lon) > 0 and len(lat) == 0):\n         geo_dict = geometry_range(lat, elev, \"lon\")\n     else:\n         geo_dict = {}\n         logger_excel.warn(\"compile_geometry: invalid coordinates: lat: {}, lon: {}\".format(lat, lon))\n     logger_excel.info(\"exit compile_geometry\")\n     return geo_dict", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/codeforamerica/three/blob/67b4a4b233a57aa7995d01f6b0f69c2e85aea6c0/three/core.py#L158-L170", "method_name": "convert", "code": "def convert(self, content, conversion):\n         if not conversion:\n             data = content\n         elif self.format == \"json\":\n             data = json.loads(content)\n         elif self.format == \"xml\":\n             content = xml(content)\n             first = list(content.keys())[0]\n             data = content[first]\n         else:\n             data = content\n         return data", "query": "json to xml conversion"}
{"id": "https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/storage/google/cloud/storage/blob.py#L2017-L2034", "method_name": "_raise_from_invalid_response", "code": "def _raise_from_invalid_response(error):\n     response = error.response\n     error_message = str(error)\n     message = u\"{method} {url}: {error}\".format(\n         method=response.request.method, url=response.request.url, error=error_message\n     )\n     raise exceptions.from_http_status(response.status_code, message, response=response)", "query": "custom http error response"}
{"id": "https://github.com/scivision/lowtran/blob/9954d859e53437436103f9ab54a7e2602ecaa1b7/lowtran/plots.py#L12-L49", "method_name": "plotscatter", "code": "def plotscatter(irrad: xarray.Dataset, c1: Dict[str, Any], log: bool = False):\n     fg = figure()\n     axs = fg.subplots(2, 1, sharex=True)\n     transtxt = \"Transmittance\"\n     ax = axs[0]\n     ax.plot(irrad.wavelength_nm, irrad[\"transmission\"].squeeze())\n     ax.set_title(transtxt)\n     ax.set_ylabel(\"Transmission (unitless)\")\n     ax.grid(True)\n     ax.legend(irrad.angle_deg.values)\n     ax = axs[1]\n     if plotNp:\n         Np = (irrad[\"pathscatter\"]*10000) * (irrad.wavelength_nm*1e9)/(h*c)\n         ax.plot(irrad.wavelength_nm, Np)\n         ax.set_ylabel(\"Photons [s$^{-1}$ \"+UNITS)\n     else:\n         ax.plot(irrad.wavelength_nm, irrad[\"pathscatter\"].squeeze())\n         ax.set_ylabel(\"Radiance [W \"+UNITS)\n     ax.set_xlabel(\"wavelength [nm]\")\n     ax.set_title(\"Single-scatter Path Radiance\")\n     ax.invert_xaxis()\n     ax.autoscale(True, axis=\"x\", tight=True)\n     ax.grid(True)\n     if log:\n         ax.set_yscale(\"log\")\n     try:\n         fg.suptitle(f\"Obs. to Space: zenith angle: {c1[\"angle\"]} deg., \")\n     except (AttributeError, TypeError):\n         pass", "query": "scatter plot"}
{"id": "https://github.com/thespacedoctor/polyglot/blob/98038d746aa67e343b73b3ccee1e02d31dab81ec/polyglot/ebook.py#L246-L282", "method_name": "_tmp_html_file", "code": "def _tmp_html_file(\n             self,\n             content):\n         self.log.debug(\"starting the ``_tmp_html_file`` method\")\n         content =  % locals()\n         now = datetime.now()\n         now = now.strftime(\"%Y%m%dt%H%M%S%f\")\n         pathToWriteFile = \"/tmp/%(now)s.html\" % locals()\n         try:\n             self.log.debug(\"attempting to open the file %s\" %\n                            (pathToWriteFile,))\n             writeFile = codecs.open(\n                 pathToWriteFile, encoding=\"utf-8\", mode=\"w\")\n         except IOError, e:\n             message = \"could not open the file %s\" % (pathToWriteFile,)\n             self.log.critical(message)\n             raise IOError(message)\n         writeFile.write(content)\n         writeFile.close()\n         self.log.debug(\"completed the ``_tmp_html_file`` method\")\n         return pathToWriteFile", "query": "output to html file"}
{"id": "https://github.com/go-macaroon-bakery/py-macaroon-bakery/blob/63ce1ef1dabe816eb8aaec48fbb46761c34ddf77/macaroonbakery/bakery/_macaroon.py#L242-L248", "method_name": "deserialize_json", "code": "def deserialize_json(cls, serialized_json):\n         serialized = json.loads(serialized_json)\n         return Macaroon.from_dict(serialized)", "query": "deserialize json"}
{"id": "https://github.com/tensorforce/tensorforce/blob/520a8d992230e382f08e315ede5fc477f5e26bfb/tensorforce/core/distributions/gaussian.py#L83-L93", "method_name": "tf_sample", "code": "def tf_sample(self, distr_params, deterministic):\n         mean, stddev, _ = distr_params\n         definite = mean\n         normal_distribution = tf.random_normal(shape=tf.shape(input=mean))\n         sampled = mean + stddev * normal_distribution\n         return tf.where(condition=deterministic, x=definite, y=sampled)", "query": "normal distribution"}
{"id": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/utils/http.py#L175-L196", "method_name": "setcookie", "code": "def setcookie(self, key, value, max_age=None, expires=None, path=\"/\", domain=None, secure=None, httponly=False):\n         newcookie = Morsel()\n         newcookie.key = key\n         newcookie.value = value\n         newcookie.coded_value = value\n         if max_age is not None:\n             newcookie[\"max-age\"] = max_age\n         if expires is not None:\n             newcookie[\"expires\"] = expires\n         if path is not None:\n             newcookie[\"path\"] = path\n         if domain is not None:\n             newcookie[\"domain\"] = domain\n         if secure:\n             newcookie[\"secure\"] = secure\n         if httponly:\n             newcookie[\"httponly\"] = httponly\n         self.sent_cookies = [c for c in self.sent_cookies if c.key != key]\n         self.sent_cookies.append(newcookie)", "query": "create cookie"}
{"id": "https://github.com/fadhiilrachman/line-py/blob/b7f5f2b3fc09fa3fbf6088d7ebdaf9e44d96ba69/linepy/server.py#L18-L19", "method_name": "urlEncode", "code": "def urlEncode(self, url, path, params=[]):\n        return url + path + \"?\" + urllib.parse.urlencode(params)", "query": "encode url"}
{"id": "https://github.com/TheRobotCarlson/DocxMerge/blob/df8d2add5ff4f33e31d36a6bd3d4495140334538/DocxMerge/DocxMerge.py#L274-L293", "method_name": "replace_doc_text", "code": "def replace_doc_text(file, replacements):\n     document = Document(file)\n     paragraphs = document.paragraphs\n     changes = False\n     for paragraph in paragraphs:\n         text = paragraph.text\n         for original, replace in replacements.items():\n             if original in replace and replace in text:\n                 continue\n             if original in text:\n                 changes = True\n                 text = text.replace(original, replace)\n                 paragraph.text = text\n     if changes:\n         print(\"changing {}\".format(file))\n     document.save(file)", "query": "replace in file"}
{"id": "https://github.com/binux/pyspider/blob/3fccfabe2b057b7a56d4a4c79dc0dd6cd2239fe9/pyspider/libs/utils.py#L72-L130", "method_name": "format_date", "code": "def format_date(date, gmt_offset=0, relative=True, shorter=False, full_format=False):\n     if not date:\n         return \"-\"\n     if isinstance(date, float) or isinstance(date, int):\n         date = datetime.datetime.utcfromtimestamp(date)\n     now = datetime.datetime.utcnow()\n     if date > now:\n         if relative and (date - now).seconds < 60:\n             date = now\n         else:\n             full_format = True\n     local_date = date - datetime.timedelta(minutes=gmt_offset)\n     local_now = now - datetime.timedelta(minutes=gmt_offset)\n     local_yesterday = local_now - datetime.timedelta(hours=24)\n     difference = now - date\n     seconds = difference.seconds\n     days = difference.days\n     format = None\n     if not full_format:\n         ret_, fff_format = fix_full_format(days, seconds, relative, shorter, local_date, local_yesterday)\n         format = fff_format\n         if ret_:\n             return format\n         else:\n             format = format\n     if format is None:\n         format = \"%(month_name)s %(day)s, %(year)s\" if shorter else \\\n             \"%(month_name)s %(day)s, %(year)s at %(time)s\"\n     str_time = \"%d:%02d\" % (local_date.hour, local_date.minute)\n     return format % {\n         \"month_name\": local_date.strftime(\"%b\"),\n         \"weekday\": local_date.strftime(\"%A\"),\n         \"day\": str(local_date.day),\n         \"year\": str(local_date.year),\n         \"month\": local_date.month,\n         \"time\": str_time\n     }", "query": "format date"}
{"id": "https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L84-L93", "method_name": "hex2dec", "code": "def hex2dec(s):\n     if not isinstance(s, str):\n         s = str(s)\n     return int(s.upper(), 16)", "query": "convert decimal to hex"}
{"id": "https://github.com/GreatFruitOmsk/tailhead/blob/a3b1324a39935f8ffcfda59328a9a458672889d9/tailhead/__init__.py#L72-L79", "method_name": "read", "code": "def read(self, read_size=-1):\n         read_str = self.file.read(read_size)\n         return len(read_str), read_str", "query": "read properties file"}
{"id": "https://github.com/maartenbreddels/ipyvolume/blob/e68b72852b61276f8e6793bc8811f5b2432a155f/ipyvolume/widgets.py#L434-L441", "method_name": "scatter", "code": "def scatter(x, y, z, color=(1, 0, 0), s=0.01):\n     global _last_figure\n     fig = _last_figure\n     if fig is None:\n         fig = volshow(None)\n     fig.scatter = Scatter(x=x, y=y, z=z, color=color, size=s)\n     fig.volume.scatter = fig.scatter\n     return fig", "query": "scatter plot"}
{"id": "https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/im.py#L171-L200", "method_name": "_do_put", "code": "def _do_put(self, uri, **kwargs):\n         scaleioapi_put_headers = {\"content-type\":\"application/json\"}\n         print \"_do_put()\"\n         if kwargs:\n             for key, value in kwargs.iteritems():\n                 if key == \"json\":\n                     payload = value\n         try:\n             self.logger.debug(\"do_put(): \" + \"{}\".format(uri))\n             response = self._session.put(url, headers=scaleioapi_put_headers, verify_ssl=self._im_verify_ssl, data=json.dumps(payload))\n             self.logger.debug(\"_do_put() - Response: \" + \"{}\".format(response.text))\n             if response.status_code == requests.codes.ok:\n                 return response\n             else:\n                 self.logger.error(\"_do_put() - HTTP response error: \" + \"{}\".format(response.status_code))\n                 raise RuntimeError(\"_do_put() - HTTP response error\" + response.status_code)\n         except:\n             raise RuntimeError(\"_do_put() - Communication error with ScaleIO gateway\")\n         return response", "query": "custom http error response"}
{"id": "https://github.com/atlassian-api/atlassian-python-api/blob/540d269905c3e7547b666fe30c647b2d512cf358/atlassian/utils.py#L80-L103", "method_name": "html_row_with_ordered_headers", "code": "def html_row_with_ordered_headers(data, headers):\n     html = \"\n\\t<tr>\"\n     for header in headers:\n         element = data[header]\n         if isinstance(element, list):\n             element = html_list(element)\n         if is_email(element):\n             element = html_email(element)\n         html += \"<td>{}</td>\".format(element)\n     return html + \"</tr>\"", "query": "reading element from html - <td>"}
{"id": "https://github.com/googleapis/google-auth-library-python/blob/2c6ad78917e936f38f87c946209c8031166dc96e/google/auth/_helpers.py#L130-L173", "method_name": "update_query", "code": "def update_query(url, params, remove=None):\n     if remove is None:\n         remove = []\n     parts = urllib.parse.urlparse(url)\n     query_params = urllib.parse.parse_qs(parts.query)\n     query_params.update(params)\n     query_params = {\n         key: value for key, value\n         in six.iteritems(query_params)\n         if key not in remove}\n     new_query = urllib.parse.urlencode(query_params, doseq=True)\n     new_parts = parts._replace(query=new_query)\n     return urllib.parse.urlunparse(new_parts)", "query": "parse query string in url"}
{"id": "https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L37-L57", "method_name": "write_csv_header", "code": "def write_csv_header(mol, csv_writer):\n     line = []\n     line.append(\"id\")\n     line.append(\"status\")\n     queryList = mol.properties.keys()\n     for queryLabel in queryList:\n         line.append(queryLabel)\n     csv_writer.writerow(line)", "query": "write csv"}
{"id": "https://github.com/SpotlightData/preprocessing/blob/180c6472bc2642afbd7a1ece08d0b0d14968a708/preprocessing/spellcheck.py#L18-L28", "method_name": "correct_word", "code": "def correct_word(word_string):\n     if word_string is None:\n         return \"\"\n     elif isinstance(word_string, str):\n         return max(find_candidates(word_string), key=find_word_prob)\n     else:\n         raise InputError(\"string or none type variable not passed as argument to correct_word\")", "query": "determine a string is a valid word"}
{"id": "https://github.com/roclark/sportsreference/blob/ea0bae432be76450e137671d2998eb38f962dffd/sportsreference/mlb/schedule.py#L172-L179", "method_name": "datetime", "code": "def datetime(self):\n         date_string = \"%s %s\" % (self._date, self._year)\n         date_string = re.sub(r\" \\(\\d+\\)\", \"\", date_string)\n         return datetime.strptime(date_string, \"%A, %b %d %Y\")", "query": "string to date"}
{"id": "https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/pipeline/factors/factor.py#L786-L843", "method_name": "linear_regression", "code": "def linear_regression(self, target, regression_length, mask=NotSpecified):\n         from .statistical import RollingLinearRegression\n         return RollingLinearRegression(\n             dependent=self,\n             independent=target,\n             regression_length=regression_length,\n             mask=mask,\n         )", "query": "linear regression"}
{"id": "https://github.com/KelSolaar/Foundations/blob/5c141330faf09dad70a12bc321f4c564917d0a91/foundations/decorators.py#L80-L129", "method_name": "memoize", "code": "def memoize(cache=None):\n     if cache is None:\n         cache = {}\n     def memoize_decorator(object):\n         @functools.wraps(object)\n         def memoize_wrapper(*args, **kwargs):\n             if kwargs:\n                 key = args, frozenset(kwargs.iteritems())\n             else:\n                 key = args\n             if key not in cache:\n                 cache[key] = object(*args, **kwargs)\n             return cache[key]\n         return memoize_wrapper\n     return memoize_decorator", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/AshleySetter/optoanalysis/blob/9d390acc834d70024d47b574aea14189a5a5714e/optoanalysis/optoanalysis/optoanalysis.py#L3311-L3329", "method_name": "_GetRealImagArray", "code": "def _GetRealImagArray(Array):\n     ImagArray = _np.array([num.imag for num in Array])\n     RealArray = _np.array([num.real for num in Array])\n     return RealArray, ImagArray", "query": "initializing array"}
{"id": "https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/storage/google/cloud/storage/blob.py#L2017-L2034", "method_name": "_raise_from_invalid_response", "code": "def _raise_from_invalid_response(error):\n     response = error.response\n     error_message = str(error)\n     message = u\"{method} {url}: {error}\".format(\n         method=response.request.method, url=response.request.url, error=error_message\n     )\n     raise exceptions.from_http_status(response.status_code, message, response=response)", "query": "custom http error response"}
{"id": "https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/utils.py#L147-L165", "method_name": "epoch_to_human_time", "code": "def epoch_to_human_time(epoch_time):\n     if isinstance(epoch_time, int):\n         try:\n             d = datetime.datetime.fromtimestamp(epoch_time / 1000)\n             return d.strftime(\"%m-%d-%Y %H:%M:%S \")\n         except ValueError:\n             return None", "query": "convert a utc time to epoch"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L67-L79", "method_name": "utc_epoch", "code": "def utc_epoch():\n     d = dt.datetime(1970, 1, 1)\n     d = d.replace(tzinfo=utc)\n     return d", "query": "convert a utc time to epoch"}
{"id": "https://github.com/OnroerendErfgoed/skosprovider_getty/blob/5aa0b5a8525d607e07b631499ff31bac7a0348b7/skosprovider_getty/providers.py#L390-L396", "method_name": "_sort", "code": "def _sort(self, items, sort, language=\"en\", reverse=False):\n         if sort is None:\n             sort = \"id\"\n         if sort == \"sortlabel\":\n             sort=\"label\"\n         items.sort(key=lambda item: item[sort], reverse=reverse)\n         return items", "query": "sort string list"}
{"id": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/file_utils.py#L138-L170", "method_name": "csv_to_json", "code": "def csv_to_json(csv_filepath, json_filepath, fieldnames, ignore_first_line=True):\n     import csv\n     import json\n     csvfile = open(csv_filepath, \"r\")\n     jsonfile = open(json_filepath, \"w\")\n     reader = csv.DictReader(csvfile, fieldnames)\n     rows = []\n     if ignore_first_line:\n         next(reader)\n     for row in reader:\n         rows.append(row)\n     json.dump(rows, jsonfile)\n     jsonfile.close()\n     csvfile.close()", "query": "convert json to csv"}
{"id": "https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/stickers.py#L281-L291", "method_name": "pdf_from_post", "code": "def pdf_from_post(self):\n         html = self.request.form.get(\"html\")\n         style = self.request.form.get(\"style\")\n         reporthtml = \"<html><head>{0}</head><body>{1}</body></html>\"\n         reporthtml = reporthtml.format(style, html)\n         reporthtml = safe_unicode(reporthtml).encode(\"utf-8\")\n         pdf_fn = tempfile.mktemp(suffix=\".pdf\")\n         pdf_file = createPdf(htmlreport=reporthtml, outfile=pdf_fn)\n         return pdf_file", "query": "convert html to pdf"}
{"id": "https://github.com/lowandrew/OLCTools/blob/88aa90ac85f84d0bbeb03e43c29b0a9d36e4ce2a/spadespipeline/fileprep.py#L28-L47", "method_name": "prep", "code": "def prep(self):\n         while True:\n             sample = self.queue.get()\n             if not os.path.isfile(sample.general.combined):\n                 for read in sample.general.fastqfiles:\n                     if \".gz\" in read:\n                         with open(sample.general.combined, \"wb\") as combined:\n                             with gzip.open(read, \"rb\") as fastq:\n                                 combined.write(fastq.read())\n                     else:\n                         with open(sample.general.combined, \"w\") as combined:\n                             with open(read, \"r\") as fastq:\n                                 combined.write(fastq.read())\n             self.queue.task_done()", "query": "read the contents of a .gz compressed file?"}
{"id": "https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L127-L131", "method_name": "__add__", "code": "def __add__(self, other):\n         if isinstance(other, Matrix):\n             return Matrix(self.matrix + other.matrix)\n         else:\n             return Matrix(self.matrix + other)", "query": "matrix multiply"}
{"id": "https://github.com/J535D165/recordlinkage/blob/87a5f4af904e0834047cd07ff1c70146b1e6d693/recordlinkage/algorithms/string.py#L52-L67", "method_name": "levenshtein_similarity", "code": "def levenshtein_similarity(s1, s2):\n     conc = pandas.Series(list(zip(s1, s2)))\n     def levenshtein_apply(x):\n         try:\n             return 1 - jellyfish.levenshtein_distance(x[0], x[1]) \\\n                 / np.max([len(x[0]), len(x[1])])\n         except Exception as err:\n             if pandas.isnull(x[0]) or pandas.isnull(x[1]):\n                 return np.nan\n             else:\n                 raise err\n     return conc.apply(levenshtein_apply)", "query": "string similarity levenshtein"}
{"id": "https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/graph.py#L58-L65", "method_name": "get_edges", "code": "def get_edges(self, node):\n         return [\n             (node, child)\n             for child in self.get_children(node)\n         ] + [\n             (parent, node)\n             for parent in self.get_parents(node)\n         ]", "query": "get all parents of xml node"}
{"id": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/math/cross.py#L96-L102", "method_name": "permutations", "code": "def permutations(x):\n   if len(x) > 1:\n     for permutation in permutations(x[1:]):\n       for i in xrange(len(permutation)+1):\n         yield permutation[:i] + x[0:1] + permutation[i:]\n   else: yield x", "query": "all permutations of a list"}
{"id": "https://github.com/ga4gh/ga4gh-common/blob/ea1b562dce5bf088ac4577b838cfac7745f08346/ga4gh/common/utils.py#L30-L40", "method_name": "getPathOfExecutable", "code": "def getPathOfExecutable(executable):\n     exe_paths = os.environ[\"PATH\"].split(\":\")\n     for exe_path in exe_paths:\n         exe_file = os.path.join(exe_path, executable)\n         if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK):\n             return exe_file\n     return None", "query": "get executable path"}
{"id": "https://github.com/mlenzen/collections-extended/blob/ee9e86f6bbef442dbebcb3a5970642c5c969e2cf/collections_extended/bags.py#L89-L96", "method_name": "_set_count", "code": "def _set_count(self, elem, count):\n \t\tif count < 0:\n \t\t\traise ValueError\n \t\tself._size += count - self.count(elem)\n \t\tif count == 0:\n \t\t\tself._dict.pop(elem, None)\n \t\telse:\n \t\t\tself._dict[elem] = count", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/mlenzen/collections-extended/blob/ee9e86f6bbef442dbebcb3a5970642c5c969e2cf/collections_extended/bags.py#L89-L96", "method_name": "_set_count", "code": "def _set_count(self, elem, count):\n \t\tif count < 0:\n \t\t\traise ValueError\n \t\tself._size += count - self.count(elem)\n \t\tif count == 0:\n \t\t\tself._dict.pop(elem, None)\n \t\telse:\n \t\t\tself._dict[elem] = count", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/maartenbreddels/ipyvolume/blob/e68b72852b61276f8e6793bc8811f5b2432a155f/ipyvolume/widgets.py#L434-L441", "method_name": "scatter", "code": "def scatter(x, y, z, color=(1, 0, 0), s=0.01):\n     global _last_figure\n     fig = _last_figure\n     if fig is None:\n         fig = volshow(None)\n     fig.scatter = Scatter(x=x, y=y, z=z, color=color, size=s)\n     fig.volume.scatter = fig.scatter\n     return fig", "query": "scatter plot"}
{"id": "https://github.com/vmirly/pyclust/blob/bdb12be4649e70c6c90da2605bc5f4b314e2d07e/pyclust/_kmeans.py#L78-L101", "method_name": "_kmeans", "code": "def _kmeans(X, n_clusters, max_iter, n_trials, tol):\n     n_samples, n_features = X.shape[0], X.shape[1]\n     centers_best = np.empty(shape=(n_clusters,n_features), dtype=float)\n     labels_best  = np.empty(shape=n_samples, dtype=int)\n     for i in range(n_trials):\n         centers, labels, sse_tot, sse_arr, n_iter  = _kmeans_run(X, n_clusters, max_iter, tol)\n         if i==0:\n             sse_tot_best = sse_tot\n             sse_arr_best = sse_arr\n             n_iter_best = n_iter\n             centers_best = centers.copy()\n             labels_best  = labels.copy()\n         if sse_tot < sse_tot_best:\n             sse_tot_best = sse_tot\n             sse_arr_best = sse_arr\n             n_iter_best = n_iter\n             centers_best = centers.copy()\n             labels_best  = labels.copy()\n     return(centers_best, labels_best, sse_arr_best, n_iter_best)", "query": "k means clustering"}
{"id": "https://github.com/ofek/pypinfo/blob/48d56e690d7667ae5854752c3a2dc07e321d5637/pypinfo/core.py#L65-L70", "method_name": "format_date", "code": "def format_date(date, timestamp_format):\n     try:\n         date = DATE_ADD.format(int(date))\n     except ValueError:\n         date = timestamp_format.format(date)\n     return date", "query": "format date"}
{"id": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/generic.py#L1878-L1924", "method_name": "empty", "code": "def empty(self):\n         return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)", "query": "empty array"}
{"id": "https://github.com/ToucanToco/toucan-data-sdk/blob/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a/toucan_data_sdk/sdk.py#L173-L191", "method_name": "extract_zip", "code": "def extract_zip(zip_file_path):\n     dfs = {}\n     with zipfile.ZipFile(zip_file_path, mode=\"r\") as z_file:\n         names = z_file.namelist()\n         for name in names:\n             content = z_file.read(name)\n             _, tmp_file_path = tempfile.mkstemp()\n             try:\n                 with open(tmp_file_path, \"wb\") as tmp_file:\n                     tmp_file.write(content)\n                 dfs[name] = joblib.load(tmp_file_path)\n             finally:\n                 shutil.rmtree(tmp_file_path, ignore_errors=True)\n     return dfs", "query": "extract zip file recursively"}
{"id": "https://github.com/milinda/htrc-sdk-for-python/blob/d08dbba1a441dcb2bade47deaebc07be68c3a173/htrc/utils.py#L45-L62", "method_name": "unzip", "code": "def unzip(zip_content, dest_dir):\n     with zipfile.ZipFile(zip_content, \"r\") as zf:\n         for member in zf.infolist():\n             words = member.filename.split(\"/\")\n             path = dest_dir\n             for word in words[:-1]:\n                 drive, word = os.path.splitdrive(word)\n                 head, word = os.path.split(word)\n                 if word in (os.curdir, os.pardir, \"\"):\n                     continue\n                 path = os.path.join(path, word)\n             zf.extract(member, path)", "query": "unzipping large files"}
{"id": "https://github.com/aboSamoor/polyglot/blob/d0d2aa8d06cec4e03bd96618ae960030f7069a17/polyglot/decorators.py#L23-L32", "method_name": "memoize", "code": "def memoize(obj):\n   cache = obj.cache = {}\n   @functools.wraps(obj)\n   def memoizer(*args, **kwargs):\n     key = tuple(list(args) + sorted(kwargs.items()))\n     if key not in cache:\n       cache[key] = obj(*args, **kwargs)\n     return cache[key]\n   return memoizer", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/GibbsConsulting/jupyter-plotly-dash/blob/19e3898372ddf7c1d20292eae1ea0df9e0808fe2/jupyter_plotly_dash/dash_wrapper.py#L104-L120", "method_name": "_repr_html_", "code": "def _repr_html_(self):\n         url = self.get_app_root_url()\n         da_id = self.session_id()\n         comm = locate_jpd_comm(da_id, self, url[1:-1])\n         external = self.add_external_link and \"<hr/><a href=\"{url}\" target=\"_new\">Open in new window</a>\".format(url=url) or \"\"\n         fb = \"frameborder=\"%i\"\" %(self.frame and 1 or 0)\n         iframe =  %{\"url\" : url,\n             \"da_id\" : da_id,\n             \"external\" : external,\n             \"width\" : self.width,\n             \"height\" : self.height,\n             \"frame\": fb,}\n         return iframe", "query": "get html of website"}
{"id": "https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/excel.py#L155-L157", "method_name": "save", "code": "def save(self):\n        pyexcel_export.save_data(self.excel_filename, data=self.excel_raw, retain_meta=True,\n                                 created=self.created, modified=datetime.now().isoformat())", "query": "export to excel"}
{"id": "https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/nfs_share.py#L38-L50", "method_name": "get_xml_node", "code": "def get_xml_node(self):\n         xb = xmlapi.XmlBuilder()\n         ret = []\n         if self.access_hosts is not None:\n             ret.append(xb.list_elements(\"AccessHosts\", self.access_hosts))\n         if self.rw_hosts is not None:\n             ret.append(xb.list_elements(\"RwHosts\", self.rw_hosts))\n         if self.ro_hosts is not None:\n             ret.append(xb.list_elements(\"RoHosts\", self.ro_hosts))\n         if self.root_hosts is not None:\n             ret.append(xb.list_elements(\"RootHosts\", self.root_hosts))\n         return ret", "query": "get all parents of xml node"}
{"id": "https://github.com/crs4/pydoop/blob/f375be2a06f9c67eaae3ce6f605195dbca143b2b/pydoop/__init__.py#L183-L193", "method_name": "read_properties", "code": "def read_properties(fname):\n     parser = configparser.SafeConfigParser()\n     parser.optionxform = str  \n     try:\n         with open(fname) as f:\n             parser_read(parser, AddSectionWrapper(f))\n     except IOError as e:\n         if e.errno != errno.ENOENT:\n             raise\n         return None  \n     return dict(parser.items(AddSectionWrapper.SEC_NAME))", "query": "read properties file"}
{"id": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mssql_hook.py#L38-L49", "method_name": "get_conn", "code": "def get_conn(self):\n         conn = self.get_connection(self.mssql_conn_id)\n         conn = pymssql.connect(\n             server=conn.host,\n             user=conn.login,\n             password=conn.password,\n             database=self.schema or conn.schema,\n             port=conn.port)\n         return conn", "query": "postgresql connection"}
{"id": "https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/db/__init__.py#L56-L62", "method_name": "execute", "code": "def execute(self, sql, args=()):\n         if isinstance(sql, (list, tuple)):\n             sql = \" \".join(sql)\n         with sqlite3.connect(self.path) as con:\n             return con.execute(sql, args)", "query": "connect to sql"}
{"id": "https://github.com/City-of-Helsinki/django-munigeo/blob/a358f0dd0eae3add660b650abec2acb4dd86d04c/munigeo/api.py#L222-L241", "method_name": "parse_lat_lon", "code": "def parse_lat_lon(query_params):\n     lat = query_params.get(\"lat\", None)\n     lon = query_params.get(\"lon\", None)\n     if not lat and not lon:\n         return None\n     if not lat or not lon:\n         raise ParseError(\"you must supply both \"lat\" and \"lon\"\")\n     try:\n         lat = float(lat)\n         lon = float(lon)\n     except ValueError:\n         raise ParseError(\"\"lat\" and \"lon\" must be floating point numbers\")\n     point = Point(lon, lat, srid=DEFAULT_SRID)\n     if DEFAULT_SRID != DATABASE_SRID:\n         ct = CoordTransform(SpatialReference(DEFAULT_SRID),\n                             SpatialReference(DATABASE_SRID))\n         point.transform(ct)\n     return point", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_formatting.py#L29-L32", "method_name": "format_dateformat", "code": "def format_dateformat(dates):\n     format = dateformat.DateFormat(\"YYYY-MM-DD hh:mm:ss\")\n     for date in dates:\n         assert isinstance(format.format(date), str)", "query": "format date"}
{"id": "https://github.com/ricobl/django-importer/blob/6967adfa7a286be7aaf59d3f33c6637270bd9df6/sample_project/tasks/importers.py#L64-L88", "method_name": "parse_date", "code": "def parse_date(self, item, field_name, source_name):\n         now = datetime.now().date()\n         val = self.get_value(item, source_name)\n         week_day, day = val.split()\n         day = int(day)\n         if now.day < day:\n             if now.month == 1:\n                 now = now.replace(month=12, year=now.year-1)\n             else:\n                 now = now.replace(month=now.month-1)\n         now = now.replace(day=day)\n         return now", "query": "get current date"}
{"id": "https://github.com/theonion/django-bulbs/blob/0c0e6e3127a7dc487b96677fab95cacd2b3806da/bulbs/utils/methods.py#L66-L68", "method_name": "datetime_to_epoch_seconds", "code": "def datetime_to_epoch_seconds(value):\n    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=timezone.utc)\n    return (value - epoch).total_seconds()", "query": "convert a utc time to epoch"}
{"id": "https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/lib/cls_filelist.py#L192-L228", "method_name": "save_filelist", "code": "def save_filelist(self, opFile, opFormat, delim=\",\", qu= \n         uses a List of files and collects meta data on them and saves \n         to an text file as a list or with metadata depending on opFormat. \n         \"\"\" \n         op_folder = os.path.dirname(opFile) \n         if op_folder is not None:   \n             if not os.path.exists(op_folder): \n                 os.makedirs(op_folder) \n         with open(opFile,\"w\") as fout: \n             fout.write(\"fullFilename\" + delim) \n             for colHeading in opFormat: \n                 fout.write(colHeading + delim) \n             fout.write(\"\n\") \n             for f in self.filelist: \n                 line = qu + f + qu + delim \n                 try: \n                     for fld in opFormat: \n                         if fld == \"name\": \n                             line = line + qu + os.path.basename(f) + qu + delim \n                         if fld == \"date\": \n                             line = line + qu + self.GetDateAsString(f) + qu + delim \n                         if fld == \"size\": \n                             line = line + qu + str(os.path.getsize(f)) + qu + delim \n                         if fld == \"path\": \n                             line = line + qu + os.path.dirname(f) + qu + delim \n                 except IOError: \n                     line += \"\n\"   \n                 try: \n                     fout.write (str(line.encode(\"ascii\", \"ignore\").decode(\"utf-8\"))) \n                     fout.write (\"\n\") \n                 except IOError: \n                     pass", "query": "save list to file"}
{"id": "https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/tasks.py#L4149-L4153", "method_name": "set_workdir", "code": "def set_workdir(self, workdir, chroot=False):\n         super().set_workdir(workdir, chroot=chroot)\n         self.output_file = self.log_file", "query": "set working directory"}
{"id": "https://github.com/Ezhil-Language-Foundation/open-tamil/blob/b7556e88878d29bbc6c944ee17cdd3f75b8ea9f0/solthiruthi/datastore.py#L340-L358", "method_name": "isWord", "code": "def isWord(self,word,ret_ref_trie=False):\n         letters = utf8.get_letters(word)\n         wLen = len(letters)\n         ref_trie = self.trie\n         ref_word_limits = self.word_limits\n         for itr,letter in enumerate(letters):\n             idx = self.getidx( letter )\n             if itr == (wLen-1):\n                 break\n             if not ref_trie[idx][1]:\n                 return False \n             ref_trie = ref_trie[idx][1]\n             ref_word_limits = ref_word_limits[idx][1]\n         if ret_ref_trie:\n             return ref_word_limits[idx][0],ref_trie,ref_word_limits\n         return ref_word_limits[idx][0]", "query": "determine a string is a valid word"}
{"id": "https://github.com/tanghaibao/goatools/blob/407682e573a108864a79031f8ca19ee3bf377626/goatools/obo_parser.py#L205-L211", "method_name": "get_all_parents", "code": "def get_all_parents(self):\n         all_parents = set()\n         for parent in self.parents:\n             all_parents.add(parent.item_id)\n             all_parents  = parent.get_all_parents()\n         return all_parents", "query": "get all parents of xml node"}
{"id": "https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/data/bcolz_daily_bars.py#L209-L237", "method_name": "write_csvs", "code": "def write_csvs(self,\n                    asset_map,\n                    show_progress=False,\n                    invalid_data_behavior=\"warn\"):\n         read = partial(\n             read_csv,\n             parse_dates=[\"day\"],\n             index_col=\"day\",\n             dtype=self._csv_dtypes,\n         )\n         return self.write(\n             ((asset, read(path)) for asset, path in iteritems(asset_map)),\n             assets=viewkeys(asset_map),\n             show_progress=show_progress,\n             invalid_data_behavior=invalid_data_behavior,\n         )", "query": "write csv"}
{"id": "https://github.com/lisael/fastidious/blob/2542db9de779ddabc3a64e9eb19a4e2de99741dc/fastidious/expressions.py#L287-L301", "method_name": "memoize", "code": "def memoize(self, code):\n         pk = hash(self.as_grammar())\n         return.format(\n             pk,\n             self._indent(code, 1),\n             self.id,\n             repr(self.rulename)\n         )", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/rnc_db.py#L1649-L1908", "method_name": "_connect", "code": "def _connect(self,\n                  engine: str = None,\n                  interface: str = None,\n                  host: str = None,\n                  port: int = None,\n                  database: str = None,\n                  driver: str = None,\n                  dsn: str = None,\n                  odbc_connection_string: str = None,\n                  user: str = None,\n                  password: str = None,\n                  autocommit: bool = True,\n                  charset: str = \"utf8\",\n                  use_unicode: bool = True) -> bool:\n         if engine == ENGINE_MYSQL:\n             self.flavour = MySQL()\n             self.schema = database\n         elif engine == ENGINE_SQLSERVER:\n             self.flavour = SQLServer()\n             if database:\n                 self.schema = database\n             else:\n                 self.schema = \"dbo\"  \n         elif engine == ENGINE_ACCESS:\n             self.flavour = Access()\n             self.schema = \"dbo\"  \n         else:\n             raise ValueError(\"Unknown engine\")\n         if interface is None:\n             if engine == ENGINE_MYSQL:\n                 interface = INTERFACE_MYSQL\n             else:\n                 interface = INTERFACE_ODBC\n         if port is None:\n             if engine == ENGINE_MYSQL:\n                 port = 3306\n             elif engine == ENGINE_SQLSERVER:\n                 port = 1433\n         if driver is None:\n             if engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n                 driver = \"{MySQL ODBC 5.1 Driver}\"\n         self._engine = engine\n         self._interface = interface\n         self._server = host\n         self._port = port\n         self._database = database\n         self._user = user\n         self._password = password\n         self._charset = charset\n         self._use_unicode = use_unicode\n         self.autocommit = autocommit\n         log.info(\n             \"Opening database: engine={e}, interface={i}, \"\n             \"use_unicode={u}, autocommit={a}\".format(\n                 e=engine, i=interface, u=use_unicode, a=autocommit))\n         if interface == INTERFACE_MYSQL:\n             if pymysql:\n                 self.db_pythonlib = PYTHONLIB_PYMYSQL\n             elif MySQLdb:\n                 self.db_pythonlib = PYTHONLIB_MYSQLDB\n             else:\n                 raise ImportError(_MSG_MYSQL_DRIVERS_UNAVAILABLE)\n         elif interface == INTERFACE_ODBC:\n             if not pyodbc:\n                 raise ImportError(_MSG_PYODBC_UNAVAILABLE)\n             self.db_pythonlib = PYTHONLIB_PYODBC\n         elif interface == INTERFACE_JDBC:\n             if not jaydebeapi:\n                 raise ImportError(_MSG_JDBC_UNAVAILABLE)\n             if host is None:\n                 raise ValueError(\"Missing host parameter\")\n             if port is None:\n                 raise ValueError(\"Missing port parameter\")\n             if user is None:\n                 raise ValueError(\"Missing user parameter\")\n             self.db_pythonlib = PYTHONLIB_JAYDEBEAPI\n         else:\n             raise ValueError(\"Unknown interface\")\n         if engine == ENGINE_MYSQL and interface == INTERFACE_MYSQL:\n             datetimetype = datetime.datetime  \n             converters = mysql.converters.conversions.copy()\n             converters[datetimetype] = datetime2literal_rnc\n             log.info(\n                 \"{i} connect: host={h}, port={p}, user={u}, \"\n                 \"database={d}\".format(\n                     i=interface, h=host, p=port, u=user, d=database))\n             self.db = mysql.connect(\n                 host=host,\n                 port=port,\n                 user=user,\n                 passwd=password,\n                 db=database,\n                 charset=charset,\n                 use_unicode=use_unicode,\n                 conv=converters\n             )\n             self.db.autocommit(autocommit)\n         elif engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n             log.info(\n                 \"ODBC connect: DRIVER={dr};SERVER={s};PORT={p};\"\n                 \"DATABASE={db};USER={u};PASSWORD=[censored]\".format(\n                     dr=driver, s=host, p=port,\n                     db=database, u=user))\n             dsn = (\n                 \"DRIVER={0};SERVER={1};PORT={2};DATABASE={3};\"\n                 \"USER={4};PASSWORD={5}\".format(driver, host, port, database,\n                                                user, password)\n             )\n             self.db = pyodbc.connect(dsn)\n             self.db.autocommit = autocommit\n         elif engine == ENGINE_MYSQL and interface == INTERFACE_JDBC:\n             jclassname = \"com.mysql.jdbc.Driver\"\n             url = \"jdbc:mysql://{host}:{port}/{database}\".format(\n                 host=host, port=port, database=database)\n             driver_args = [url, user, password]\n             jars = None\n             libs = None\n             log.info(\n                 \"JDBC connect: jclassname={jclassname}, \"\n                 \"url={url}, user={user}, password=[censored]\".format(\n                     jclassname=jclassname,\n                     url=url,\n                     user=user,\n                 )\n             )\n             self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n         elif engine == ENGINE_SQLSERVER and interface == INTERFACE_ODBC:\n             if odbc_connection_string:\n                 log.info(\"Using raw ODBC connection string [censored]\")\n                 connectstring = odbc_connection_string\n             elif dsn:\n                 log.info(\n                     \"ODBC connect: DSN={dsn};UID={u};PWD=[censored]\".format(\n                         dsn=dsn, u=user))\n                 connectstring = \"DSN={};UID={};PWD={}\".format(dsn, user,\n                                                               password)\n             else:\n                 log.info(\n                     \"ODBC connect: DRIVER={dr};SERVER={s};DATABASE={db};\"\n                     \"UID={u};PWD=[censored]\".format(\n                         dr=driver, s=host, db=database, u=user))\n                 connectstring = (\n                     \"DRIVER={};SERVER={};DATABASE={};UID={};PWD={}\".format(\n                         driver, host, database, user, password)\n                 )\n             self.db = pyodbc.connect(connectstring, unicode_results=True)\n             self.db.autocommit = autocommit\n         elif engine == ENGINE_SQLSERVER and interface == INTERFACE_JDBC:\n             jclassname = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n             urlstem = \"jdbc:sqlserver://{host}:{port};\".format(\n                 host=host,\n                 port=port\n             )\n             nvp = {}\n             if database:\n                 nvp[\"databaseName\"] = database\n             nvp[\"user\"] = user\n             nvp[\"password\"] = password\n             nvp[\"responseBuffering\"] = \"adaptive\"  \n             nvp[\"selectMethod\"] = \"cursor\"  \n             url = urlstem + \";\".join(\n                 \"{}={}\".format(x, y) for x, y in nvp.items())\n             nvp[\"password\"] = \"[censored]\"\n             url_censored = urlstem + \";\".join(\n                 \"{}={}\".format(x, y) for x, y in nvp.items())\n             log.info(\n                 \"jdbc connect: jclassname={jclassname}, url = {url}\".format(\n                     jclassname=jclassname,\n                     url=url_censored\n                 )\n             )\n             driver_args = [url]\n             jars = None\n             libs = None\n             self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n         elif engine == ENGINE_ACCESS and interface == INTERFACE_ODBC:\n             dsn = \"DSN={}\".format(dsn)\n             log.info(\"ODBC connect: DSN={}\", dsn)\n             self.db = pyodbc.connect(dsn)\n             self.db.autocommit = autocommit\n         else:\n             raise ValueError(\n                 \"Unknown \"engine\"/\"interface\" combination: {}/{}\".format(\n                     engine, interface\n                 )\n             )\n         return True", "query": "connect to sql"}
{"id": "https://github.com/robinandeer/puzzle/blob/9476f05b416d3a5135d25492cb31411fdf831c58/puzzle/plugins/sql/store.py#L62-L86", "method_name": "connect", "code": "def connect(self, db_uri, debug=False):\n         kwargs = {\"echo\": debug, \"convert_unicode\": True}\n         if \"mysql\" in db_uri:\n             kwargs[\"pool_recycle\"] = 3600\n         elif \"://\" not in db_uri:\n             logger.debug(\"detected sqlite path URI: {}\".format(db_uri))\n             db_path = os.path.abspath(os.path.expanduser(db_uri))\n             db_uri = \"sqlite:///{}\".format(db_path)\n         self.engine = create_engine(db_uri, **kwargs)\n         logger.debug(\"connection established successfully\")\n         BASE.metadata.bind = self.engine\n         self.session = scoped_session(sessionmaker(bind=self.engine))\n         self.query = self.session.query\n         return self", "query": "connect to sql"}
{"id": "https://github.com/DIPSAS/SwarmManagement/blob/c9ef1165b240c145d42e2d363925c8200fc19f43/SwarmManagement/SwarmTools.py#L170-L180", "method_name": "TimeoutCounter", "code": "def TimeoutCounter(secTimeout):\n     startTime = time.time()\n     elapsedTime = time.time() - startTime\n     timeLeft = secTimeout - int(elapsedTime)\n     printedTime = timeLeft\n     while elapsedTime < secTimeout:\n         timeLeft = secTimeout - int(elapsedTime)\n         if timeLeft < printedTime:\n             printedTime = timeLeft\n             print(\"Restarting Swarm in %d seconds\" % printedTime)\n         elapsedTime = time.time() - startTime", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/lazygunner/xunleipy/blob/cded7598a7bf04495156bae2d747883d1eacb3f4/xunleipy/rsa_lib.py#L167-L177", "method_name": "findAPrime", "code": "def findAPrime(a, b, k):\n     x = random.randint(a, b)\n     for i in range(0, int(10 * math.log(x) + 3)):\n         if millerRabin(x, k):\n             return x\n         else:\n             x += 1\n     raise ValueError", "query": "randomly extract x items from a list"}
{"id": "https://github.com/collectiveacuity/labPack/blob/52949ece35e72e3cc308f54d9ffa6bfbd96805b8/labpack/records/time.py#L181-L206", "method_name": "fromEpoch", "code": "def fromEpoch(cls, epoch_time): \n         title = \"Epoch time input for labDT.fromEpoch\" \n         if not isinstance(epoch_time, float) and not isinstance(epoch_time, int): \n             raise TypeError(\"\n%s must be an integer or float.\" % title) \n         dT = datetime.utcfromtimestamp(epoch_time).replace(tzinfo=pytz.utc) \n         dt_kwargs = { \n             \"year\": dT.year, \n             \"month\": dT.month, \n             \"day\": dT.day, \n             \"hour\": dT.hour, \n             \"minute\": dT.minute, \n             \"second\": dT.second, \n             \"microsecond\": dT.microsecond, \n             \"tzinfo\": dT.tzinfo \n         } \n         return labDT(**dt_kwargs)", "query": "convert a utc time to epoch"}
{"id": "https://github.com/dragnet-org/dragnet/blob/532c9d9f28e5b1b57f3cabc708218d3863a16322/dragnet/__init__.py#L9-L13", "method_name": "extract_content", "code": "def extract_content(html, encoding=None, as_blocks=False):\n     if \"content\" not in _LOADED_MODELS:\n         _LOADED_MODELS[\"content\"] = load_pickled_model(\n             \"kohlschuetter_readability_weninger_content_model.pkl.gz\")\n     return _LOADED_MODELS[\"content\"].extract(html, encoding=encoding, as_blocks=as_blocks)", "query": "extract data from html content"}
{"id": "https://github.com/kibitzr/kibitzr/blob/749da312488f1dda1ed1093cf4c95aaac0a604f7/kibitzr/transformer/jinja_transform.py#L81-L88", "method_name": "text_filter", "code": "def text_filter(html):\n     if isinstance(html, list):\n         html = \"\".join(html)\n     ok, content = SoupOps.extract_text(html)\n     if ok:\n         return content\n     else:\n         raise RuntimeError(\"Extract text failed\")", "query": "extract data from html content"}
{"id": "https://github.com/tjguk/winshell/blob/1509d211ab3403dd1cff6113e4e13462d6dec35b/winshell.py#L266-L294", "method_name": "copy_file", "code": "def copy_file(\n     source_path,\n     target_path,\n     allow_undo=True,\n     no_confirm=False,\n     rename_on_collision=True,\n     silent=False,\n     extra_flags=0,\n     hWnd=None\n ):\n     return _file_operation(\n         shellcon.FO_COPY,\n         source_path,\n         target_path,\n         allow_undo,\n         no_confirm,\n         rename_on_collision,\n         silent,\n         extra_flags,\n         hWnd\n     )", "query": "copying a file to a path"}
{"id": "https://github.com/uuazed/numerapi/blob/fc9dcc53b32ede95bfda1ceeb62aec1d67d26697/numerapi/numerapi.py#L76-L88", "method_name": "_unzip_file", "code": "def _unzip_file(self, src_path, dest_path, filename):\n         self.logger.info(\"unzipping file...\")\n         unzip_path = os.path.join(dest_path, filename)\n         utils.ensure_directory_exists(unzip_path)\n         with zipfile.ZipFile(src_path, \"r\") as z:\n             z.extractall(unzip_path)\n         return True", "query": "unzipping large files"}
{"id": "https://github.com/indygreg/python-zstandard/blob/74fa5904c3e7df67a4260344bf919356a181487e/bench.py#L32-L71", "method_name": "timer", "code": "def timer(fn, miniter=3, minwall=3.0):\n     results = []\n     count = 0\n     wall_begin = time.time()\n     while True:\n         wstart = time.time()\n         start = os.times()\n         fn()\n         end = os.times()\n         wend = time.time()\n         count += 1\n         user = end[0] - start[0]\n         system = end[1] - start[1]\n         cpu = user + system\n         wall = wend - wstart\n         results.append((cpu, user, system, wall))\n         if count < miniter:\n             continue\n         elapsed = wend - wall_begin\n         if elapsed < minwall:\n             continue\n         break\n     return results", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/coderholic/pyradio/blob/c5219d350bccbccd49dbd627c1f886a952ea1963/pyradio/config.py#L258-L306", "method_name": "save_playlist_file", "code": "def save_playlist_file(self, stationFile=\"\"):\n         if self._playlist_format_changed():\n             self.dirty_playlist = True\n             self.new_format = not self.new_format\n         if stationFile:\n             st_file = stationFile\n         else:\n             st_file = self.stations_file\n         if not self.dirty_playlist:\n             if logger.isEnabledFor(logging.DEBUG):\n                 logger.debug(\"Playlist not modified...\")\n             return 0\n         st_new_file = st_file.replace(\".csv\", \".txt\")\n         tmp_stations = self.stations[:]\n         tmp_stations.reverse()\n         if self.new_format:\n             tmp_stations.append([ \"\n         else:\n             tmp_stations.append([ \"\n         tmp_stations.reverse()\n         try:\n             with open(st_new_file, \"w\") as cfgfile:\n                 writter = csv.writer(cfgfile)\n                 for a_station in tmp_stations:\n                     writter.writerow(self._format_playlist_row(a_station))\n         except:\n             if logger.isEnabledFor(logging.DEBUG):\n                 logger.debug(\"Cannot open playlist file for writing,,,\")\n             return -1\n         try:\n             move(st_new_file, st_file)\n         except:\n             if logger.isEnabledFor(logging.DEBUG):\n                 logger.debug(\"Cannot rename playlist file...\")\n             return -2\n         self.dirty_playlist = False\n         return 0", "query": "save list to file"}
{"id": "https://github.com/raymontag/kppy/blob/a43f1fff7d49da1da4b3d8628a1b3ebbaf47f43a/kppy/database.py#L861-L871", "method_name": "_cbc_encrypt", "code": "def _cbc_encrypt(self, content, final_key):\n         aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv)\n         padding = (16 - len(content) % AES.block_size)\n         for _ in range(padding):\n             content += chr(padding).encode()\n         temp = bytes(content)\n         return aes.encrypt(temp)", "query": "aes encryption"}
{"id": "https://github.com/Netflix-Skunkworks/cloudaux/blob/c4b0870c3ac68b1c69e71d33cf78b6a8bdf437ea/cloudaux/orchestration/aws/s3.py#L160-L178", "method_name": "get_website", "code": "def get_website(bucket_name, **conn):\n     try:\n         result = get_bucket_website(Bucket=bucket_name, **conn)\n     except ClientError as e:\n         if \"NoSuchWebsiteConfiguration\" not in str(e):\n             raise e\n         return None\n     website = {}\n     if result.get(\"IndexDocument\"):\n         website[\"IndexDocument\"] = result[\"IndexDocument\"]\n     if result.get(\"RoutingRules\"):\n         website[\"RoutingRules\"] = result[\"RoutingRules\"]\n     if result.get(\"RedirectAllRequestsTo\"):\n         website[\"RedirectAllRequestsTo\"] = result[\"RedirectAllRequestsTo\"]\n     if result.get(\"ErrorDocument\"):\n         website[\"ErrorDocument\"] = result[\"ErrorDocument\"]\n     return website", "query": "get html of website"}
{"id": "https://github.com/cs50/lib50/blob/941767f6c0a3b81af0cdea48c25c8d5a761086eb/lib50/_api.py#L570-L611", "method_name": "_lfs_add", "code": "def _lfs_add(files, git):\n     larges, huges = [], []\n     for file in files:\n         size = os.path.getsize(file)\n         if size > (100 * 1024 * 1024):\n             larges.append(file)\n         elif size > (2 * 1024 * 1024 * 1024):\n             huges.append(file)\n     if huges:\n         raise Error(_(\"These files are too large to be submitted:\n{}\n\"\n                       \"Remove these files from your directory \"\n                       \"and then re-run {}!\").format(\"\n\".join(huges), org))\n     if larges:\n         if not shutil.which(\"git-lfs\"):\n             raise Error(_(\"These files are too large to be submitted:\n{}\n\"\n                           \"Install git-lfs (or remove these files from your directory) \"\n                           \"and then re-run!\").format(\"\n\".join(larges)))\n         _run(git(\"lfs install --local\"))\n         _run(git(\"config credential.helper cache\"))\n         for large in larges:\n             _run(git(\"rm --cached {}\".format(shlex.quote(large))))\n             _run(git(\"lfs track {}\".format(shlex.quote(large))))\n             _run(git(\"add {}\".format(shlex.quote(large))))\n         _run(git(\"add --force .gitattributes\"))", "query": "unzipping large files"}
{"id": "https://github.com/GetmeUK/MongoFrames/blob/7d2bd792235dfa77a9deecab5366f5f73480823d/mongoframes/factory/makers/selections.py#L231-L283", "method_name": "p", "code": "def p(i, sample_size, weights):\n         weight_i = weights[i]\n         weights_sum = sum(weights)\n         other_weights = list(weights)\n         del other_weights[i]\n         probability_of_i = 0\n         for picks in range(0, sample_size):\n             permutations = list(itertools.permutations(other_weights, picks))\n             permutation_probabilities = []\n             for permutation in permutations:\n                 pick_probabilities = []\n                 pick_weight_sum = weights_sum\n                 for pick in permutation:\n                     pick_probabilities.append(pick / pick_weight_sum)\n                     pick_weight_sum -= pick\n                 pick_probabilities += [weight_i / pick_weight_sum]\n                 permutation_probability = reduce(\n                     lambda x, y: x * y, pick_probabilities\n                     )\n                 permutation_probabilities.append(permutation_probability)\n             probability_of_i += sum(permutation_probabilities)\n         return probability_of_i", "query": "all permutations of a list"}
{"id": "https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/examples/llcp-dta-iut.py#L156-L180", "method_name": "recv", "code": "def recv(self, recv_socket, llc):\n         time.sleep(0.1) \n         echo_buffer = queue.Queue(self.options.co_echo_buffer)\n         send_socket = nfc.llcp.Socket(llc, nfc.llcp.DATA_LINK_CONNECTION)\n         if self.options.pattern_number == 0x1200:\n             send_socket.connect(self.options.sap_lt_co_out_dest)\n         elif self.options.pattern_number == 0x1240:\n             send_socket.connect(\"urn:nfc:sn:dta-co-echo-out\")\n         elif self.options.pattern_number == 0x1280:\n             send_socket.connect(llc.resolve(\"urn:nfc:sn:dta-co-echo-out\"))\n         send_thread = Thread(target=self.send, args=(send_socket, echo_buffer))\n         send_thread.start()\n         log.info(\"receiving from sap %d\", recv_socket.getpeername())\n         while recv_socket.poll(\"recv\"):\n             data = recv_socket.recv()\n             if data == None: break\n             log.info(\"rcvd %d byte\", len(data))\n             recv_socket.setsockopt(nfc.llcp.SO_RCVBSY, echo_buffer.full())\n             echo_buffer.put(data)\n         log.info(\"remote side closed connection\")\n         try: echo_buffer.put_nowait(int(0))\n         except queue.Full: pass\n         send_thread.join()\n         recv_socket.close()\n         log.info(\"recv thread terminated\")", "query": "socket recv timeout"}
{"id": "https://github.com/mvcisback/py-aiger-bv/blob/855819844c429c35cdd8dc0b134bcd11f7b2fda3/aigerbv/common.py#L179-L184", "method_name": "reverse_gate", "code": "def reverse_gate(wordlen, input=\"x\", output=\"rev(x)\"):\n     circ = identity_gate(wordlen, input, output)\n     output_map = frozenset(\n         (k, tuple(reversed(vs))) for k, vs in circ.output_map\n     )\n     return attr.evolve(circ, output_map=output_map)", "query": "reverse a string"}
{"id": "https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_rdf.py#L9-L17", "method_name": "parse_command_line_arguments", "code": "def parse_command_line_arguments():\n     parser = argparse.ArgumentParser()\n     parser.add_argument( \"xdatcar\" )\n     parser.add_argument( \"label\", nargs = 2 )\n     parser.add_argument( \"max_r\", type = float )\n     parser.add_argument( \"n_bins\", type = int )\n     args = parser.parse_args()\n     return( args )", "query": "parse command line argument"}
{"id": "https://github.com/chitamoor/Rester/blob/1865b17f70b7c597aeadde2d0907cb1b59f10c0f/rester/apirunner.py#L9-L18", "method_name": "parse_cmdln_args", "code": "def parse_cmdln_args():\n     parser = argparse.ArgumentParser(description=\"Process command line args\")\n     parser.add_argument(\"--log\", help=\"log help\", default=\"INFO\")\n     parser.add_argument(\n         \"--tc\", help=\"tc help\")\n     parser.add_argument(\n         \"--ts\", help=\"ts help\")\n     args = parser.parse_args()\n     return (args.log.upper(), args.tc, args.ts)", "query": "parse command line argument"}
{"id": "https://github.com/go-macaroon-bakery/py-macaroon-bakery/blob/63ce1ef1dabe816eb8aaec48fbb46761c34ddf77/macaroonbakery/bakery/_macaroon.py#L242-L248", "method_name": "deserialize_json", "code": "def deserialize_json(cls, serialized_json):\n         serialized = json.loads(serialized_json)\n         return Macaroon.from_dict(serialized)", "query": "deserialize json"}
{"id": "https://github.com/google/prettytensor/blob/75daa0b11252590f548da5647addc0ea610c4c45/prettytensor/tutorial/data_utils.py#L82-L89", "method_name": "permute_data", "code": "def permute_data(arrays, random_state=None):\n   if any(len(a) != len(arrays[0]) for a in arrays):\n     raise ValueError(\"All arrays must be the same length.\")\n   if not random_state:\n     random_state = np.random\n   order = random_state.permutation(len(arrays[0]))\n   return [a[order] for a in arrays]", "query": "sorting multiple arrays based on another arrays sorted order"}
{"id": "https://github.com/riptano/ccm/blob/275699f79d102b5039b79cc17fa6305dccf18412/ccmlib/common.py#L208-L219", "method_name": "replaces_in_file", "code": "def replaces_in_file(file, replacement_list):\n     rs = [(re.compile(regexp), repl) for (regexp, repl) in replacement_list]\n     file_tmp = file + \".\" + str(os.getpid()) + \".tmp\"\n     with open(file, \"r\") as f:\n         with open(file_tmp, \"w\") as f_tmp:\n             for line in f:\n                 for r, replace in rs:\n                     match = r.search(line)\n                     if match:\n                         line = replace + \"\n\"\n                 f_tmp.write(line)\n     shutil.move(file_tmp, file)", "query": "replace in file"}
{"id": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/engine/worker.py#L338-L374", "method_name": "_ExtractMetadataFromFileEntry", "code": "def _ExtractMetadataFromFileEntry(self, mediator, file_entry, data_stream):\n     if file_entry.IsRoot() and file_entry.type_indicator not in (\n         self._TYPES_WITH_ROOT_METADATA):\n       return\n     if data_stream and not data_stream.IsDefault():\n       return\n     display_name = mediator.GetDisplayName()\n     logger.debug(\n         \"[ExtractMetadataFromFileEntry] processing file entry: {0:s}\".format(\n             display_name))\n     self.processing_status = definitions.STATUS_INDICATOR_EXTRACTING\n     if self._processing_profiler:\n       self._processing_profiler.StartTiming(\"extracting\")\n     self._event_extractor.ParseFileEntryMetadata(mediator, file_entry)\n     if self._processing_profiler:\n       self._processing_profiler.StopTiming(\"extracting\")\n     self.processing_status = definitions.STATUS_INDICATOR_RUNNING", "query": "extracting data from a text file"}
{"id": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/service/sdn/plugins/networkvxlandriver.py#L217-L233", "method_name": "_isavaliablevni", "code": "def _isavaliablevni(vnirange,allocated,vni):\n     find = False\n     for start,end in vnirange:\n         if start <= int(vni) <= end:\n             find = True\n             break\n     if find:\n         if str(vni) not in allocated:\n             find = True\n         else:\n             find = False\n     else:\n         find = False\n     return find", "query": "find int in string"}
{"id": "https://github.com/ethereum/pyethereum/blob/b704a5c6577863edc539a1ec3d2620a443b950fb/ethereum/tools/keys.py#L56-L61", "method_name": "aes_ctr_encrypt", "code": "def aes_ctr_encrypt(text, key, params):\n     iv = big_endian_to_int(decode_hex(params[\"iv\"]))\n     ctr = Counter.new(128, initial_value=iv, allow_wraparound=True)\n     mode = AES.MODE_CTR\n     encryptor = AES.new(key, mode, counter=ctr)\n     return encryptor.encrypt(text)", "query": "aes encryption"}
{"id": "https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/operation/datashader.py#L932-L936", "method_name": "uint32_to_uint8", "code": "def uint32_to_uint8(cls, img):\n         return np.flipud(img.view(dtype=np.uint8).reshape(img.shape + (4,)))", "query": "converting uint8 array to image"}
{"id": "https://github.com/lorien/grab/blob/8b301db2a08c830245b61c589e58af6234f4db79/grab/cookie.py#L118-L152", "method_name": "create_cookie", "code": "def create_cookie(name, value, domain, httponly=None, **kwargs):\n     if domain == \"localhost\":\n         domain = \"\"\n     config = dict(\n         name=name,\n         value=value,\n         version=0,\n         port=None,\n         domain=domain,\n         path=\"/\",\n         secure=False,\n         expires=None,\n         discard=True,\n         comment=None,\n         comment_url=None,\n         rfc2109=False,\n         rest={\"HttpOnly\": httponly},\n     )\n     for key in kwargs:\n         if key not in config:\n             raise GrabMisuseError(\"Function `create_cookie` does not accept \"\n                                   \"`%s` argument\" % key)\n     config.update(**kwargs)\n     config[\"rest\"][\"HttpOnly\"] = httponly\n     config[\"port_specified\"] = bool(config[\"port\"])\n     config[\"domain_specified\"] = bool(config[\"domain\"])\n     config[\"domain_initial_dot\"] = (config[\"domain\"] or \"\").startswith(\".\")\n     config[\"path_specified\"] = bool(config[\"path\"])\n     return Cookie(**config)", "query": "create cookie"}
{"id": "https://github.com/santoshphilip/eppy/blob/55410ff7c11722f35bc4331ff5e00a0b86f787e1/eppy/EPlusInterfaceFunctions/parse_idd.py#L142-L385", "method_name": "extractidddata", "code": "def extractidddata(fname, debug=False):\n     try:\n         if isinstance(fname, (file, StringIO)):\n             astr = fname.read()\n             try:\n                 astr = astr.decode(\"ISO-8859-2\")\n             except AttributeError:\n                 pass \n         else:\n             astr = mylib2.readfile(fname)\n     except NameError:\n         if isinstance(fname, (FileIO, StringIO)):\n             astr = fname.read()\n             try:\n                 astr = astr.decode(\"ISO-8859-2\")\n             except AttributeError:\n                 pass\n         else:\n             astr = mylib2.readfile(fname)\n     (nocom, nocom1, blocklst) = get_nocom_vars(astr)\n     astr = nocom\n     st1 = removeblanklines(astr)\n     if debug:\n         mylib1.write_str2file(\"nocom2.txt\", st1.encode(\"latin-1\"))\n     groupls = []\n     alist = st1.splitlines()\n     for element in alist:\n         lss = element.split()\n         if lss[0].upper() == \"\\\\group\".upper():\n             groupls.append(element)\n     groupstart = []\n     for i in range(len(groupls)):\n         iindex = alist.index(groupls[i])\n         groupstart.append([alist[iindex], alist[iindex+1]])\n     for element in groupls:\n         alist.remove(element)\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom3.txt\", st1.encode(\"latin-1\"))\n     for i in range(len(alist)):\n         alist[i] = alist[i].strip()\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom4.txt\", st1.encode(\"latin-1\"))\n     lss = []\n     for i in range(len(alist)):\n         if alist[i][0] != \"\\\\\":\n             pnt = alist[i].find(\"\\\\\")\n             if pnt != -1:\n                 lss.append(alist[i][:pnt].strip())\n                 lss.append(alist[i][pnt:].strip())\n             else:\n                 lss.append(alist[i])\n         else:\n             lss.append(alist[i])\n     alist = lss[:]\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom5.txt\", st1.encode(\"latin-1\"))\n     lss = []\n     for element in alist:\n         if element[0] != \"\\\\\":\n             llist = element.split(\",\")\n             if llist[-1] == \"\":\n                 tmp = llist.pop()\n             for elm in llist:\n                 if elm[-1] == \";\":\n                     lss.append(elm.strip())\n                 else:\n                     lss.append((elm+\",\").strip())\n         else:\n             lss.append(element)\n     ls_debug = alist[:] \n     alist = lss[:]\n     if debug:\n         st1 = \"\n\".join(alist)\n         mylib1.write_str2file(\"nocom6.txt\", st1.encode(\"latin-1\"))\n     if debug:\n         lss_debug = []\n         for element in ls_debug:\n             if element[0] != \"\\\\\":\n                 llist = element.split(\",\")\n                 if llist[-1] == \"\":\n                     tmp = llist.pop()\n                 for elm in llist:\n                     if elm[-1] == \";\":\n                         lss_debug.append(elm[:-1].strip())\n                     else:\n                         lss_debug.append((elm).strip())\n             else:\n                 lss_debug.append(element)\n         ls_debug = lss_debug[:]\n         st1 = \"\n\".join(ls_debug)\n         mylib1.write_str2file(\"nocom7.txt\", st1.encode(\"latin-1\"))\n     for i in range(len(lss)):\n         if lss[i][0] != \"\\\\\":\n             lss[i] = \"=====var=====\"\n     st2 = \"\n\".join(lss)\n     lss = st2.split(\"=====var=====\n\")\n     lss.pop(0) \n     if debug:\n         fname = \"nocom8.txt\"\n         fhandle = open(fname, \"wb\")\n         k = 0\n         for i in range(len(blocklst)):\n             for j in range(len(blocklst[i])):\n                 atxt = blocklst[i][j]+\"\n\"\n                 fhandle.write(atxt)\n                 atxt = lss[k]\n                 fhandle.write(atxt.encode(\"latin-1\"))\n                 k = k+1\n         fhandle.close()\n     k = 0\n     lst = []\n     for i in range(len(blocklst)):\n         lst.append([])\n         for j in range(len(blocklst[i])):\n             lst[i].append(lss[k])\n             k = k+1\n     if debug:\n         fname = \"nocom9.txt\"\n         fhandle = open(fname, \"wb\")\n         k = 0\n         for i in range(len(blocklst)):\n             for j in range(len(blocklst[i])):\n                 atxt = blocklst[i][j]+\"\n\"\n                 fhandle.write(atxt)\n                 fhandle.write(lst[i][j].encode(\"latin-1\"))\n                 k = k+1\n         fhandle.close()\n     for i in range(len(lst)):\n         for j in range(len(lst[i])):\n             lst[i][j] = lst[i][j].splitlines()\n             for k in range(len(lst[i][j])):\n                 lst[i][j][k] = lst[i][j][k][1:]\n     commlst = lst\n     clist = lst\n     lss = []\n     for i in range(0, len(clist)):\n         alist = []\n         for j in range(0, len(clist[i])):\n             itt = clist[i][j]\n             ddtt = {}\n             for element in itt:\n                 if len(element.split()) == 0:\n                     break\n                 ddtt[element.split()[0].lower()] = []\n             for element in itt:\n                 if len(element.split()) == 0:\n                     break\n                 ddtt[element.split()[0].lower()].append(\" \".join(element.split()[1:]))\n             alist.append(ddtt)\n         lss.append(alist)\n     commdct = lss\n     return blocklst, commlst, commdct", "query": "extracting data from a text file"}
{"id": "https://github.com/dustin/twitty-twister/blob/8524750ee73adb57bbe14ef0cfd8aa08e1e59fb3/twittytwister/twitter.py#L171-L177", "method_name": "_urlencode", "code": "def _urlencode(self, h):\n         rv = []\n         for k,v in h.iteritems():\n             rv.append(\"%s=%s\" %\n                 (urllib.quote(k.encode(\"utf-8\")),\n                 urllib.quote(v.encode(\"utf-8\"))))\n         return \"&\".join(rv)", "query": "encode url"}
{"id": "https://github.com/mlenzen/collections-extended/blob/ee9e86f6bbef442dbebcb3a5970642c5c969e2cf/collections_extended/bags.py#L89-L96", "method_name": "_set_count", "code": "def _set_count(self, elem, count):\n \t\tif count < 0:\n \t\t\traise ValueError\n \t\tself._size += count - self.count(elem)\n \t\tif count == 0:\n \t\t\tself._dict.pop(elem, None)\n \t\telse:\n \t\t\tself._dict[elem] = count", "query": "hash set for counting distinct elements"}
{"id": "https://github.com/BerkeleyAutomation/perception/blob/03d9b37dd6b66896cdfe173905c9413c8c3c5df6/perception/image.py#L252-L291", "method_name": "from_array", "code": "def from_array(x, frame=\"unspecified\"):\n         if not Image.can_convert(x):\n             raise ValueError(\"Cannot convert array to an Image!\")\n         dtype = x.dtype\n         height = x.shape[0]\n         width = x.shape[1]\n         channels = 1\n         if len(x.shape) == 3:\n             channels = x.shape[2]\n         if dtype == np.uint8:\n             if channels == 1:\n                 if np.any((x % BINARY_IM_MAX_VAL) > 0):\n                     return GrayscaleImage(x, frame)\n                 return BinaryImage(x, frame)\n             elif channels == 3:\n                 return ColorImage(x, frame)\n             else:\n                 raise ValueError(\n                     \"No available image conversion for uint8 array with 2 channels\")\n         elif dtype == np.uint16:\n             if channels != 1:\n                 raise ValueError(\n                     \"No available image conversion for uint16 array with 2 or 3 channels\")\n             return GrayscaleImage(x, frame)\n         elif dtype == np.float32 or dtype == np.float64:\n             if channels == 1:\n                 return DepthImage(x, frame)\n             elif channels == 2:\n                 return GdImage(x, frame)\n             elif channels == 3:\n                 logging.warning(\"Converting float array to uint8\")\n                 return ColorImage(x.astype(np.uint8), frame)\n             return RgbdImage(x, frame)\n         else:\n             raise ValueError(\n                 \"Conversion for dtype %s not supported!\" %\n                 (str(dtype)))", "query": "converting uint8 array to image"}
{"id": "https://github.com/thomasdelaet/python-velbus/blob/af2f8af43f1a24bf854eff9f3126fd7b5c41b3dd/velbus/messages/relay_status.py#L120-L130", "method_name": "data_to_binary", "code": "def data_to_binary(self):\n         return bytes([\n             COMMAND_CODE,\n             self.channels_to_byte([self.channel]),\n             self.disable_inhibit_forced,\n             self.status,\n             self.led_status\n         ]) + struct.pack(\">L\", self.delay_time)[-3:]", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/transformations.py#L11-L75", "method_name": "toUIntArray", "code": "def toUIntArray(img, dtype=None, cutNegative=True, cutHigh=True, \n                 range=None, copy=True): \n     mn, mx = None, None \n     if range is not None: \n         mn, mx = range \n     if dtype is None: \n         if mx is None: \n             mx = np.nanmax(img) \n         dtype = np.uint16 if mx > 255 else np.uint8 \n     dtype = np.dtype(dtype) \n     if dtype == img.dtype: \n         return img \n     b = {\"uint8\": 255, \n          \"uint16\": 65535, \n          \"uint32\": 4294967295, \n          \"uint64\": 18446744073709551615}[dtype.name] \n     if copy: \n         img = img.copy() \n     if range is not None: \n         img = np.asfarray(img) \n         img -= mn \n         img *= b / (mx - mn) \n         img = np.clip(img, 0, b) \n     else: \n         if cutNegative: \n             img[img < 0] = 0 \n         else: \n             mn = np.min(img) \n             if mn < 0: \n                 img -= mn  \n         if cutHigh: \n             img[img > b] = b \n         else: \n             mx = np.nanmax(img) \n             img = np.asfarray(img) * (float(b) / mx) \n     img = img.astype(dtype) \n     return img", "query": "converting uint8 array to image"}
{"id": "https://github.com/honzajavorek/danube-delta/blob/d0a72f0704d52b888e7fb2b68c4fdc696d370018/danube_delta/plugins/utils.py#L8-L17", "method_name": "modify_html", "code": "def modify_html(content, prop=\"_content\"):\n     html_string = getattr(content, prop)\n     html_tree = html.fromstring(html_string)\n     yield html_tree\n     html_string = html.tostring(html_tree, encoding=\"unicode\")\n     html_string = re.sub(r\"%7B(\\w+)%7D\", r\"{\\1}\", html_string)\n     html_string = re.sub(r\"%7C(\\w+)%7C\", r\" \\1 \", html_string)\n     setattr(content, prop, html_string)", "query": "html encode string"}
{"id": "https://github.com/seleniumbase/SeleniumBase/blob/62e5b43ee1f90a9ed923841bdd53b1b38358f43a/seleniumbase/common/encryption.py#L43-L55", "method_name": "reverse_shuffle_string", "code": "def reverse_shuffle_string(string):\n     if len(string) < 2:\n         return string\n     new_string = \"\"\n     odd = (len(string) % 2 == 1)\n     part1 = string[:int(len(string) / 2):1]\n     part2 = string[int(len(string) / 2)::1]\n     for c in range(len(part1)):\n         new_string += part2[c]\n         new_string += part1[c]\n     if odd:\n         new_string += part2[-1]\n     return new_string", "query": "reverse a string"}
{"id": "https://github.com/ModisWorks/modis/blob/1f1225c9841835ec1d1831fc196306527567db8b/modis/discord_modis/modules/music/api_music.py#L94-L234", "method_name": "parse_query", "code": "def parse_query(query, ilogger):\n     p = urlparse(query)\n     if p and p.scheme and p.netloc:\n         if \"youtube\" in p.netloc and p.query and ytdiscoveryapi is not None:\n             query_parts = p.query.split(\"&\")\n             yturl_parts = {}\n             for q in query_parts:\n                 s = q.split(\"=\")\n                 if len(s) < 2:\n                     continue\n                 q_name = s[0]\n                 q_val = \"=\".join(s[1:])\n                 if q_name not in yturl_parts:\n                     yturl_parts[q_name] = q_val\n             if \"list\" in yturl_parts:\n                 ilogger.info(\"Queued YouTube playlist from link\")\n                 return get_queue_from_playlist(yturl_parts[\"list\"])\n             elif \"v\" in yturl_parts:\n                 ilogger.info(\"Queued YouTube video from link\")\n                 return [[\"https://www.youtube.com/watch?v={}\".format(yturl_parts[\"v\"]), query]]\n         elif \"soundcloud\" in p.netloc:\n             if scclient is None:\n                 ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                 return [[query, query]]\n             try:\n                 result = scclient.get(\"/resolve\", url=query)\n                 track_list = []\n                 if isinstance(result, ResourceList):\n                     for r in result.data:\n                         tracks = get_sc_tracks(r)\n                         if tracks is not None:\n                             for t in tracks:\n                                 track_list.append(t)\n                 elif isinstance(result, Resource):\n                     tracks = get_sc_tracks(result)\n                     if tracks is not None:\n                         for t in tracks:\n                             track_list.append(t)\n                 if track_list is not None and len(track_list) > 0:\n                     ilogger.info(\"Queued SoundCloud songs from link\")\n                     return track_list\n                 else:\n                     ilogger.error(\"Could not queue from SoundCloud API\")\n                     return [[query, query]]\n             except Exception as e:\n                 logger.exception(e)\n                 ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                 return [[query, query]]\n         else:\n             ilogger.debug(\"Using url: {}\".format(query))\n             return [[query, query]]\n     args = query.split(\" \")\n     if len(args) == 0:\n         ilogger.error(\"No query given\")\n         return []\n     if args[0].lower() in [\"sp\", \"spotify\"] and spclient is not None:\n         if spclient is None:\n             ilogger.error(\"Host does not support Spotify\")\n             return []\n         try:\n             if len(args) > 2 and args[1] in [\"album\", \"artist\", \"song\", \"track\", \"playlist\"]:\n                 query_type = args[1].lower()\n                 query_search = \" \".join(args[2:])\n             else:\n                 query_type = \"track\"\n                 query_search = \" \".join(args[1:])\n             query_type = query_type.replace(\"song\", \"track\")\n             ilogger.info(\"Queueing Spotify {}: {}\".format(query_type, query_search))\n             spotify_tracks = search_sp_tracks(query_type, query_search)\n             if spotify_tracks is None or len(spotify_tracks) == 0:\n                 ilogger.error(\"Could not queue Spotify {}: {}\".format(query_type, query_search))\n                 return []\n             ilogger.info(\"Queued Spotify {}: {}\".format(query_type, query_search))\n             return spotify_tracks\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Error queueing from Spotify\")\n             return []\n     elif args[0].lower() in [\"sc\", \"soundcloud\"]:\n         if scclient is None:\n             ilogger.error(\"Host does not support SoundCloud\")\n             return []\n         try:\n             requests = [\"song\", \"songs\", \"track\", \"tracks\", \"user\", \"playlist\", \"tagged\", \"genre\"]\n             if len(args) > 2 and args[1] in requests:\n                 query_type = args[1].lower()\n                 query_search = \" \".join(args[2:])\n             else:\n                 query_type = \"track\"\n                 query_search = \" \".join(args[1:])\n             query_type = query_type.replace(\"song\", \"track\")\n             ilogger.info(\"Queueing SoundCloud {}: {}\".format(query_type, query_search))\n             soundcloud_tracks = search_sc_tracks(query_type, query_search)\n             ilogger.info(\"Queued SoundCloud {}: {}\".format(query_type, query_search))\n             return soundcloud_tracks\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Could not queue from SoundCloud\")\n             return []\n     elif args[0].lower() in [\"yt\", \"youtube\"] and ytdiscoveryapi is not None:\n         if ytdiscoveryapi is None:\n             ilogger.error(\"Host does not support YouTube\")\n             return []\n         try:\n             query_search = \" \".join(args[1:])\n             ilogger.info(\"Queued Youtube search: {}\".format(query_search))\n             return get_ytvideos(query_search, ilogger)\n         except Exception as e:\n             logger.exception(e)\n             ilogger.error(\"Could not queue YouTube search\")\n             return []\n     if ytdiscoveryapi is not None:\n         ilogger.info(\"Queued YouTube search: {}\".format(query))\n         return get_ytvideos(query, ilogger)\n     else:\n         ilogger.error(\"Host does not support YouTube\".format(query))\n         return []", "query": "parse query string in url"}
{"id": "https://github.com/KelSolaar/Umbra/blob/66f45f08d9d723787f1191989f8b0dda84b412ce/umbra/reporter.py#L468-L477", "method_name": "__set_html", "code": "def __set_html(self, html=None):\n         self.__html = self.__get_html(html)\n         self.__view.setHtml(self.__html)", "query": "get inner html"}
{"id": "https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/peewee.py#L3708-L3712", "method_name": "_connect", "code": "def _connect(self):\n         if mysql is None:\n             raise ImproperlyConfigured(\"MySQL driver not installed!\")\n         conn = mysql.connect(db=self.database, **self.connect_params)\n         return conn", "query": "connect to sql"}
{"id": "https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/plotting/matplot_dep/plot_definitions.py#L99-L102", "method_name": "scatter", "code": "def scatter(self, ax, X, Y, Z=None, color=Tango.colorsHex[\"mediumBlue\"], label=None, marker=\"o\", **kwargs):\n         if Z is not None:\n             return ax.scatter(X, Y, c=color, zs=Z, label=label, marker=marker, **kwargs)\n         return ax.scatter(X, Y, c=color, label=label, marker=marker, **kwargs)", "query": "scatter plot"}
{"id": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Model_Data.py#L176-L203", "method_name": "linear_regression", "code": "def linear_regression(self):\n         model = LinearRegression()\n         scores = []\n         kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n         for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)):\n             model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])\n             scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test]))\n         mean_score = sum(scores) / len(scores)\n         self.models.append(model)\n         self.model_names.append(\"Linear Regression\")\n         self.max_scores.append(mean_score)\n         self.metrics[\"Linear Regression\"] = {}\n         self.metrics[\"Linear Regression\"][\"R2\"] = mean_score\n         self.metrics[\"Linear Regression\"][\"Adj R2\"] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])", "query": "linear regression"}
{"id": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/excel.py#L1227-L1269", "method_name": "compile_geometry", "code": "def compile_geometry(lat, lon, elev):\n     logger_excel.info(\"enter compile_geometry\")\n     lat = _remove_geo_placeholders(lat)\n     lon = _remove_geo_placeholders(lon)\n     if len(lat) == 2 and len(lon) == 2:\n         logger_excel.info(\"found 4 coordinates\")\n         geo_dict = geometry_linestring(lat, lon, elev)\n     elif len(lat) == 1 and len(lon) == 1:\n         logger_excel.info(\"found 2 coordinates\")\n         geo_dict = geometry_point(lat, lon, elev)\n     elif (None in lon and None not in lat) or (len(lat) > 0 and len(lon) == 0):\n         geo_dict = geometry_range(lat, elev, \"lat\")\n     elif (None in lat and None not in lon) or (len(lon) > 0 and len(lat) == 0):\n         geo_dict = geometry_range(lat, elev, \"lon\")\n     else:\n         geo_dict = {}\n         logger_excel.warn(\"compile_geometry: invalid coordinates: lat: {}, lon: {}\".format(lat, lon))\n     logger_excel.info(\"exit compile_geometry\")\n     return geo_dict", "query": "extract latitude and longitude from given input"}
{"id": "https://github.com/binux/pyspider/blob/3fccfabe2b057b7a56d4a4c79dc0dd6cd2239fe9/pyspider/libs/utils.py#L72-L130", "method_name": "format_date", "code": "def format_date(date, gmt_offset=0, relative=True, shorter=False, full_format=False):\n     if not date:\n         return \"-\"\n     if isinstance(date, float) or isinstance(date, int):\n         date = datetime.datetime.utcfromtimestamp(date)\n     now = datetime.datetime.utcnow()\n     if date > now:\n         if relative and (date - now).seconds < 60:\n             date = now\n         else:\n             full_format = True\n     local_date = date - datetime.timedelta(minutes=gmt_offset)\n     local_now = now - datetime.timedelta(minutes=gmt_offset)\n     local_yesterday = local_now - datetime.timedelta(hours=24)\n     difference = now - date\n     seconds = difference.seconds\n     days = difference.days\n     format = None\n     if not full_format:\n         ret_, fff_format = fix_full_format(days, seconds, relative, shorter, local_date, local_yesterday)\n         format = fff_format\n         if ret_:\n             return format\n         else:\n             format = format\n     if format is None:\n         format = \"%(month_name)s %(day)s, %(year)s\" if shorter else \\\n             \"%(month_name)s %(day)s, %(year)s at %(time)s\"\n     str_time = \"%d:%02d\" % (local_date.hour, local_date.minute)\n     return format % {\n         \"month_name\": local_date.strftime(\"%b\"),\n         \"weekday\": local_date.strftime(\"%A\"),\n         \"day\": str(local_date.day),\n         \"year\": str(local_date.year),\n         \"month\": local_date.month,\n         \"time\": str_time\n     }", "query": "format date"}
{"id": "https://github.com/APSL/transmanager/blob/79157085840008e146b264521681913090197ed1/transmanager/export.py#L58-L67", "method_name": "export_translations", "code": "def export_translations(tasks_ids):\n         qs = TransTask.objects.filter(pk__in=tasks_ids)\n         export = ExportQueryset(\n             qs,\n             TransTask,\n             (\"id\", \"object_name\", \"object_pk\", \"object_field_label\", \"object_field_value\", \"number_of_words\",\n              \"object_field_value_translation\", \"date_modification\", \"done\")\n         )\n         excel = export.get_excel()\n         return excel", "query": "export to excel"}
{"id": "https://github.com/djf604/chunky-pipes/blob/cd5b2a31ded28ab949da6190854228f1c8897882/chunkypipes/util/decorators/__init__.py#L6-L14", "method_name": "timed", "code": "def timed(func):\n     @functools.wraps(func)\n     def timer(*args, **kwargs):\n         start_time = time()\n         result = func(*args, **kwargs)\n         elapsed_time = str(timedelta(seconds=int(time() - start_time)))\n         print(\"Elapsed time is {}\".format(elapsed_time))\n         return result\n     return timer", "query": "finding time elapsed using a timer"}
{"id": "https://github.com/cmbruns/pyopenvr/blob/68395d26bb3df6ab1f0f059c38d441f962938be6/src/openvr/gl_renderer.py#L17-L32", "method_name": "matrixForOpenVrMatrix", "code": "def matrixForOpenVrMatrix(mat):\n     if len(mat.m) == 4: \n         result = numpy.matrix(\n                 ((mat.m[0][0], mat.m[1][0], mat.m[2][0], mat.m[3][0]),\n                  (mat.m[0][1], mat.m[1][1], mat.m[2][1], mat.m[3][1]), \n                  (mat.m[0][2], mat.m[1][2], mat.m[2][2], mat.m[3][2]), \n                  (mat.m[0][3], mat.m[1][3], mat.m[2][3], mat.m[3][3]),)\n             , numpy.float32)\n     elif len(mat.m) == 3: \n         result = numpy.matrix(\n                 ((mat.m[0][0], mat.m[1][0], mat.m[2][0], 0.0),\n                  (mat.m[0][1], mat.m[1][1], mat.m[2][1], 0.0), \n                  (mat.m[0][2], mat.m[1][2], mat.m[2][2], 0.0), \n                  (mat.m[0][3], mat.m[1][3], mat.m[2][3], 1.0),)\n             , numpy.float32)\n     return result", "query": "matrix multiply"}
{"id": "https://github.com/GibbsConsulting/jupyter-plotly-dash/blob/19e3898372ddf7c1d20292eae1ea0df9e0808fe2/jupyter_plotly_dash/dash_wrapper.py#L104-L120", "method_name": "_repr_html_", "code": "def _repr_html_(self):\n         url = self.get_app_root_url()\n         da_id = self.session_id()\n         comm = locate_jpd_comm(da_id, self, url[1:-1])\n         external = self.add_external_link and \"<hr/><a href=\"{url}\" target=\"_new\">Open in new window</a>\".format(url=url) or \"\"\n         fb = \"frameborder=\"%i\"\" %(self.frame and 1 or 0)\n         iframe =  %{\"url\" : url,\n             \"da_id\" : da_id,\n             \"external\" : external,\n             \"width\" : self.width,\n             \"height\" : self.height,\n             \"frame\": fb,}\n         return iframe", "query": "get html of website"}
{"id": "https://github.com/mattja/sdeint/blob/7cf807cdf97b3bb39d29e1c2dc834b519499b601/sdeint/_broadcast.py#L70-L108", "method_name": "broadcast_to", "code": "def broadcast_to(array, shape, subok=False):\n     return _broadcast_to(array, shape, subok=subok, readonly=True)", "query": "readonly array"}
{"id": "https://github.com/mattjj/pybasicbayes/blob/76aef00f011415cc5c858cd1a101f3aab971a62d/pybasicbayes/distributions/gaussian.py#L1017-L1019", "method_name": "_empty_stats", "code": "def _empty_stats(self):\n        return np.array([0.,np.zeros_like(self.mu_0),np.zeros_like(self.mu_0)],\n                dtype=np.object)", "query": "empty array"}
{"id": "https://github.com/deeshugupta/tes/blob/217db49aa211ebca2d9258380765a0c31abfca91/es_commands/base.py#L38-L40", "method_name": "pretty_print", "code": "def pretty_print(response):\n    parsed = json.loads(json.dumps(response))\n    click.echo(json.dumps(parsed, indent=4, sort_keys=True))", "query": "pretty print json"}
{"id": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L373-L389", "method_name": "multiply", "code": "def multiply(self, matrix):\n         if not isinstance(matrix, DenseMatrix):\n             raise ValueError(\"Only multiplication with DenseMatrix \"\n                              \"is supported.\")\n         j_model = self._java_matrix_wrapper.call(\"multiply\", matrix)\n         return RowMatrix(j_model)", "query": "matrix multiply"}
{"id": "https://github.com/datajoint/datajoint-python/blob/4f29bb154a7ed2b8b64b4d3a9c8be4c16b39621c/datajoint/connection.py#L20-L44", "method_name": "conn", "code": "def conn(host=None, user=None, password=None, init_fun=None, reset=False):\n     if not hasattr(conn, \"connection\") or reset:\n         host = host if host is not None else config[\"database.host\"]\n         user = user if user is not None else config[\"database.user\"]\n         password = password if password is not None else config[\"database.password\"]\n         if user is None:  \n             user = input(\"Please enter DataJoint username: \")\n         if password is None:  \n             password = getpass(prompt=\"Please enter DataJoint password: \")\n         init_fun = init_fun if init_fun is not None else config[\"connection.init_function\"]\n         conn.connection = Connection(host, user, password, init_fun)\n     return conn.connection", "query": "postgresql connection"}
{"id": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341", "method_name": "check_checkbox", "code": "def check_checkbox(step, value):\n     with AssertContextManager(step):\n         check_box = find_field(world.browser, \"checkbox\", value)\n         if not check_box.is_selected():\n             check_box.click()", "query": "check if a checkbox is checked"}
{"id": "https://github.com/crs4/pydoop/blob/f375be2a06f9c67eaae3ce6f605195dbca143b2b/pydoop/__init__.py#L183-L193", "method_name": "read_properties", "code": "def read_properties(fname):\n     parser = configparser.SafeConfigParser()\n     parser.optionxform = str  \n     try:\n         with open(fname) as f:\n             parser_read(parser, AddSectionWrapper(f))\n     except IOError as e:\n         if e.errno != errno.ENOENT:\n             raise\n         return None  \n     return dict(parser.items(AddSectionWrapper.SEC_NAME))", "query": "read properties file"}
{"id": "https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/acquisitions/base.py#L52-L60", "method_name": "optimize", "code": "def optimize(self, duplicate_manager=None):\n         if not self.analytical_gradient_acq:\n             out = self.optimizer.optimize(f=self.acquisition_function, duplicate_manager=duplicate_manager)\n         else:\n             out = self.optimizer.optimize(f=self.acquisition_function, f_df=self.acquisition_function_withGradients, duplicate_manager=duplicate_manager)\n         return out", "query": "nelder mead optimize"}
{"id": "https://github.com/tehmaze/ipcalc/blob/d436b95d2783347c3e0084d76ec3c52d1f5d2f0b/ipcalc.py#L544-L558", "method_name": "to_reverse", "code": "def to_reverse(self):\n         if self.v == 4:\n             return \".\".join(list(self.dq.split(\".\")[::-1]) + [\"in-addr\", \"arpa\"])\n         else:\n             return \".\".join(list(self.hex())[::-1] + [\"ip6\", \"arpa\"])", "query": "reverse a string"}
{"id": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/distribution_layer.py#L1210-L1215", "method_name": "__call__", "code": "def __call__(self, distribution_a):\n     if hasattr(distribution_a, \"_tfp_distribution\"):\n       distribution_a = distribution_a._tfp_distribution  \n     return self._kl_divergence_fn(distribution_a)", "query": "normal distribution"}
{"id": "https://github.com/Adyen/adyen-python-api-library/blob/928f6409ab6e2fac300b9fa29d89f3f508b23445/Adyen/httpclient.py#L282-L316", "method_name": "request", "code": "def request(self, url,\n                 json=\"\",\n                 data=\"\",\n                 username=\"\",\n                 password=\"\",\n                 headers=None,\n                 timout=30):\n         raise NotImplementedError(\"request of HTTPClient should have been \"\n                                   \"overridden on initialization. \"\n                                   \"Otherwise, can be overridden to \"\n                                   \"supply your own post method\")", "query": "httpclient post json"}
{"id": "https://github.com/nutechsoftware/alarmdecoder/blob/b0c014089e24455228cb4402cf30ba98157578cd/alarmdecoder/devices/serial_device.py#L149-L172", "method_name": "write", "code": "def write(self, data):\n         try:\n             if isinstance(data, str) or (sys.version_info < (3,) and isinstance(data, unicode)):\n                 data = data.encode(\"utf-8\")\n             self._device.write(data)\n         except serial.SerialTimeoutException:\n             pass\n         except serial.SerialException as err:\n             raise CommError(\"Error writing to device.\", err)\n         else:\n             self.on_write(data=data)", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/zkbt/the-friendly-stars/blob/50d3f979e79e63c66629065c75595696dc79802e/thefriendlystars/constellations/constellation.py#L281-L314", "method_name": "plot", "code": "def plot(self, sizescale=10, color=None, alpha=0.5, label=None, edgecolor=\"none\", **kw):\n         size = np.maximum(sizescale*(1 + self.magnitudelimit - self.magnitude), 1)\n         scatter = plt.scatter(self.ra, self.dec,\n                                     s=size,\n                                     color=color or self.color,\n                                     label=label or \"{} ({:.1f})\".format(self.name, self.epoch),\n                                     alpha=alpha,\n                                     edgecolor=edgecolor,\n                                     **kw)\n         return scatter", "query": "scatter plot"}
{"id": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40", "method_name": "binomial", "code": "def binomial(n,k):\n     if n==k: return 1\n     assert n>k, \"Attempting to call binomial(%d,%d)\" % (n,k)\n     return factorial(n)//(factorial(k)*factorial(n-k))", "query": "binomial distribution"}
{"id": "https://github.com/pndurette/gTTS/blob/b01ac4eb22d40c6241202e202d0418ccf4f98460/gtts/tts.py#L238-L250", "method_name": "save", "code": "def save(self, savefile):\n         with open(str(savefile), \"wb\") as f:\n             self.write_to_fp(f)\n             log.debug(\"Saved to %s\", savefile)", "query": "save list to file"}
{"id": "https://github.com/pikepdf/pikepdf/blob/07154f4dec007e2e9c0c6a8c07b964fd06bc5f77/src/pikepdf/_methods.py#L76-L83", "method_name": "_single_page_pdf", "code": "def _single_page_pdf(page):\n     pdf = Pdf.new()\n     pdf.pages.append(page)\n     bio = BytesIO()\n     pdf.save(bio)\n     bio.seek(0)\n     return bio.read()", "query": "convert html to pdf"}
{"id": "https://github.com/ChristianTremblay/BAC0/blob/8d95b065ea068524a08f5b0c34322ebeeba95d06/BAC0/core/devices/Points.py#L734-L744", "method_name": "enumValue", "code": "def enumValue(self):\n         try:\n             return self.properties.units_state[int(self.lastValue) - 1]\n         except IndexError:\n             value = \"unknown\"\n         except ValueError:\n             value = \"NaN\"\n         return value", "query": "get name of enumerated value"}
{"id": "https://github.com/fedora-python/pyp2rpm/blob/853eb3d226689a5ccdcdb9358b1a3394fafbd2b5/pyp2rpm/utils.py#L44-L56", "method_name": "memoize_by_args", "code": "def memoize_by_args(func):\n     memory = {}\n     @functools.wraps(func)\n     def memoized(*args):\n         if args not in memory.keys():\n             value = func(*args)\n             memory[args] = value\n         return memory[args]\n     return memoized", "query": "memoize to disk  - persistent memoization"}
{"id": "https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L90-L113", "method_name": "csv_writer", "code": "def csv_writer(molecules, options, prefix):\n     outdir = os.getcwd()\n     filename = prefix + \".csv\"\n     outfile = os.path.join(outdir, filename)\n     f = open(outfile, \"w\")\n     csv_writer = csv.writer(f)\n     mol = molecules[0]\n     write_csv_header(mol, csv_writer)\n     for mol in molecules:\n         write_csv_line(mol, csv_writer, options)\n     f.close()", "query": "write csv"}
{"id": "https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/imgIO.py#L39-L73", "method_name": "imread", "code": "def imread(img, color=None, dtype=None): \n     COLOR2CV = {\"gray\": cv2.IMREAD_GRAYSCALE, \n                 \"all\": cv2.IMREAD_COLOR, \n                 None: cv2.IMREAD_ANYCOLOR \n                 } \n     c = COLOR2CV[color] \n     if callable(img): \n         img = img() \n     elif isinstance(img, string_types): \n         if dtype in (None, \"noUint\") or np.dtype(dtype) != np.uint8: \n             c  = cv2.IMREAD_ANYDEPTH \n         img2 = cv2.imread(img, c) \n         if img2 is None: \n             raise IOError(\"image \"%s\" is not existing\" % img) \n         img = img2 \n     elif color == \"gray\" and img.ndim == 3:  \n         img = toGray(img) \n     if dtype is not None: \n         if isinstance(img, np.ndarray): \n             img = _changeArrayDType(img, dtype, cutHigh=False) \n     return img", "query": "converting uint8 array to image"}
{"id": "https://github.com/polyaxon/polyaxon/blob/e1724f0756b1a42f9e7aa08a976584a84ef7f016/polyaxon/libs/json_utils.py#L75-L79", "method_name": "dumps", "code": "def dumps(value, escape=False, **kwargs):\n     if escape:\n         return _default_escaped_encoder.encode(value)\n     return _default_encoder.encode(value)", "query": "html encode string"}
{"id": "https://github.com/release-engineering/productmd/blob/49256bf2e8c84124f42346241140b986ad7bfc38/productmd/common.py#L302-L315", "method_name": "parse_file", "code": "def parse_file(self, f):\n         if hasattr(f, \"seekable\"):\n             if f.seekable():\n                 f.seek(0)\n         elif hasattr(f, \"seek\"):\n             f.seek(0)\n         if six.PY3 and isinstance(f, six.moves.http_client.HTTPResponse):\n             reader = codecs.getreader(\"utf-8\")\n             parser = json.load(reader(f))\n         else:\n             parser = json.load(f)\n         return parser", "query": "parse json file"}
{"id": "https://github.com/adsabs/adsutils/blob/fb9d6b4f6ed5e6ca19c552efc3cdd6466c587fdb/adsutils/Unicode.py#L153-L159", "method_name": "encode", "code": "def encode(self,str):\n         data = self.u2ent(str)\n         data = data.replace(\"&\", \"&amp;\")\n         data = data.replace(\"<\", \"&lt;\")\n         data = data.replace(\"\\\"\", \"&quot;\")\n         data = data.replace(\">\", \"&gt;\")\n         return data", "query": "html encode string"}
{"id": "https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/__init__.py#L180-L205", "method_name": "random_elements", "code": "def random_elements(self, elements=(\"a\", \"b\", \"c\"), length=None, unique=False):\n         fn = choices_distribution_unique if unique else choices_distribution\n         if length is None:\n             length = self.generator.random.randint(1, len(elements))\n         if unique and length > len(elements):\n             raise ValueError(\n                 \"Sample length cannot be longer than the number of unique elements to pick from.\")\n         if isinstance(elements, dict):\n             choices = elements.keys()\n             probabilities = elements.values()\n         else:\n             if unique:\n                 return self.generator.random.sample(elements, length)\n             choices = elements\n             probabilities = [1.0 for _ in range(len(choices))]\n         return fn(\n             list(choices),\n             list(probabilities),\n             self.generator.random,\n             length=length,\n         )", "query": "unique elements"}
{"id": "https://github.com/alejandroautalan/pygubu/blob/41c8fb37ef973736ec5d68cbe1cd4ecb78712e40/pygubudesigner/main.py#L491-L495", "method_name": "do_save", "code": "def do_save(self, fname):\n         self.save_file(fname)\n         self.currentfile = fname\n         self.is_changed = False\n         logger.info(_(\"Project saved to {0}\").format(fname))", "query": "save list to file"}
{"id": "https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/egmm_mixpl.py#L69-L90", "method_name": "Dictionarize", "code": "def Dictionarize(rankings, m):\n     rankcnt = {}\n     for ranking in rankings:\n         flag = 0\n         l = len(ranking)\n         if len(set(ranking)) < l:\n             print(\"Orders with duplicate alternatives are ignored!\")\n             continue\n         for i in range(l):\n             if ranking[i] >= m or ranking[i] < 0:\n                 flag = 1\n         if flag == 1:\n             print(\"Alternative index out of range! Ranking ignored!\")\n             continue\n         key = rank2str(ranking)\n         if key in rankcnt:\n             rankcnt[key] += 1\n         else:\n             rankcnt[key] = 1\n     return rankcnt", "query": "fuzzy match ranking"}
{"id": "https://github.com/tzutalin/labelImg/blob/6afd15aa88f89f41254e0004ed219b3965eb2c0d/labelImg.py#L664-L674", "method_name": "editLabel", "code": "def editLabel(self):\n         if not self.canvas.editing():\n             return\n         item = self.currentItem()\n         if not item:\n             return\n         text = self.labelDialog.popUp(item.text())\n         if text is not None:\n             item.setText(text)\n             item.setBackground(generateColorByText(text))\n             self.setDirty()", "query": "underline text in label widget"}
{"id": "https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/utils/permutations.py#L166-L184", "method_name": "permutation_from_block_permutations", "code": "def permutation_from_block_permutations(permutations):\n     offset = 0\n     new_perm = []\n     for p in permutations:\n         new_perm[offset: offset +len(p)] = [p_i + offset for p_i in p]\n         offset += len(p)\n     return tuple(new_perm)", "query": "all permutations of a list"}
{"id": "https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/snep/client.py#L51-L74", "method_name": "recv_response", "code": "def recv_response(socket, acceptable_length, timeout):\n     if socket.poll(\"recv\", timeout):\n         snep_response = socket.recv()\n         if len(snep_response) < 6:\n             log.debug(\"snep response initial fragment too short\")\n             return None\n         version, status, length = struct.unpack(\">BBL\", snep_response[:6])\n         if length > acceptable_length:\n             log.debug(\"snep response exceeds acceptable length\")\n             return None\n         if len(snep_response) - 6 < length:\n             socket.send(b\"\\x10\\x00\\x00\\x00\\x00\\x00\")\n             while len(snep_response) - 6 < length:\n                 if socket.poll(\"recv\", timeout):\n                     snep_response += socket.recv()\n                 else:\n                     return None\n         return bytearray(snep_response)", "query": "socket recv timeout"}
{"id": "https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/Simple6809/Simple6809_rom.py#L40-L66", "method_name": "extract_zip", "code": "def extract_zip(self):\n         assert self.FILE_COUNT>0\n         try:\n             with zipfile.ZipFile(self.archive_path, \"r\") as zip:\n                 namelist = zip.namelist()\n                 print(\"namelist():\", namelist)\n                 if namelist != self.ARCHIVE_NAMES:\n                     msg = (\n                         \"Wrong archive content?!?\"\n                         \" namelist should be: %r\"\n                     ) % self.ARCHIVE_NAMES\n                     log.error(msg)\n                     raise RuntimeError(msg)\n                 zip.extractall(path=self.ROM_PATH)\n         except BadZipFile as err:\n             msg = \"Error extracting archive %r: %s\" % (self.archive_path, err)\n             log.error(msg)\n             raise BadZipFile(msg)\n         hex2bin(\n             src=os.path.join(self.ROM_PATH, \"ExBasROM.hex\"),\n             dst=self.rom_path,\n             verbose=False\n         )", "query": "extract zip file recursively"}
{"id": "https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149", "method_name": "_dt_to_epoch", "code": "def _dt_to_epoch(self, dt):\n         if PY2:\n             time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n             return int(time_delta.total_seconds())\n         else:\n             return int(dt.timestamp())", "query": "convert a utc time to epoch"}
{"id": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/utils/serialization.py#L47-L60", "method_name": "deserialize", "code": "def deserialize(s):\n     s = s.strip()\n     try:\n         doc = etree.fromstring(s)\n         if is_tmdd(doc):\n             from ..converter.tmdd import tmdd_to_json\n             return (tmdd_to_json(doc), \"json\")\n         return (doc, \"xml\")\n     except etree.XMLSyntaxError:\n         try:\n             return (json.loads(s), \"json\")\n         except ValueError:\n             raise Exception(\"Doesn\"t look like either JSON or XML\")", "query": "deserialize json"}
{"id": "https://github.com/tyiannak/pyAudioAnalysis/blob/e3da991e7247492deba50648a4c7c0f41e684af4/pyAudioAnalysis/audioVisualization.py#L32-L52", "method_name": "levenshtein", "code": "def levenshtein(str1, s2):\n     N1 = len(str1)\n     N2 = len(s2)\n     stringRange = [range(N1 + 1)] * (N2 + 1)\n     for i in range(N2 + 1):\n         stringRange[i] = range(i,i + N1 + 1)\n     for i in range(0,N2):\n         for j in range(0,N1):\n             if str1[j] == s2[i]:\n                 stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                             stringRange[i][j+1] + 1,\n                                             stringRange[i][j])\n             else:\n                 stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                             stringRange[i][j+1] + 1,\n                                             stringRange[i][j] + 1)\n     return stringRange[N2][N1]", "query": "string similarity levenshtein"}
{"id": "https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/cfgpars.py#L752-L790", "method_name": "saveParList", "code": "def saveParList(self, *args, **kw):\n         if \"filename\" in kw:\n             filename = kw[\"filename\"]\n         if not filename:\n             filename = self.getFilename()\n         if not filename:\n             raise ValueError(\"No filename specified to save parameters\")\n         if hasattr(filename,\"write\"):\n             fh = filename\n             absFileName = os.path.abspath(fh.name)\n         else:\n             absFileName = os.path.expanduser(filename)\n             absDir = os.path.dirname(absFileName)\n             if len(absDir) and not os.path.isdir(absDir): os.makedirs(absDir)\n             fh = open(absFileName,\"w\")\n         numpars = len(self.__paramList)\n         if self._forUseWithEpar: numpars -= 1\n         if not self.final_comment: self.final_comment = [\"\"] \n at EOF\n         while len(self.defaults):\n             self.defaults.pop(-1) \n         for key in self._neverWrite:\n             self.defaults.append(key)\n         self.write(fh)\n         fh.close()\n         retval = str(numpars) + \" parameters written to \" + absFileName\n         self.filename = absFileName \n         self.debug(\"Keys not written: \"+str(self.defaults))\n         return retval", "query": "save list to file"}
{"id": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_numpy.py#L323-L365", "method_name": "random_sample", "code": "def random_sample(list_, nSample, strict=False, rng=None, seed=None):\n     rng = ensure_rng(seed if rng is None else rng)\n     if isinstance(list_, list):\n         list2_ = list_[:]\n     else:\n         list2_ = np.copy(list_)\n     if len(list2_) == 0 and not strict:\n         return list2_\n     rng.shuffle(list2_)\n     if nSample is None and strict is False:\n         return list2_\n     if not strict:\n         nSample = min(max(0, nSample), len(list2_))\n     sample_list = list2_[:nSample]\n     return sample_list", "query": "randomly extract x items from a list"}
{"id": "https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/acquisitions/base.py#L52-L60", "method_name": "optimize", "code": "def optimize(self, duplicate_manager=None):\n         if not self.analytical_gradient_acq:\n             out = self.optimizer.optimize(f=self.acquisition_function, duplicate_manager=duplicate_manager)\n         else:\n             out = self.optimizer.optimize(f=self.acquisition_function, f_df=self.acquisition_function_withGradients, duplicate_manager=duplicate_manager)\n         return out", "query": "nelder mead optimize"}
{"id": "https://github.com/securestate/termineter/blob/d657d25d97c7739e650b951c396404e857e56625/lib/c1218/connection.py#L210-L222", "method_name": "read", "code": "def read(self, size):\n \t\tdata = self.serial_h.read(size)\n \t\tself.logger.debug(\"read data, length: \" + str(len(data)) + \" data: \" + binascii.b2a_hex(data).decode(\"utf-8\"))\n \t\tself.serial_h.write(ACK)\n \t\tif sys.version_info[0] == 2:\n \t\t\tdata = bytearray(data)\n \t\treturn data", "query": "sending binary data over a serial connection"}
{"id": "https://github.com/dlancer/django-crispy-contact-form/blob/3d422556add5aea3607344a034779c214f84da04/contact_form/helpers.py#L8-L19", "method_name": "get_user_ip", "code": "def get_user_ip(request):\n     ip = get_real_ip(request)\n     if ip is None:\n         ip = get_ip(request)\n         if ip is None:\n             ip = \"127.0.0.1\"\n     return ip", "query": "get current ip address"}
{"id": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_numpy.py#L323-L365", "method_name": "random_sample", "code": "def random_sample(list_, nSample, strict=False, rng=None, seed=None):\n     rng = ensure_rng(seed if rng is None else rng)\n     if isinstance(list_, list):\n         list2_ = list_[:]\n     else:\n         list2_ = np.copy(list_)\n     if len(list2_) == 0 and not strict:\n         return list2_\n     rng.shuffle(list2_)\n     if nSample is None and strict is False:\n         return list2_\n     if not strict:\n         nSample = min(max(0, nSample), len(list2_))\n     sample_list = list2_[:nSample]\n     return sample_list", "query": "randomly extract x items from a list"}
{"id": "https://github.com/vicalloy/lbutils/blob/66ae7e73bc939f073cdc1b91602a95e67caf4ba6/lbutils/xlsxutils.py#L20-L32", "method_name": "export_xlsx", "code": "def export_xlsx(wb, output, fn):\n     wb.close()\n     output.seek(0)\n     response = HttpResponse(output.read(), content_type=\"application/vnd.ms-excel\")\n     cd = codecs.encode(\"attachment;filename=%s\" % fn, \"utf-8\")\n     response[\"Content-Disposition\"] = cd\n     return response", "query": "export to excel"}
{"id": "https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149", "method_name": "_dt_to_epoch", "code": "def _dt_to_epoch(self, dt):\n         if PY2:\n             time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n             return int(time_delta.total_seconds())\n         else:\n             return int(dt.timestamp())", "query": "convert a utc time to epoch"}
{"id": "https://github.com/ARMmbed/mbed-cloud-sdk-python/blob/c0af86fb2cdd4dc7ed26f236139241067d293509/examples/connect/set-resource.py#L23-L42", "method_name": "_main", "code": "def _main():\n     api = ConnectAPI()\n     api.start_notifications()\n     devices = api.list_connected_devices().data\n     if not devices:\n         raise Exception(\"No connected devices registered. Aborting\")\n     value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE)\n     print(\"Current value: %r\" % (value,))\n     api.set_resource_value(device_id=devices[0].id,\n                            resource_path=WRITEABLE_RESOURCE,\n                            resource_value=\"10\")\n     value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE)\n     print(\"Current value: %r\" % (value,))", "query": "get current observable value"}
{"id": "https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/structural/cnvkit.py#L633-L646", "method_name": "_add_plots_to_output", "code": "def _add_plots_to_output(out, data):\n     out[\"plot\"] = {}\n     diagram_plot = _add_diagram_plot(out, data)\n     if diagram_plot:\n         out[\"plot\"][\"diagram\"] = diagram_plot\n     scatter = _add_scatter_plot(out, data)\n     if scatter:\n         out[\"plot\"][\"scatter\"] = scatter\n     scatter_global = _add_global_scatter_plot(out, data)\n     if scatter_global:\n         out[\"plot\"][\"scatter_global\"] = scatter_global\n     return out", "query": "scatter plot"}
{"id": "https://github.com/myusuf3/delorean/blob/3e8a7b8cfd4c26546f62bde2f34002893adfa08a/delorean/dates.py#L492-L512", "method_name": "epoch", "code": "def epoch(self):\n         epoch_sec = pytz.utc.localize(datetime.utcfromtimestamp(0))\n         now_sec = pytz.utc.normalize(self._dt)\n         delta_sec = now_sec - epoch_sec\n         return get_total_second(delta_sec)", "query": "convert a utc time to epoch"}
{"id": "https://github.com/simonw/datasette/blob/11b352b4d52fd02a422776edebb14f12e4994d3b/datasette/app.py#L521-L527", "method_name": "table_metadata", "code": "def table_metadata(self, database, table):\n         \"Fetch table-specific metadata.\"\n         return (self.metadata(\"databases\") or {}).get(database, {}).get(\n             \"tables\", {}\n         ).get(\n             table, {}\n         )", "query": "get database table name"}
{"id": "https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/tools/model_dump.py#L99-L127", "method_name": "read_properties", "code": "def read_properties(entry):\n     stream = entry.get(\"properties\")\n     if stream is None:\n         raise Exception(\"can not find properties\")\n     s = stream.open()\n     f = BytesIO(s.read())\n     byte_order = read_u8(f)\n     if byte_order != 0x4c:\n         raise NotImplementedError(\"be byteorder\")\n     version = read_u8(f)\n     entry_count = read_u16le(f)\n     props = []\n     for i in range(entry_count):\n         pid = read_u16le(f)\n         format = read_u16le(f)\n         byte_size = read_u16le(f)\n         props.append([pid, format, byte_size])\n     property_entries = {}\n     for pid, format, byte_size in props:\n         data = f.read(byte_size)\n         property_entries[pid] = data\n     return property_entries", "query": "read properties file"}
{"id": "https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/src/external/coremltools_wrap/coremltools/deps/protobuf/python/google/protobuf/descriptor.py#L321-L337", "method_name": "EnumValueName", "code": "def EnumValueName(self, enum, value):\n     return self.enum_types_by_name[enum].values_by_number[value].name", "query": "get name of enumerated value"}
{"id": "https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/NDataHandler.py#L469-L481", "method_name": "read_properties", "code": "def read_properties(self):\n         with self.__lock:\n             absolute_file_path = self.__file_path\n             with open(absolute_file_path, \"rb\") as fp:\n                 local_files, dir_files, eocd = parse_zip(fp)\n                 properties = read_json(fp, local_files, dir_files, b\"metadata.json\")\n             return properties", "query": "read properties file"}
{"id": "https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/flows.py#L254-L266", "method_name": "set_workdir", "code": "def set_workdir(self, workdir, chroot=False):\n         if not chroot and hasattr(self, \"workdir\") and self.workdir != workdir:\n             raise ValueError(\"self.workdir != workdir: %s, %s\" % (self.workdir,  workdir))\n         self.workdir = os.path.abspath(workdir)\n         self.indir = Directory(os.path.join(self.workdir, \"indata\"))\n         self.outdir = Directory(os.path.join(self.workdir, \"outdata\"))\n         self.tmpdir = Directory(os.path.join(self.workdir, \"tmpdata\"))\n         self.wdir = Directory(self.workdir)", "query": "set working directory"}
{"id": "https://github.com/tisimst/mcerp/blob/2bb8260c9ad2d58a806847f1b627b6451e407de1/mcerp/__init__.py#L1150-L1167", "method_name": "Binomial", "code": "def Binomial(n, p, tag=None):\n     assert (\n         int(n) == n and n > 0\n     ), \"Binomial number of trials \"n\" must be an integer greater than zero\"\n     assert (\n         0 < p < 1\n     ), \"Binomial probability \"p\" must be between zero and one, non-inclusive\"\n     return uv(ss.binom(n, p), tag=tag)", "query": "binomial distribution"}
{"id": "https://github.com/talkincode/txradius/blob/b86fdbc9be41183680b82b07d3a8e8ea10926e01/txradius/mschap/mschap.py#L94-L101", "method_name": "convert_to_hex_string", "code": "def convert_to_hex_string(string):\n     hex_str = \"\"\n     for c in string:\n         hex_tmp = hex(ord(c))[2:]\n         if len(hex_tmp) == 1:\n             hex_tmp = \"0\" + hex_tmp\n         hex_str += hex_tmp\n     return hex_str.upper()", "query": "convert decimal to hex"}
